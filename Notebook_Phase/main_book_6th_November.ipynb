{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-06T23:40:26.030136Z",
     "start_time": "2023-11-06T23:40:16.847786Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (3.7.2)\r\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from spacy) (3.0.12)\r\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from spacy) (1.0.5)\r\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from spacy) (1.0.10)\r\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from spacy) (2.0.8)\r\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from spacy) (3.0.9)\r\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from spacy) (8.2.1)\r\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from spacy) (1.1.2)\r\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from spacy) (2.4.8)\r\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from spacy) (2.0.10)\r\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from spacy) (0.3.4)\r\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from spacy) (0.9.0)\r\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from spacy) (6.4.0)\r\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from spacy) (4.66.1)\r\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from spacy) (2.31.0)\r\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from spacy) (2.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from spacy) (3.1.2)\r\n",
      "Requirement already satisfied: setuptools in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from spacy) (68.0.0)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from spacy) (23.1)\r\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from spacy) (3.3.0)\r\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from spacy) (1.24.3)\r\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.10.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.7.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\r\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.11)\r\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.1.3)\r\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\r\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from jinja2->spacy) (2.1.1)\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (4.35.0)\r\n",
      "Requirement already satisfied: filelock in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from transformers) (3.9.0)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from transformers) (0.17.3)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from transformers) (1.24.3)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from transformers) (23.1)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from transformers) (6.0.1)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from transformers) (2023.10.3)\r\n",
      "Requirement already satisfied: requests in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from transformers) (2.31.0)\r\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from transformers) (0.14.1)\r\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from transformers) (0.4.0)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from transformers) (4.66.1)\r\n",
      "Requirement already satisfied: fsspec in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.10.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.7.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from requests->transformers) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from requests->transformers) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from requests->transformers) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from requests->transformers) (2023.7.22)\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fastcoref in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (2.1.6)\r\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from fastcoref) (4.66.1)\r\n",
      "Requirement already satisfied: numpy>=1.21.6 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from fastcoref) (1.24.3)\r\n",
      "Requirement already satisfied: scipy>=1.7.3 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from fastcoref) (1.10.1)\r\n",
      "Requirement already satisfied: spacy>=3.0.6 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from fastcoref) (3.7.2)\r\n",
      "Requirement already satisfied: torch>=1.10.0 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from fastcoref) (2.1.0)\r\n",
      "Requirement already satisfied: transformers>=4.11.3 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from fastcoref) (4.35.0)\r\n",
      "Requirement already satisfied: datasets>=2.5.2 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from fastcoref) (2.14.6)\r\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from datasets>=2.5.2->fastcoref) (14.0.0)\r\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from datasets>=2.5.2->fastcoref) (0.3.7)\r\n",
      "Requirement already satisfied: pandas in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from datasets>=2.5.2->fastcoref) (2.0.3)\r\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from datasets>=2.5.2->fastcoref) (2.31.0)\r\n",
      "Requirement already satisfied: xxhash in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from datasets>=2.5.2->fastcoref) (3.4.1)\r\n",
      "Requirement already satisfied: multiprocess in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from datasets>=2.5.2->fastcoref) (0.70.15)\r\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets>=2.5.2->fastcoref) (2023.10.0)\r\n",
      "Requirement already satisfied: aiohttp in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from datasets>=2.5.2->fastcoref) (3.8.6)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from datasets>=2.5.2->fastcoref) (0.17.3)\r\n",
      "Requirement already satisfied: packaging in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from datasets>=2.5.2->fastcoref) (23.1)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from datasets>=2.5.2->fastcoref) (6.0.1)\r\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from spacy>=3.0.6->fastcoref) (3.0.12)\r\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from spacy>=3.0.6->fastcoref) (1.0.5)\r\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from spacy>=3.0.6->fastcoref) (1.0.10)\r\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from spacy>=3.0.6->fastcoref) (2.0.8)\r\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from spacy>=3.0.6->fastcoref) (3.0.9)\r\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from spacy>=3.0.6->fastcoref) (8.2.1)\r\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from spacy>=3.0.6->fastcoref) (1.1.2)\r\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from spacy>=3.0.6->fastcoref) (2.4.8)\r\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from spacy>=3.0.6->fastcoref) (2.0.10)\r\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from spacy>=3.0.6->fastcoref) (0.3.4)\r\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from spacy>=3.0.6->fastcoref) (0.9.0)\r\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from spacy>=3.0.6->fastcoref) (6.4.0)\r\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from spacy>=3.0.6->fastcoref) (2.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from spacy>=3.0.6->fastcoref) (3.1.2)\r\n",
      "Requirement already satisfied: setuptools in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from spacy>=3.0.6->fastcoref) (68.0.0)\r\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from spacy>=3.0.6->fastcoref) (3.3.0)\r\n",
      "Requirement already satisfied: filelock in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from torch>=1.10.0->fastcoref) (3.9.0)\r\n",
      "Requirement already satisfied: typing-extensions in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from torch>=1.10.0->fastcoref) (4.7.1)\r\n",
      "Requirement already satisfied: sympy in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from torch>=1.10.0->fastcoref) (1.11.1)\r\n",
      "Requirement already satisfied: networkx in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from torch>=1.10.0->fastcoref) (3.1)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from transformers>=4.11.3->fastcoref) (2023.10.3)\r\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from transformers>=4.11.3->fastcoref) (0.14.1)\r\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from transformers>=4.11.3->fastcoref) (0.4.0)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from aiohttp->datasets>=2.5.2->fastcoref) (23.1.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from aiohttp->datasets>=2.5.2->fastcoref) (3.3.2)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from aiohttp->datasets>=2.5.2->fastcoref) (6.0.4)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from aiohttp->datasets>=2.5.2->fastcoref) (4.0.3)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from aiohttp->datasets>=2.5.2->fastcoref) (1.9.2)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from aiohttp->datasets>=2.5.2->fastcoref) (1.4.0)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from aiohttp->datasets>=2.5.2->fastcoref) (1.3.1)\r\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.0.6->fastcoref) (0.6.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.0.6->fastcoref) (2.10.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from requests>=2.19.0->datasets>=2.5.2->fastcoref) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from requests>=2.19.0->datasets>=2.5.2->fastcoref) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from requests>=2.19.0->datasets>=2.5.2->fastcoref) (2023.7.22)\r\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from thinc<8.3.0,>=8.1.8->spacy>=3.0.6->fastcoref) (0.7.11)\r\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from thinc<8.3.0,>=8.1.8->spacy>=3.0.6->fastcoref) (0.1.3)\r\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from typer<0.10.0,>=0.3.0->spacy>=3.0.6->fastcoref) (8.1.7)\r\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from weasel<0.4.0,>=0.1.0->spacy>=3.0.6->fastcoref) (0.16.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from jinja2->spacy>=3.0.6->fastcoref) (2.1.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from pandas->datasets>=2.5.2->fastcoref) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from pandas->datasets>=2.5.2->fastcoref) (2023.3.post1)\r\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from pandas->datasets>=2.5.2->fastcoref) (2023.3)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from sympy->torch>=1.10.0->fastcoref) (1.3.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.5.2->fastcoref) (1.16.0)\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: intervaltree in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (3.1.0)\r\n",
      "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from intervaltree) (2.4.0)\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: trafilatura in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (1.6.2)\r\n",
      "Requirement already satisfied: certifi in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from trafilatura) (2023.7.22)\r\n",
      "Requirement already satisfied: courlan>=0.9.4 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from trafilatura) (0.9.4)\r\n",
      "Requirement already satisfied: htmldate>=1.5.1 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from trafilatura) (1.5.2)\r\n",
      "Requirement already satisfied: justext>=3.0.0 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from trafilatura) (3.0.0)\r\n",
      "Requirement already satisfied: lxml==4.9.2 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from trafilatura) (4.9.2)\r\n",
      "Requirement already satisfied: charset-normalizer>=3.2.0 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from trafilatura) (3.3.2)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from trafilatura) (1.26.18)\r\n",
      "Requirement already satisfied: langcodes>=3.3.0 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from courlan>=0.9.4->trafilatura) (3.3.0)\r\n",
      "Requirement already satisfied: tld>=0.13 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from courlan>=0.9.4->trafilatura) (0.13)\r\n",
      "Requirement already satisfied: dateparser>=1.1.2 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from htmldate>=1.5.1->trafilatura) (1.1.8)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from htmldate>=1.5.1->trafilatura) (2.8.2)\r\n",
      "Requirement already satisfied: pytz in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from dateparser>=1.1.2->htmldate>=1.5.1->trafilatura) (2023.3.post1)\r\n",
      "Requirement already satisfied: regex!=2019.02.19,!=2021.8.27 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from dateparser>=1.1.2->htmldate>=1.5.1->trafilatura) (2023.10.3)\r\n",
      "Requirement already satisfied: tzlocal in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from dateparser>=1.1.2->htmldate>=1.5.1->trafilatura) (5.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from python-dateutil>=2.8.2->htmldate>=1.5.1->trafilatura) (1.16.0)\r\n",
      "Requirement already satisfied: backports.zoneinfo in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from tzlocal->dateparser>=1.1.2->htmldate>=1.5.1->trafilatura) (0.2.1)\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting TextBlob\r\n",
      "  Using cached textblob-0.17.1-py2.py3-none-any.whl (636 kB)\r\n",
      "Collecting nltk>=3.1 (from TextBlob)\r\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\r\n",
      "Requirement already satisfied: click in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from nltk>=3.1->TextBlob) (8.1.7)\r\n",
      "Collecting joblib (from nltk>=3.1->TextBlob)\r\n",
      "  Using cached joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\r\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from nltk>=3.1->TextBlob) (2023.10.3)\r\n",
      "Requirement already satisfied: tqdm in /Users/ameek/anaconda3/envs/Project/lib/python3.8/site-packages (from nltk>=3.1->TextBlob) (4.66.1)\r\n",
      "Using cached joblib-1.3.2-py3-none-any.whl (302 kB)\r\n",
      "Installing collected packages: joblib, nltk, TextBlob\r\n",
      "Successfully installed TextBlob-0.17.1 joblib-1.3.2 nltk-3.8.1\r\n"
     ]
    }
   ],
   "source": [
    "# Python==3.8 (3.7.10 backup plan)\n",
    "\n",
    "!pip install spacy\n",
    "!pip install transformers\n",
    "!pip install fastcoref    # Used for coreference resolution\n",
    "!pip install intervaltree # Used for finding overlapping intervals between sentence start and end positions and mention positions within the text. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Attempting a fetch of a provided web page"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e21d4911564df4e"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "'Leveraging AI to empower all developers at GitHub Universe 2023\\nGitHub Universe 2023 is just around the corner! Join us and learn how GitHub is leveraging AI to empower all developers, including developers with disabilities.\\nWe’re spending Women’s History Month with women leaders who are making history every day in the tech community. Read more about Erin Spiceland: Software Engineer at SpaceX.\\nEvery March we recognize the women who have shaped history—and now, we’re taking a look forward. From driving software development in large companies to maintaining thriving open source communities, we’re spending Women’s History Month with women leaders who are making history every day in the tech community. Erin Spiceland is a Software Engineer for SpaceX. Born and raised in rural south Georgia, she is a Choctaw and Chickasaw mother of two now living in downtown Los Angeles. Erin didn’t finish college—she’s a predominantly self-taught software engineer. In her spare time, she makes handmade Native American beadwork and regalia and attends powwows.\\nMy career has been a winding road through periods of stimulation and health as well as periods of personal misery. During it all, I’ve learned a variety of programming languages and technologies while working on a diverse array of products and services. I’m a domestic abuse survivor and a Choctaw bisexual polyamorous woman. I’m so proud of myself that I made it this far considering where I came from.\\nIn 2007, I had a three-year-old daughter and I was trying to finish my computer science degree one class at a time, all while keeping my house and family running smoothly. I found the math classes exciting and quickly finished my math minor, leaving only computer science classes. I was looking at about five years before I would graduate. Then, my husband at the time recommended me for an entry software developer position at a telecom and digital communications company.\\nWhen faced with the choice between an expensive computer science degree and getting paid to do what I loved, I dropped out of college and accepted the job. I was hired to work on internal tooling, and eventually, products. I did a lot of development on product front-ends, embedded network devices, and a distributed platform-as-a-service. I learned Java/JSP, Python, JavaScript/CSS, Node.js, as well as MySQL, PostgreSQL, and distributed systems architecture. It was an intense experience that required a lot of self-teaching, asking others for help, and daycare, but it set me up for my later successes.\\nI appreciate and admire technical, effective leaders who care for their reports as humans, not as lines on a burndown chart, and forego heavy-handed direction in favor of communication and mutual dialogue. I think it’s as important for a leader to concern herself with her coworkers’ personal well-being as it is for her to direct their performance.\\nLast year I took a pay cut to move from a safe, easy job where I had security to work in a language I hadn’t seen in years and with systems more complicated than anything I’d worked with before. I moved from a place where I had a huge four bedroom house to a studio apartment that was twice the price. I moved away from my children, of who I share custody with my ex-husband. We fly across the U.S. to see each other now. I miss my children every day. However, I get to be a wonderful role model for them.\\nI can’t wait to wake up every day with my partner who loves me so much. I’m looking forward to showing my children exactly how far they can go. I’m excited to keep exploring Los Angeles.\\nWant to know more about Erin Spiceland? Follow them on GitHub or Twitter.\\nWant to learn more about featured leaders for Women’s History Month? Read about:\\nCheck back in soon—we’ll be adding new interviews weekly throughout March.'"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import trafilatura\n",
    "downloaded = trafilatura.fetch_url('https://github.blog/2019-03-29-leader-spotlight-erin-spiceland/')\n",
    "trafilatura.extract(downloaded)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T23:35:22.595637Z",
     "start_time": "2023-11-06T23:35:20.951075Z"
    }
   },
   "id": "940b5977c62bce23"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Attempting a fetch of URLs from an RSS (Really Simple Syndication) feed "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "71377ee5030d5cbc"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.dailymail.co.uk/?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/debate/article-12663637/alex-brummer-joe-biden-bond-markets-sunak-starmer.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/debate/article-12663701/NADINE-DORRIES-Googled-learnt-Big-Tech.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/debate/article-12663827/QUENTIN-LETTS-Jeremy-Corbyn-Rishi-Sunak-crisply-recalled-hed-hailed-Hamas-friends.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/debate/article-12672813/STEPHEN-GLOVER-rocket-Jeremy-Hunt-doesnt-promise-tax-cuts-Tories-finished.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/debate/article-12677181/QUENTIN-LETTS-Sunaks-gaze-framed-begging-dog-eyebrows-told-AI-herald-mankinds-obliteration.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/debate/article-12678233/Banning-China-UKs-AI-summit-make-no-sense-writes-former-head-GCHQs-national-security-centre-CIARAN-MARTIN.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/debate/article-12704993/ALEX-BRUMMER-Bank-Englands-gloomy-forecast-2024-pinch-salt.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/debate/article-12711593/DAN-HODGES-war-Israel-disaster-Keir-Instead-making-him.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/debate/article-12717505/NADINE-DORRIES-ringleaders-book-damage.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/femail/article-12706159/plastic-free-Poppy-row.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/galleries/article-12674735/Keir-Starmer-accused-gaslighting-Muslim-Labour-MPs-Gaza-row.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/galleries/article-12706383/Elon-Musk-tells-Sunak-AI-eventually-mean-no-one-need-job.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/health/article-12549373/UKs-calorific-chocolate-bars-revealed-including-one-worse-waistline-McDonalds-cheeseburger-bad-favourite.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/health/article-12670545/Hidden-NHS-appointments-waiting-list-exceeds-11MILLION-soared-50-Covid.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/health/article-12671907/How-big-hidden-waiting-list-hospital-Use-search-tool-investigation-reveals-11MILLION-people-stuck-NHS-needing-follow-care.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/health/article-12676171/Be-warned-NHS-waiting-list-going-BIGGER-Stark-analysis-suggests-queue-exceed-8million-summer.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/health/article-12715223/How-kids-age-TEN-hospitalised-vaping.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12658595/BAN-Irans-Islamic-Revolutionary-Guard-Corps-urges-Britain-America-tells-Rishi-Sunak-designate-IRGC-terrorists-Tehran-backed-Hamas-attacked-Israel.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12658601/Tory-poll-LABOUR-taxes-Rishi-Sunak-MPs-confidence-Keir-Starmer.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12658869/Cost-living-crunch-forces-half-Brits-downgrade-cancel-winter-holiday-plans.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12659043/Benjamin-Netanyahu-warns-Iran-backed-Hezbollah-against-opening-second-against-Israel-risk-massive-devastating-retaliation-fears-grow-Hamas-conflict-turn-Middle-East-war.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12659077/Security-fears-prank-callers-voicemail-Rishi-Sunaks-old-mobile-number.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12659219/Cabinet-Secretary-Simon-Case-medical-leave-Covid-inquiry.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12659253/Tories-Reform-UK-split-right-wing-Keir-Starmer-Downing-Street-election.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12659309/Mortgage-guarantee-scheme-Chancellor-Jeremy-Hunt-property-prices.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12659755/Top-academics-urge-Rishi-Sunak-not-buckle-pressure-Big-Tech-escape-regulation.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12659759/tech-boss-warns-artificial-intelligence-one-four-chance-destroying-us.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12659839/Immigration-minister-says-illegal-migration-falling.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12659923/Prime-Minister-shelves-conversion-therapy-ban-MPs-demand-scrap-plans-long-promised-manifesto-pledge.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12659927/Rishi-Sunak-faces-mounting-pressure-reshuffle-cabinet-weeks-bruising-election-defeats.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12660279/Government-stop-using-100-migrant-hotels-key-battleground-seats-bid-Sunak-improve-Tory-election-hopes.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12661351/Politicians-areas-like-West-Midlands-Manchester-powers-raise-tax-Michael-Gove.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12661529/Rishi-Sunaks-old-mobile-number-disconnected-WhatsApp-footage-prank-call-deleted-social-media-security-alert-voicemail-works.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12661869/Suella-Braverman-police-Israeli-protesters-chanting-jihad.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12662471/Rishi-Sunak-tells-MPs-hospital-tragedy-caused-Hamas-PM-says-missile-launched-extremists-linked-terror-group-swipes-rush-judgment-Israel-blame-says-terrorists-not-allowed-control-Gaza.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12663679/We-got-real-s-going-Joe-Whats-plan-tan-chill-come-50-Cent-slams-Joe-Biden-taking-beach-vacation-Delaware-war-rages-Israel-Hamas-terrorists.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12663819/Schools-sex-education-material-Education-Parent-asks-Secretary-Gillian-Keegan-warns.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12664165/DAILY-MAIL-COMMENT-glimpse-evil-seek-deny.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12664189/Chanting-jihad-threat-British-democracy-Sunak-says-Met-police-officers-frustrated-inability-make-arrests-pro-Palestinian-protest.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12664231/The-number-people-claiming-benefits-without-required-look-job-rises-2-2-million.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12665285/Rishi-Sunak-stage-reshuffle-week-Tory-right-pushing-Chancellor-axed-Labour-poll-lead-stretches-PMs-anniversary-No10.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12665293/Tory-seats-Dominic-Raab-Iain-Duncan-Smith-lost-stealth-tax-backlash-election.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12665853/Trouble-backyards-Labour-MPs-face-councillor-mutinies-constituencies-amid-growing-party-revolt-Keir-Starmers-Middle-East-stance-wake-Hamas-attacks.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12666071/I-took-photo-womans-XL-Bully-tore-beloved-Jack-Russell-shreds-terrified-kids-never-apologised-just-walked-away.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12666595/Bankers-bonus-cap-scrapped-week-Britain-ditches-EU-rule-post-Brexit-shake-City-regulations-help-London-compete-New-York-Paris-Frankfurt.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12667921/Steve-Coogan-accuses-Keir-Starmer-Jeremy-Corbyn-attacks-Labour-leader.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12667927/Rishi-Sunak-sack-Chancellor-Jeremy-Hunt-Tory-MPs-urge.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12669717/Labour-Israel-Hamas-Keir-Starmer-Muslim-MPs-meeting-councillors.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12670003/MPs-set-suspend-Tory-Peter-Bone-today-exposing-young-aide-damaging-election-Rishi-Sunak.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12670157/Rishi-Sunak-anniversary-PM-inflation-Channel-taxes.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12670749/Labour-meltdown-backing-Israel-Keir-Starmer-dodges-mentioning-Middle-East-crisis-PMQs-faces-anger-Muslim-MPs-meeting-TODAY-150-councillors-demanding-calls-ceasefire-mosque-accusing-misrepresenting-visit.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12671893/Now-furious-Labour-frontbenchers-join-rebellion-demand-Israel-ceasefire-Party-crisis-escalates-Keirs-tense-crunch-meeting-Muslim-MPs-fails-break-standoff-150-councillors-signed-extraordinary-letter-warning-stance.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12672121/Rishi-Sunak-faces-prospect-fierce-election-battle-Labour-Commons-approves-six-week-suspension-veteran-MP-Peter-Bone-exposing-young-aide.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12672945/Rishi-Sunak-vows-protect-UK-AI-amid-fears-technology-used-hackers-scammers-terrorists-18-months.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12673021/Kemi-Badenochs-trade-mission-Japan-promote-British-luxury-firms-branded-ironic-business-leaders-criticise-UKs-tourist-tax.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12673519/The-government-stepping-efforts-stop-copycat-convicts-refusing-appear-court-Justice-Secretary-Alex-Chalk-says.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12674453/Moment-powerful-XL-Bully-growls-police-pacing-garden-force-release-footage-terrifying-encounters-officers-dangerous-dogs.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12674547/Keir-Starmer-accused-gaslighting-Muslim-Labour-MPs-Gaza.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12674703/Rishi-Sunak-warns-AI-used-terrorists-build-chemical-biological-weapons-criminals-carry-child-sex-abuse.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12675989/Liz-Truss-urges-PM-rescind-Chinas-invite-weeks-AI-summit-blasts-Beijings-cavalier-attitude-international-rules-Rishi-Sunak-insists-West-engage-Asian-giant-new-tech.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12676263/Anger-Channel-pair-jailed-illegally-entering-UK-wont-automatically-deported-Home-Office-vows-make-checks-sent-home-countries.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12676395/Labour-frontbencher-Imran-Hussain-joins-left-wing-rebellion-against-Keir-Starmer-Gaza-Israel.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12676521/Rishi-Sunak-AI-statement-given-tech-firms-free-pass.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12676565/Prominent-Tory-MP-arrested-rape-allegation-Politician-quizzed-police-sex-attack-possession-controlled-substance.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12676865/Crispin-Blunt-says-Tory-MP-arrested-rape-allegation-possession-controlled-substance.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12677851/Border-force-rescue-200-Britons-trapped-Gaza-Rishi-Sunak-Palestinians.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12678101/Rishi-Sunak-tax-cuts-inflation.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12678127/DAILY-MAIL-COMMENT-trust-China-age-AI.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12678935/Britains-banned-dog-explosion-3-500-illegal-pooches-UK-1-200-past-10-years-officials-ban-huge-XL-Bully-breed-following-spate-attacks.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12679235/Keir-Starmer-urged-stand-ground-support-Israel-Gaza-crisis-Muslim-Labour-politicians-labelled-traitors.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12679519/Rishi-Sunak-warned-China-send-nasty-dodgy-people-Britain-weeks-AI-summit-Cabinet-minister-insists-not-embarrassing-German-French-Canadian-leaders-stay-away.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12680439/Met-Police-say-intervene-protesters-shout-jihad-pro-Palestine-marches-weekend-terror-cops-issue-fresh-appeal-hunt-activists-photos-paragliding-terrorists-support-Hamas-sign.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12680497/Keir-Starmer-faces-major-Labour-rebellion-Israel-Gaza-Sadiq-Khan-Andy-Burnham-Anas-Sarwar.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12681977/Sanctuary-brave-months-delay-2-000-Afghan-interpreters-arrive-Britain.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12682103/Make-arrests-not-excuses-Jewish-leaders-urge-Met-action-pro-Palestine-rally-chants-jihad-100-000-protesters-expected-descend-streets-London.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12683199/Rishi-Sunak-plans-delay-holding-General-Election-Halloween-2024-bids-avoid-horror-polling-day-surveys-showing-Tories-trail-Labour-20-points.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12683685/Tyson-fury-prime-minister-rishi-sunak-emotional-knife-crime-message-cousin-victim.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12683949/Heartbroken-Londoner-lost-20-loved-ones-family-home-Gaza-blown-pleads-Rishi-Sunak-help-end-bloodshed.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12684339/Ministers-planning-encourage-police-deploy-AI-powerful-tool-good-use-tackle-crimes-shoplifting.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12685067/All-new-houses-England-Wales-sold-freehold-properties-Michael-Gove-revives-bid-phase-feudal-leasehold-system.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12685799/AI-chief-urges-Britain-build-rival-ChatGPT-UKs-security-services-Rishi-Sunak-warned-containing-artificial-intelligence-global-priority-alongside-pandemics-nuclear-war.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12686383/ai-trigger-damaging-extreme-online-content-schoolgirl-molly-russell-suicide.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12686673/ai-humans-robots-rishi-sunak-meta-elon-musk.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12688143/Rishi-Sunak-donates-Poppy-Appeal-wife-Akshata-Murty-meet-fundraisers-Downing-Street-PM-appears-struggle-fit-banknote-collection-pot.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12689545/Rishi-Sunak-piles-pressure-spineless-Keir-Starmer-SACKS-parliamentary-aide-Paul-Bristow-defying-demand-permanent-ceasefire-Gaza.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12689597/Labour-extends-poll-lead-Tories-20-points.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12690683/Boost-Rishi-Sunak-Elon-Musk-says-attend-UKs-AI-summit-Bletchley-Park-week.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12690961/Rishi-Sunak-tells-police-security-chiefs-prepare-terror-attack-streets-UK-emergency-Cobra-meeting-amid-Israel-Hamas-conflict.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12691057/Big-Tech-left-grade-homework-UKs-AI-summit-critics-warn-ahead-event-week.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12692359/Rishi-Sunak-says-hell-discuss-dangers-fears-Artificial-Intelligence-tech-billionaire-Elon-Musk-Bletchley-Park-summit-emerges-civil-servants-wanted-ban-Israel-gathering-amid-Gaza-crisis.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12693251/American-XL-Bullies-officially-banned-following-spate-attacks-Owning-one-without-exemption-certificate-criminal-offence-February.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12694225/Crispin-Blunt-breaks-cover-time-revealing-Tory-MP-arrested-suspicion-rape-drugs-offences.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12694273/Rishi-Sunaks-invite-China-attend-AI-summit-slap-face-Uyghur-people-suffering-worlds-high-tech-genocide-hands-Beijings-oppressive-regime-say-campaigners.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12695351/potholes-machine-fix-road-three-minutes-IRAM-RAMZAN-puts-test.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12696493/Deputy-PM-Oliver-Dowden-AI-summit-Bletchley-Park-Rishi-Sunak.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12696849/Rishi-Sunaks-billionaire-father-law-outcry-India-young-people-work-70-HOUR-weeks.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12697407/Nick-Clegg-warns-AI-interfere-upcoming-election-Bletchley-Park-Elon-Musk.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12697481/Bobby-Charlton-cause-death-revealed.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12697805/Elon-Musk-warns-AI-one-biggest-threats-humanity-Bletchley-Park.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12699017/Rishi-Sunak-says-tech-giants-mark-homework-Artificial-Intelligence-praises-Elon-Musk-warning-risks-ahead-talks-billionaire-PM-denies-hes-lining-post-No10-career.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12699607/electric-car-public-chargers-plunges-counties-England.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12699615/Kings-tailor-protest-Savile-Row-designers-demanding-Rishi-Sunak-scrap-tourist-tax.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12700125/Trusted-journalism-faces-growing-threat-AI-key-countering-fake-news-government-report-says.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12700203/DAILY-MAIL-COMMENT-AI-giants-safety-profit.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12701501/Rishi-Sunak-AI-pandemics-nuclear-war-Bletchley-Park-summit-Elon-Musk.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12701511/Sir-Keir-Starmer-Labour-Gaza-meltdown-ceasefire.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12702027/Cabinet-minister-Michelle-Donelan-warns-misogyny-display-Dominic-Cummings-foul-mouthed-Covid-WhatsApp-messages-referred-female-official-c-risks-putting-women-entering-politics.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12702495/Keir-Starmer-poppy-row-Labour-leader-Islamophobia-video.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12703015/Rishis-high-level-summit-Moment-PM-takes-centre-stage-helpfully-flanked-5ft2-Kamala-Harris-5ft3-Giorgia-Meloni-Bletchley-Park-gathering-world-leaders-tech-bosses-discuss-AI-dangers.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12703517/Rishi-Sunak-frontier-AI-systems-summit-Bletchley.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12703529/Jeremy-Hunt-targeted-affluent-Surrey-seat-stealth-tax-mortgage-rises-Chancellor-Lib-Dems-Blue-Wall.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12703963/Now-Birmingham-New-Street-station-invaded-pro-Palestinian-supporters-disruptive-sit-action.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12704459/Nadine-Dorries-new-book-Plot.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12704709/Elon-Musk-tells-Rishi-Sunak-AI-eventually-mean-no-one-needs-job-conversation-Prime-Minister-following-summit-Bletchley-Park.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12704981/Rishi-Sunaks-target-halve-inflation-met-Bank-England-says-warns-UKs-growth-grinding-halt.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12705125/DAILY-MAIL-COMMENT-lets-turbocharge-growth-tax-cuts.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12705195/Reform-UK-attracts-2019-Tory-voters-poll-Rishi-Sunak-illegal-migration-slash-taxes.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12705823/Well-never-work-Elon-Musk-tells-Rishi-Sunak-rise-magic-genie-AI-eventually-mean-no-one-need-job-extraordinary-interview-Prime-Minister-Bletchley-Park-summit.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12707137/Rishi-Sunak-Palestinian-protest-London-Armistice-Day-Cenotaph-Sadiq-Khan.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12707559/Met-Police-facial-recognition-palestine-day-action-tomorrow-london.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12708197/Gary-Lineker-criticises-Suella-Braverman-Pro-Palestinian-demo-Armistice-Day-hate-march.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12709077/Rishi-Sunak-British-Steels-blast-furnace-hostile-states.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12709095/Gary-Lineker-ministers-urged-ban-Palestinian-march-Armistice-Day.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12711411/New-law-crack-airlines-sneaky-add-charges-new-laws-drafted-Rishi-Sunak.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12711541/Kings-smooth-visit-Kenya-praising-helping-hand-Artificial-Intelligence.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12711641/Free-speech-law-Rishi-Sunak-legislation-newspapers-legal-privacy.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12711701/Scotland-Yard-chief-hate-march-Armistice-Day.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12712259/Shocking-moment-shoplifter-brazenly-ransacks-Sainsburys-shelves-hes-banned-branch-supermarket-stealing-2-500-worth-meat-cheese-amid-calls-tackle-scourge-shoplifting.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12712545/Remember-Fifth-November-Thousands-people-attend-bonfire-nights-UK-including-famous-Lewes-event-effigies-Guy-Fawkes-politicians-burned.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12712675/BBC-Laura-Kuenssberg-thanks-fans-return-TV-absence-death-father.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12712717/Tensions-grow-pro-Palestine-protest-Remembrance-Day-British-patriots-wave-Union-flags-counter-demo-Cenotaph-war-memorials-draped-Palestinian-flag-amid-fears-huge-march-spark-clashes-November-11.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12713303/African-countries-blast-Britains-neo-colonial-ban-importing-hunting-trophies-landmark-poll-showing-half-people-eight-countries-disagree-ban.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12713415/Fury-Westminster-amid-bombshell-claims-Tory-party-covered-alleged-rapes-Conservative-MP-Rishi-Sunak-mounting-pressure-launch-urgent-enquiry-claims.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12713551/Rishi-unveils-plan-skewer-Labour-North-Sea-oil-introducing-legislation-forcing-ministers-consider-new-exploration-licences-year-bid-secure-Britains-energy-security.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12713871/Rishi-Sunak-urged-clamp-Islamophobia-figures-seven-fold-rise-reported-anti-Muslim-incidents-outbreak-violence-Gaza-Israel.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12715081/Sadiq-Khan-Tory-mayor-Susan-Hall-London-election-Rishi-Sunak-ULEZ.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12715115/Rishi-Sunak-Keir-Starmer-Kings-Speech-Net-Zero-crime.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12715201/poppy-seller-forced-leave-waverley-station-edinburgh-protests-charing-cross.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12715345/MPs-clocked-early-half-sitting-days-year.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12715357/Rishi-Sunak-allegations-rape-Tory-MP-complaints-Dorries.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12715447/Should-pro-Palestinian-demonstration-place-London-Remembrance-weekend-cancelled.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12715983/Rishi-Sunak-urged-cheap-words-new-laws-clarity-legal-definition-word-woman-following-Scottish-court-ruling.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12716305/Cabinet-Rishi-Sunak-Tory-leader-election-defeat-Labour-Keir-Starmer.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12716437/Rishi-Sunak-acts-stop-militant-trade-unions-railing-Christmas-PM-brings-forward-new-laws-ensure-four-10-trains-running-strike-days-border-staff-ambulance-crews-issued-minimum-service-requirements.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12716505/israel-president-isaac-herzog-condemns-armistice-day-march-london.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12717621/NADINE-DORRIES-shadowy-group-Tory-party-scheming-against-Rishi-Sunak-replace-Kemi-Badenoch-time-right-sources-say.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/article-12717771/Sadistic-killers-die-jail-measures-set-Kings-Speech-Suella-Braverman-promises.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/news/royals/article-12697607/King-Charles-attend-Cop28-climate-change-conference-Dubai.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/sciencetech/article-12676367/Artificial-Armageddon-5-worst-case-scenarios-AI-revealed-Terminator-like-killer-robots-helping-terrorists-develop-deadly-bioweapons.html?ito=1490&ns_campaign=1490&ns_mchannel=rss', 'https://www.dailymail.co.uk/sciencetech/article-12696583/Rise-robots-AI-crowned-Word-Year-closely-followed-greedflation-nepo-baby-ULEZ.html?ito=1490&ns_campaign=1490&ns_mchannel=rss']\n"
     ]
    }
   ],
   "source": [
    "from trafilatura import feeds\n",
    "mylist = feeds.find_feed_urls('https://www.dailymail.co.uk/news/rishi-sunak/index.rss') # mylist = feeds.find_feed_urls('https://www.dailymail.co.uk/news/uk-politics/index.rss')\n",
    "print(mylist)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T23:40:44.760546Z",
     "start_time": "2023-11-06T23:40:44.181977Z"
    }
   },
   "id": "83f9ca63105ade79"
  },
  {
   "cell_type": "markdown",
   "source": [
    "https://adrien.barbaresi.eu/blog/how-to-download-parallel-politeness-rules-python.html\n",
    "Provides a method to check the robot txt of a site and check if the url being extracted meets the politeness policy.\n",
    "\n",
    "When extracting from bing check for each domain and store in data structure with allow/disallow. If the same base url appears again then no need to recheck robot.txt"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b1cf89cf0ad55d5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Consider when implementing News Article fetch adding politeness / adherence to robot policy."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ffa4c9fb28951d5a"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-agent: Mediapartners-Google\n",
      "Allow: \n",
      "\n",
      "User-agent: NewsNow\n",
      "Disallow: /\n",
      "\n",
      "User-agent: GPTBot\n",
      "Disallow: /\n",
      "\n",
      "User-agent: CCBot\n",
      "Disallow: /\n",
      "\n",
      "User-agent: TurnitinBot\n",
      "Disallow: /\n",
      "\n",
      "User-agent: PetalBot\n",
      "Disallow: /\n",
      "\n",
      "User-agent: MoodleBot\n",
      "Disallow: /\n",
      "\n",
      "User-agent: *\n",
      "Disallow: /sendarticle/\n",
      "Disallow: /Users/\n",
      "Disallow: /users/\n",
      "Disallow: /%2A/print%24\n",
      "Disallow: /email/\n",
      "Disallow: /contactus/\n",
      "Disallow: /share/\n",
      "Disallow: /websearch\n",
      "Disallow: /%2A%3Fcommentpage%3D\n",
      "Disallow: /whsmiths/\n",
      "Disallow: /external/overture/\n",
      "Disallow: /discussion/report-abuse/%2A\n",
      "Disallow: /discussion/report-abuse-ajax/%2A\n",
      "Disallow: /discussion/comment-permalink/%2A\n",
      "Disallow: /discussion/report-abuse/%2A\n",
      "Disallow: /discussion/user-report-abuse/%2A\n",
      "Disallow: /discussion/handlers/%2A\n",
      "Disallow: /discussion/your-profile\n",
      "Disallow: /discussion/your-comments\n",
      "Disallow: /discussion/edit-profile\n",
      "Disallow: /discussion/search/comments\n",
      "Disallow: /discussion/%2A\n",
      "Disallow: /search\n",
      "Disallow: /music/artist/%2A\n",
      "Disallow: /music/album/%2A\n",
      "Disallow: /books/data/%2A\n",
      "Disallow: /settings/\n",
      "Disallow: /embed/\n",
      "Disallow: /%2Astyles/js-on.css%24\n",
      "Disallow: /sport/olympics/2008/events/%2A\n",
      "Disallow: /sport/olympics/2008/medals/%2A\n",
      "Disallow: /f/healthcheck\n",
      "Disallow: /sections\n",
      "Disallow: /top-stories\n",
      "Disallow: /most-read/sport\n",
      "Disallow: /articles\n",
      "Disallow: /global%24\n",
      "Disallow: /%2A/feedarticle/%2A\n",
      "Disallow: /travel/2013/aug/22/been-there-readers-competition%3F%2A\n",
      "Disallow: /preference/%2A\n",
      "Disallow: /59666047/\n",
      "Disallow: /print/\n",
      "Disallow: /info/tech-feedback\n",
      "Disallow: /production-monitoring/\n",
      "Disallow: %2A.emailjson\n",
      "Disallow: %2A.emailtxt\n",
      "Disallow: /headline.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.robotparser\n",
    "\n",
    "# define a website to look for rules\n",
    "base_url = \"https://www.theguardian.com\"\n",
    "\n",
    "# load the necessary components, fetch and parse the file\n",
    "rules = urllib.robotparser.RobotFileParser()\n",
    "rules.set_url(base_url + \"/robots.txt\")\n",
    "rules.read()\n",
    "\n",
    "print(rules)\n",
    "\n",
    "# determine if a page can be fetched by all crawlers\n",
    "rules.can_fetch(\"*\", \"https://www.theguardian.com/import urllib.robotparser\")\n",
    "# returns True or False"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T23:40:58.016508Z",
     "start_time": "2023-11-06T23:40:57.726444Z"
    }
   },
   "id": "2cbfbf0a6243ef85"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Extraction from 1 article from the RSS list and setting some extra parameters in Trafilatura."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "64a2a7fe65d54ceb"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "\"Banning China from the UK's AI summit would make no sense, writes former head of GCHQ's national cyber security centre CIARAN MARTIN\\nThe Prime Minister trod a difficult line in yesterday's landmark speech on Artificial Intelligence.\\nOn the one hand, he was right to downplay fears that the technology might bring about the apocalypse. Rishi Sunak understands that, despite the hysterical warnings of some Silicon Valley gurus, we shouldn't fear killer robots enslaving or extinguishing our species.\\nInstead, as he pointed out, AI is revolutionary – and that revolution brings opportunities, especially in medical research and making our economy more productive.\\nBut it undoubtedly brings risks, too. And as head of cyber security at GCHQ for seven years, I learned to appreciate them.\\nAs Sunak outlined yesterday, AI could help rogue states unleash new cyber-attacks with unprecedented penetration, design devastating biochemical weapons and generate vast amounts of disinformation that hostile nations could use to undermine their enemies' politics.\\nSome of this process is already under way. A parliamentary election in Slovakia last month was mired in controversy after a 'deepfake' audio file emerged only 48 hours before the vote.\\nCiaran Martin, former head of GCHQ's national security centre\\nThe Prime Minister trod a difficult line in yesterday's landmark speech on Artificial Intelligence\\nHighly convincing, it purported to record the leader of the main liberal party hatching a plot with a newspaper journalist on rigging the election.\\nChallenges like these are only going to become more common – which is why Rishi Sunak is right to address them at next week's international summit on AI at Bletchley Park.\\nAs the PM acknowledged, some members of his party will be dismayed at the invitation being extended to China, a totalitarian state that has long held ambitions to outdo the West in this area.\\nBeijing already deploys more than 500 million surveillance cameras nationwide – more than half the world's total – and this, combined with AI-powered facial-recognition software, is transforming life for ordinary Chinese people in frankly dystopian ways.\\nChina has developed and used 'gait-recognition' software, in which cameras can distinguish individuals' walks and body shapes even when their faces are obscured.\\nMinority groups such as the Uyghurs are persecuted in the regime's internment camps, watched by overseeing eyes.\\nAnd even lavatories are no longer private, as some public conveniences will dispense loo roll only to those who pass a facial- recognition scan.\\nMr Sunak is being commendably hard-headed, despite these abuses. Excluding China from a conference on AI would make no sense. Beijing will develop advanced AI whether it comes to Bletchley Park or not – whatever we might think about it.\\nChinese absence from the conference would mean that there could be no genuinely global framework for managing the biggest risks posed by the technology.\\nAt present, even states with powerful cyber capabilities think twice before unleashing computer viruses that could cause a devastating worldwide IT meltdown. The consequences are simply too unpredictable.\\nBut a terrorist group would likely have no such compunction. And AI, which may well come to write computer code far better than any human, could certainly make this job easier for them. China's leaders are nothing if not calculating: they have no more interest in allowing terrorists to exploit dangerous technology than we do. So a realistic approach is needed.\\nThere is a lesson from history here.\\nSome 40 years ago, when the internet was in its infancy created, its early pioneers failed to predict that people would intentionally take advantage of the network they were creating to commit serious crime.\\nWe pay the price of that complacency on a daily basis with cyber-attacks and other online harms.\\nBy gathering world leaders together at the dawn of a new digital age, Rishi Sunak is trying to ensure that we don't make the same mistake.\\nThe comments below have not been moderated.\\nThe views expressed in the contents above are those of our users and do not necessarily reflect the views of MailOnline.\\nWe are no longer accepting comments on this article.\""
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "downloaded = trafilatura.fetch_url(mylist[6])\n",
    "trafilatura.extract(downloaded, favour_precision=True, include_comments=False, include_images=False, include_tables=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T23:41:04.326629Z",
     "start_time": "2023-11-06T23:41:03.892219Z"
    }
   },
   "id": "cf44eccaf4fc6b04"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''Article is saved as text file anyway'''\n",
    "# downloaded = trafilatura.fetch_url('https://www.dailymail.co.uk/news/article-12350195/Think-twice-imposing-Ulez-tax-hard-working-Brits-Rishi-Sunak-blasts-Sadiq-Khan-hated-low-emission-zone-expansion-plan.html?ns_mchannel=rss&ns_campaign=1490&ito=1490')\n",
    "# article_text = trafilatura.extract(downloaded)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "67fec4cf3f288e54"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Saving article to file - useful for demo/consistent experimental purposes - if article is deleted or edited my copy will stay the same."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "704e42e89fc6ad03"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# file_path = \"ULEZ-daily-mail-research-article.txt\"\n",
    "# \n",
    "# # Write the text to the file\n",
    "# with open(file_path, 'w', encoding='utf-8') as file:\n",
    "#     file.write(article_text)\n",
    "# \n",
    "# print(f\"Article text saved to {file_path}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "99dd3acdab59d29d"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article text read from ULEZ-daily-mail-research-article.txt:\n",
      "'Think twice before imposing Ulez tax on hard-working Brits': Rishi Sunak blasts Sadiq Khan for his hated low emission zone expansion plan\n",
      "- Keir Starmer told to 'get off the fence' over Sadiq Khan's Ulez by senior Tory MPs\n",
      "- Rishi Sunak led MPs asking Starmer to get London mayor to postpone the levy\n",
      "- READ MORE: Labour civil war over ULEZ intensifies after Khan's High Court win\n",
      "Keir Starmer faced fresh calls last night for him to try to delay the expansion of London's clean air zone.\n",
      "Senior Tories said he should 'get off the fence' and force the capital's mayor Sadiq Khan to protect motorists set to pay the Ulez levy.\n",
      "The High Court yesterday backed the extension of Ulez into outer London – a move that could lead to similar schemes elsewhere in Britain.\n",
      "Sir Keir has refused to say whether the zone should expand, claiming it is not a 'simple political decision'. He remained silent after yesterday's court ruling.\n",
      "Rishi Sunak last night led MPs and campaigners asking the Labour leader to make Mr Khan either postpone the £12.50 daily levy for older, more polluting vehicles or ditch it entirely. The zone expands on August 29.\n",
      "Keir Starmer (left) faced fresh calls last night for him to try to delay the expansion of London mayor Sadiq Khan's (right) Ulez\n",
      "Rishi Sunak (pictured) last night led MPs and campaigners asking the Labour leader to make Mr Khan either postpone the £12.50 daily levy for older, more polluting vehicles or ditch it\n",
      "The Prime Minister said: 'Sir Keir Starmer's mayor should think twice before choosing to press ahead with imposing this Ulez tax on hard-working people.\n",
      "READ MORE: Labour civil war over ULEZ intensifies as shadow health secretary says Starmer 'doesn't want expansion to happen' but Sadiq Khan vows to extend £12.50 daily charge from 29 August after High Court win\n",
      "It will mean ordinary people will have to pay £12.50 every time they take their kids to school, go to the doctors or to do their weekly food shop. And at a time of high inflation and cost of living that would be an incredibly unnecessary burden to inflict on hardworking families.'\n",
      "Mr Khan's plans, which will see nearly 700,000 cars across Greater London pay the daily charge, according to the RAC, have caused outrage among some families and tradesmen who rely on their vehicles.\n",
      "More than 1.5million people living outside London could also be affected by the expanded Ulez zone, data from the Driver and Vehicle Licensing Agency revealed this year.\n",
      "Responding to yesterday's ruling, Mr Khan insisted nine out of ten cars in outer London would not need to pay the charge, adding: 'The decision to expand the Ulez was very difficult and not something I took lightly and I continue to do everything possible to address any concerns Londoners may have.\n",
      "'This unambiguous decision today in the High Court allows us to press on with the difficult but vital task of cleaning up London's air and tackling the climate crisis.'\n",
      "Senior Labour figures blamed Ulez for their failure to win last week's by-election in Uxbridge, Boris Johnson's old seat in west London. Sir Keir said last weekend: 'We are doing something very wrong if policies put forward by the Labour party end up on each and every Tory leaflet. We've got to face up to that and learn the lessons.'\n",
      "But on Wednesday the Labour leader dodged a question on whether the Ulez extension should proceed, saying it was up to Mr Khan to decide whether to press on with the policy.\n",
      "Shadow Chancellor Rachel Reeves this week said it was 'not the right time' to expand Ulez in London. The party's health spokesman, Wes Streeting, told Times Radio: 'Keir has been very clear that he doesn't want it to go ahead at this stage, as has Rachel Reeves.\n",
      "'I would agree with them. But Sadiq is the mayor of London. He doesn't answer to us, he answers to Londoners. If you believe in devolution, you believe in his right to do that. We're going to have to take it on the chin. And he's going to take the criticism on the chin and we'll see what happens.'\n",
      "Energy Secretary Grant Shapps said the issue would show what type of leader Sir Keir was: 'Time to get off the fence and tell your mayor to do the right thing and stop the Ulez expansion.'\n",
      "'Ulez premium' on used cars\n",
      "Drivers buying used cars that will avoid daily fees from the planned expansion of London's ultra-low emission zone are being hit by a 'price penalty', analysis suggests.\n",
      "Auto Trader, an online marketplace, said some motorists were being charged over £3,000 more for Ulez-compliant vehicles compared with identical models only a year older that do not meet the emissions standards.\n",
      "To avoid the £12.50 daily fee for driving in the Ulez area, diesel cars must generally have been first registered after September 2015, while most petrol cars registered after 2005 are also exempt. Auto Trader said the biggest price gap across the UK involved a used Volkswagen Golf 2016 model being sold for £13,046 – £3,601 more than a 2015 non-compliant price of £9,445.\n",
      "The company's Ian Plummer said: 'Drivers are having to pay a price penalty to follow the rules. This doesn't need to be a case of pocket over planet, it is possible to achieve both. But it's vital we get the balance right between the carrots and the sticks.'\n",
      "Culture Secretary Lucy Frazer added: 'Sir Keir needs to do the right thing, and tell the Labour mayor to ditch his Ulez expansion. It will only punish hardworking Londoners.'\n",
      "Transport Secretary Mark Harper said it 'wasn't very clear that this delivers against emissions targets at all... this is about raising money – that is what people can see and that is what they object to'.\n",
      "He added: 'The expansion of Ulez to the whole of Greater London does not make sense and we are not in favour of it.'\n",
      "The High Court challenge, which was brought by the London boroughs of Bexley, Bromley, Harrow and Hillingdon along with Surrey County Council, was defeated on the grounds that Mr Khan's Ulez policy was an expansion.\n",
      "Mr Justice Swift said: 'I am satisfied that the mayor's decision to expand the Ulez area by amendment of the present road charging scheme, rather than by making an entirely new... scheme, was within his powers.' He said the consultation process gave enough information to produce informed responses.\n",
      "Responding to the ruling in a joint press release the five councils questioned whether Mr Khan had a 'moral right' to expand the scheme.\n",
      "Seven other areas have clean air zones that charge motorists in England: Bath, Birmingham, Bradford, Bristol, Portsmouth, Sheffield, and Newcastle and Gateshead. Labour mayor Andy Burnham is considering one in Manchester. In Scotland there is already a fully-operating low emissions zone in Glasgow, with similar schemes due to start in Edinburgh, Dundee and Aberdeen next year.\n",
      "Chris Clarkson, the Tory MP for Heywood and Middleton in Greater Manchester, said the court ruling had serious implications for motorists across England, especially in his constituency.\n",
      "Anti-ULEZ protestors demonstrate outside The Royal Courts of Justice ahead of the ruling on the expansion of London's clean air zone\n",
      "The AA urged Mr Khan to give drivers 'more time to react' to the change or reduce the impact on them, while the RAC called on the mayor to give additional support to key workers.\n",
      "But climate charity Possible described the judgment as 'fantastic', arguing that Ulez expansion was 'sorely needed to tackle our overreliance on cars and improve air quality'.\n",
      "From Monday, access to the mayor's £110million scheme which provides grants supporting the scrapping of non Ulez-compliant vehicles in London will be extended.\n",
      "It will now include all families in receipt of child benefit and every small business.\n",
      "The comments below have not been moderated.\n",
      "The views expressed in the contents above are those of our users and do not necessarily reflect the views of MailOnline.\n",
      "We are no longer accepting comments on this article.\n"
     ]
    }
   ],
   "source": [
    "file_path = \"ULEZ-daily-mail-research-article.txt\"\n",
    "\n",
    "# Read the text from the file\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    article_text = file.read()\n",
    "\n",
    "# Now, article_text contains the text you read from the file\n",
    "print(f\"Article text read from {file_path}:\\n{article_text}\")    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T23:41:08.427795Z",
     "start_time": "2023-11-06T23:41:08.407476Z"
    }
   },
   "id": "aab3850c8fc7d602"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "import torch\n",
    "import spacy"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T23:41:12.005456Z",
     "start_time": "2023-11-06T23:41:11.986123Z"
    }
   },
   "id": "b038e7403d136218"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "# Check Metal Performance Shaders (MPS) backend for GPU training acceleration.\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T23:41:19.605615Z",
     "start_time": "2023-11-06T23:41:19.572440Z"
    }
   },
   "id": "d1d8304cf8207cc1"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/06/2023 23:41:25 - INFO - \t missing_keys: []\n",
      "11/06/2023 23:41:25 - INFO - \t unexpected_keys: []\n",
      "11/06/2023 23:41:25 - INFO - \t mismatched_keys: []\n",
      "11/06/2023 23:41:25 - INFO - \t error_msgs: []\n",
      "11/06/2023 23:41:25 - INFO - \t Model Parameters: 90.5M, Transformer: 82.1M, Coref head: 8.4M\n",
      "11/06/2023 23:41:25 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/1 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ac6b4a4bab2541409c355dad7d60df50"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/06/2023 23:41:26 - INFO - \t ***** Running Inference on 1 texts *****\n"
     ]
    },
    {
     "data": {
      "text/plain": "Inference:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cd4ed11417f941fc88ae213fc60c6e42"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['We', 'our'], ['our coref package', 'This package']]\n"
     ]
    },
    {
     "data": {
      "text/plain": "18.852913"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fastcoref import FCoref\n",
    "\n",
    "model = FCoref(device='mps')\n",
    "\n",
    "preds = model.predict(\n",
    "   texts=['We are so happy to see you using our coref package. This package is very fast!']\n",
    ")\n",
    "\n",
    "preds[0].get_clusters(as_strings=False)\n",
    "\n",
    "\n",
    "print(preds[0].get_clusters())\n",
    "\n",
    "\n",
    "preds[0].get_logit(\n",
    "   span_i=(33, 50), span_j=(52, 64)\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T23:41:26.081770Z",
     "start_time": "2023-11-06T23:41:24.314986Z"
    }
   },
   "id": "afda5c52bc99f688"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Playing around with extracting coreference clusters using fastcoref directly\n",
    "\"Processing can be applied to a collection of texts of any length in a batched and parallel fashion\"\n",
    "https://pypi.org/project/fastcoref/"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cd4d34389e2404f7"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/06/2023 23:36:38 - INFO - \t missing_keys: []\n",
      "11/06/2023 23:36:38 - INFO - \t unexpected_keys: []\n",
      "11/06/2023 23:36:38 - INFO - \t mismatched_keys: []\n",
      "11/06/2023 23:36:38 - INFO - \t error_msgs: []\n",
      "11/06/2023 23:36:38 - INFO - \t Model Parameters: 90.5M, Transformer: 82.1M, Coref head: 8.4M\n",
      "11/06/2023 23:36:38 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/1 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "95bdd32ebac24f448ec3e1c038348c53"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/06/2023 23:36:38 - INFO - \t ***** Running Inference on 1 texts *****\n"
     ]
    },
    {
     "data": {
      "text/plain": "Inference:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a25d9288751c4ba58644efbc7a5a719f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Sadiq Khan', 'his', \"Sadiq Khan's\", 'London mayor', \"Khan's\", \"the capital's mayor Sadiq Khan\", 'Mr Khan', \"London mayor Sadiq Khan's\", 'Mr Khan', \"Sir Keir Starmer's mayor\", 'Sadiq Khan', \"Mr Khan's\", 'Mr Khan', 'I', 'I', 'Mr Khan', 'Sadiq', 'He', 'he', 'his', 'he', 'your mayor', 'the Labour mayor', 'his', \"Mr Khan's\", \"the mayor's\", 'his', 'Mr Khan', 'Mr Khan', 'the mayor', \"the mayor's\"], ['Rishi Sunak', 'Rishi Sunak', 'Rishi Sunak', 'Rishi Sunak (pictured)'], ['Keir Starmer', 'Starmer', 'Keir Starmer', 'him', 'he', 'Sir Keir', 'He', 'the Labour leader', 'Keir Starmer (left)', 'him', 'the Labour leader', \"Sir Keir Starmer's\", 'Starmer', 'Sir Keir', 'the Labour leader', 'Keir', 'he', 'Sir Keir', 'Sir Keir'], [\"Sadiq Khan's Ulez\", 'ULEZ', 'ULEZ'], ['London', \"London's\", \"the capital's\", 'London', \"London's\", 'London', 'London', \"London's\", 'London', \"London's\", 'London'], ['the levy', 'the Ulez levy', 'the £12.50 daily levy for older, more polluting vehicles', 'it', 'the £12.50 daily levy for older, more polluting vehicles', 'it', 'this Ulez tax on hard-working people'], ['High Court', 'The High Court', 'High Court', 'the High Court', 'High Court'], [\"London's clean air zone\", 'Ulez', 'the zone', 'The zone', \"London mayor Sadiq Khan's (right) Ulez\", 'the expanded Ulez zone', 'the Ulez', 'Ulez', 'Ulez', 'Ulez', 'Ulez', 'Ulez', 'Ulez', 'the scheme', \"London's clean air zone\", 'Ulez'], ['yesterday', \"yesterday's\", \"yesterday's\"], ['last night', 'last night', 'last night', 'last night'], ['Labour', 'Labour', 'Labour', 'Labour', 'Labour', 'the Labour party', 'Labour', \"The party's\", 'Labour', 'Labour'], [\"the expansion of London's clean air zone\", \"the expansion of London mayor Sadiq Khan's (right) Ulez\", 'expansion', 'the Ulez extension', 'the policy', 'expand', 'it', 'the Ulez expansion', 'his Ulez expansion', 'It', 'this', 'this', 'expand', 'expand', \"the expansion of London's clean air zone\", 'the change', 'Ulez expansion'], ['August 29', '29 August'], ['extend', 'It'], ['ordinary people', 'they', 'their', 'their'], ['pay', 'that'], [\"yesterday's court ruling\", \"yesterday's ruling\", 'This unambiguous decision today in the High Court', 'the ruling', 'the court ruling', \"the ruling on the expansion of London's clean air zone\", 'the judgment'], ['outer London', 'outer London'], ['the daily charge', 'the charge'], ['Ulez', 'their'], ['We', 'We'], ['end', 'that'], ['Shadow Chancellor Rachel Reeves', 'Rachel Reeves', 'Mr Justice Swift', 'I', 'He'], [\"The party's health spokesman, Wes Streeting,\", 'I'], ['devolution', 'that'], [\"London's ultra-low emission zone\", 'the Ulez area', 'the Ulez area'], ['Auto Trader, an online marketplace,', 'Auto Trader', \"The company's\"], ['Britain', 'the UK', 'England', 'England'], ['raising', 'that', 'that'], ['people', 'they'], ['Transport Secretary Mark Harper', 'He'], ['The expansion of Ulez to the whole of Greater London', 'it'], ['the London boroughs of Bexley, Bromley, Harrow and Hillingdon along with Surrey County Council', 'the five councils'], ['Chris Clarkson, the Tory MP for Heywood and Middleton in Greater Manchester', 'his'], ['drivers', 'them'], ['the RAC', 'the RAC'], [\"access to the mayor's £110million scheme which provides grants supporting the scrapping of non Ulez-compliant vehicles in London\", 'It'], ['our', 'MailOnline', 'We']]\n",
      "Cluster 1 : ['Sadiq Khan', 'his', \"Sadiq Khan's\", 'London mayor', \"Khan's\", \"the capital's mayor Sadiq Khan\", 'Mr Khan', \"London mayor Sadiq Khan's\", 'Mr Khan', \"Sir Keir Starmer's mayor\", 'Sadiq Khan', \"Mr Khan's\", 'Mr Khan', 'I', 'I', 'Mr Khan', 'Sadiq', 'He', 'he', 'his', 'he', 'your mayor', 'the Labour mayor', 'his', \"Mr Khan's\", \"the mayor's\", 'his', 'Mr Khan', 'Mr Khan', 'the mayor', \"the mayor's\"]\n",
      "Cluster Text Count: 31\n",
      "Positions: [(81, 91), (96, 99), (187, 199), (268, 280), (360, 366), (549, 579), (1006, 1013), (1230, 1255), (1360, 1367), (1479, 1503), (1735, 1745), (2099, 2108), (2503, 2510), (2669, 2670), (2688, 2689), (3393, 3400), (3741, 3746), (3771, 3773), (3796, 3798), (3866, 3869), (3936, 3938), (4136, 4146), (5326, 5342), (5352, 5355), (5915, 5924), (5999, 6010), (6154, 6157), (6342, 6349), (7103, 7110), (7216, 7225), (7468, 7479)]\n",
      "Cluster 2 : ['Keir Starmer', 'Starmer', 'Keir Starmer', 'him', 'he', 'Sir Keir', 'He', 'the Labour leader', 'Keir Starmer (left)', 'him', 'the Labour leader', \"Sir Keir Starmer's\", 'Starmer', 'Sir Keir', 'the Labour leader', 'Keir', 'he', 'Sir Keir', 'Sir Keir']\n",
      "Cluster Text Count: 19\n",
      "Positions: [(141, 153), (253, 260), (382, 394), (428, 431), (509, 511), (765, 773), (875, 877), (980, 997), (1140, 1159), (1193, 1196), (1334, 1351), (1479, 1497), (1688, 1695), (3075, 3083), (3291, 3308), (3613, 3617), (3643, 3645), (4086, 4094), (5279, 5287)]\n",
      "Cluster 3 : [\"the expansion of London's clean air zone\", \"the expansion of London mayor Sadiq Khan's (right) Ulez\", 'expansion', 'the Ulez extension', 'the policy', 'expand', 'it', 'the Ulez expansion', 'his Ulez expansion', 'It', 'this', 'this', 'expand', 'expand', \"the expansion of London's clean air zone\", 'the change', 'Ulez expansion']\n",
      "Cluster Text Count: 17\n",
      "Positions: [(448, 488), (1213, 1268), (1710, 1719), (3338, 3356), (3436, 3446), (3526, 3532), (3659, 3661), (4178, 4196), (5352, 5370), (5372, 5374), (5480, 5484), (5530, 5534), (6023, 6029), (6373, 6379), (7049, 7089), (7151, 7161), (7350, 7364)]\n",
      "Cluster 4 : [\"London's clean air zone\", 'Ulez', 'the zone', 'The zone', \"London mayor Sadiq Khan's (right) Ulez\", 'the expanded Ulez zone', 'the Ulez', 'Ulez', 'Ulez', 'Ulez', 'Ulez', 'Ulez', 'Ulez', 'the scheme', \"London's clean air zone\", 'Ulez']\n",
      "Cluster Text Count: 16\n",
      "Positions: [(465, 488), (676, 680), (801, 809), (1109, 1117), (1230, 1268), (2375, 2397), (2623, 2631), (3342, 3346), (3533, 3537), (4182, 4186), (5356, 5360), (5650, 5654), (5925, 5929), (6380, 6390), (7066, 7089), (7350, 7354)]\n",
      "Cluster 5 : ['London', \"London's\", \"the capital's\", 'London', \"London's\", 'London', 'London', \"London's\", 'London', \"London's\", 'London']\n",
      "Cluster Text Count: 11\n",
      "Positions: [(268, 274), (465, 473), (549, 562), (2342, 2348), (2891, 2899), (3541, 3547), (3763, 3769), (4309, 4317), (5790, 5796), (7066, 7074), (7580, 7586)]\n",
      "Cluster 6 : ['Labour', 'Labour', 'Labour', 'Labour', 'Labour', 'the Labour party', 'Labour', \"The party's\", 'Labour', 'Labour']\n",
      "Cluster Text Count: 10\n",
      "Positions: [(315, 321), (984, 990), (1338, 1344), (1617, 1623), (2945, 2951), (3165, 3181), (3295, 3301), (3549, 3560), (5330, 5336), (6554, 6560)]\n",
      "Cluster 7 : ['the levy', 'the Ulez levy', 'the £12.50 daily levy for older, more polluting vehicles', 'it', 'the £12.50 daily levy for older, more polluting vehicles', 'it', 'this Ulez tax on hard-working people']\n",
      "Cluster Text Count: 7\n",
      "Positions: [(293, 301), (612, 625), (1030, 1086), (1096, 1098), (1384, 1440), (1450, 1452), (1568, 1604)]\n",
      "Cluster 8 : [\"yesterday's court ruling\", \"yesterday's ruling\", 'This unambiguous decision today in the High Court', 'the ruling', 'the court ruling', \"the ruling on the expansion of London's clean air zone\", 'the judgment']\n",
      "Cluster Text Count: 7\n",
      "Positions: [(900, 924), (2483, 2501), (2770, 2819), (6269, 6279), (6853, 6869), (7035, 7089), (7308, 7320)]\n",
      "Cluster 9 : ['High Court', 'The High Court', 'High Court', 'the High Court', 'High Court']\n",
      "Cluster Text Count: 5\n",
      "Positions: [(367, 377), (627, 641), (1802, 1812), (2805, 2819), (5743, 5753)]\n",
      "Cluster 10 : ['Shadow Chancellor Rachel Reeves', 'Rachel Reeves', 'Mr Justice Swift', 'I', 'He']\n",
      "Cluster Text Count: 5\n",
      "Positions: [(3448, 3479), (3696, 3709), (5955, 5971), (5979, 5980), (6167, 6169)]\n",
      "Cluster 11 : ['Rishi Sunak', 'Rishi Sunak', 'Rishi Sunak', 'Rishi Sunak (pictured)']\n",
      "Cluster Text Count: 4\n",
      "Positions: [(62, 73), (226, 237), (926, 937), (1269, 1291)]\n",
      "Cluster 12 : ['last night', 'last night', 'last night', 'last night']\n",
      "Cluster Text Count: 4\n",
      "Positions: [(413, 423), (938, 948), (1178, 1188), (1292, 1302)]\n",
      "Cluster 13 : ['ordinary people', 'they', 'their', 'their']\n",
      "Cluster Text Count: 4\n",
      "Positions: [(1830, 1845), (1881, 1885), (1891, 1896), (1940, 1945)]\n",
      "Cluster 14 : ['Britain', 'the UK', 'England', 'England']\n",
      "Cluster Text Count: 4\n",
      "Positions: [(756, 763), (4852, 4858), (6456, 6463), (6916, 6923)]\n",
      "Cluster 15 : [\"Sadiq Khan's Ulez\", 'ULEZ', 'ULEZ']\n",
      "Cluster Text Count: 3\n",
      "Positions: [(187, 204), (337, 341), (1639, 1643)]\n",
      "Cluster 16 : ['yesterday', \"yesterday's\", \"yesterday's\"]\n",
      "Cluster Text Count: 3\n",
      "Positions: [(642, 651), (900, 911), (2483, 2494)]\n",
      "Cluster 17 : [\"London's ultra-low emission zone\", 'the Ulez area', 'the Ulez area']\n",
      "Cluster Text Count: 3\n",
      "Positions: [(4309, 4341), (4653, 4666), (6030, 6043)]\n",
      "Cluster 18 : ['Auto Trader, an online marketplace,', 'Auto Trader', \"The company's\"]\n",
      "Cluster Text Count: 3\n",
      "Positions: [(4397, 4432), (4806, 4817), (4982, 4995)]\n",
      "Cluster 19 : ['raising', 'that', 'that']\n",
      "Cluster Text Count: 3\n",
      "Positions: [(5544, 5551), (5560, 5564), (5592, 5596)]\n",
      "Cluster 20 : ['our', 'MailOnline', 'We']\n",
      "Cluster Text Count: 3\n",
      "Positions: [(7791, 7794), (7845, 7855), (7857, 7859)]\n",
      "Cluster 21 : ['August 29', '29 August']\n",
      "Cluster Text Count: 2\n",
      "Positions: [(1129, 1138), (1786, 1795)]\n",
      "Cluster 22 : ['extend', 'It']\n",
      "Cluster Text Count: 2\n",
      "Positions: [(1754, 1760), (1817, 1819)]\n",
      "Cluster 23 : ['pay', 'that']\n",
      "Cluster Text Count: 2\n",
      "Positions: [(1859, 1862), (2015, 2019)]\n",
      "Cluster 24 : ['outer London', 'outer London']\n",
      "Cluster Text Count: 2\n",
      "Positions: [(686, 698), (2544, 2556)]\n",
      "Cluster 25 : ['the daily charge', 'the charge']\n",
      "Cluster Text Count: 2\n",
      "Positions: [(2177, 2193), (2579, 2589)]\n",
      "Cluster 26 : ['Ulez', 'their']\n",
      "Cluster Text Count: 2\n",
      "Positions: [(2967, 2971), (2976, 2981)]\n",
      "Cluster 27 : ['We', 'We']\n",
      "Cluster Text Count: 2\n",
      "Positions: [(3104, 3106), (3221, 3223)]\n",
      "Cluster 28 : ['end', 'that']\n",
      "Cluster Text Count: 2\n",
      "Positions: [(3182, 3185), (3245, 3249)]\n",
      "Cluster 29 : [\"The party's health spokesman, Wes Streeting,\", 'I']\n",
      "Cluster Text Count: 2\n",
      "Positions: [(3549, 3593), (3712, 3713)]\n",
      "Cluster 30 : ['devolution', 'that']\n",
      "Cluster Text Count: 2\n",
      "Positions: [(3839, 3849), (3882, 3886)]\n",
      "Cluster 31 : ['people', 'they']\n",
      "Cluster Text Count: 2\n",
      "Positions: [(5573, 5579), (5605, 5609)]\n",
      "Cluster 32 : ['Transport Secretary Mark Harper', 'He']\n",
      "Cluster Text Count: 2\n",
      "Positions: [(5416, 5447), (5622, 5624)]\n",
      "Cluster 33 : ['The expansion of Ulez to the whole of Greater London', 'it']\n",
      "Cluster Text Count: 2\n",
      "Positions: [(5633, 5685), (5734, 5736)]\n",
      "Cluster 34 : ['the London boroughs of Bexley, Bromley, Harrow and Hillingdon along with Surrey County Council', 'the five councils']\n",
      "Cluster Text Count: 2\n",
      "Positions: [(5786, 5880), (6305, 6322)]\n",
      "Cluster 35 : ['Chris Clarkson, the Tory MP for Heywood and Middleton in Greater Manchester', 'his']\n",
      "Cluster Text Count: 2\n",
      "Positions: [(6771, 6846), (6939, 6942)]\n",
      "Cluster 36 : ['drivers', 'them']\n",
      "Cluster Text Count: 2\n",
      "Positions: [(7119, 7126), (7186, 7190)]\n",
      "Cluster 37 : ['the RAC', 'the RAC']\n",
      "Cluster Text Count: 2\n",
      "Positions: [(2208, 2215), (7198, 7205)]\n",
      "Cluster 38 : [\"access to the mayor's £110million scheme which provides grants supporting the scrapping of non Ulez-compliant vehicles in London\", 'It']\n",
      "Cluster Text Count: 2\n",
      "Positions: [(7458, 7586), (7605, 7607)]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "I am performing coreference resolution using fastcoref on a given text and then analysing and printing information about the resolved coreference clusters. \n",
    "This helps me identify and understand how words or phrases in the text refer to the same entities,\n",
    "and I am prioritising clusters based on their text content length.\n",
    "\n",
    "Sorted_combined_clusters will be used later with the NER resulted from spaCy.\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "from fastcoref import FCoref\n",
    "\n",
    "model = FCoref(device='mps')\n",
    "\n",
    "preds = model.predict(\n",
    "   texts=[article_text]\n",
    ")\n",
    "\n",
    "clustersText = preds[0].get_clusters()\n",
    "clustersPositions = preds[0].get_clusters(as_strings=False)\n",
    "\n",
    "print(clustersText)\n",
    "\n",
    "combined_clusters = [(text, positions, len(text)) for text, positions in zip(clustersText, clustersPositions)]\n",
    "\n",
    "sorted_combined_clusters = sorted(combined_clusters, key=lambda x: x[2], reverse=True)\n",
    "\n",
    "for i, (clusterText, clusterPos, cluster_text_count) in enumerate(sorted_combined_clusters, 1):\n",
    "    print(f\"Cluster {i} : {clusterText}\")\n",
    "    print(f\"Cluster Text Count: {cluster_text_count}\")\n",
    "    print(\"Positions:\", clusterPos)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T23:36:38.824287Z",
     "start_time": "2023-11-06T23:36:36.786549Z"
    }
   },
   "id": "eea537c22abe6e0d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "LingMess Coref - alternative state-of-the-art coreference resolution but doesn't want to play with my M1 Mac GPU so have to fall back to slower CPU device :("
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9af7c02e08c807ba"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/06/2023 23:42:30 - INFO - \t missing_keys: []\n",
      "11/06/2023 23:42:30 - INFO - \t unexpected_keys: []\n",
      "11/06/2023 23:42:30 - INFO - \t mismatched_keys: []\n",
      "11/06/2023 23:42:30 - INFO - \t error_msgs: []\n",
      "11/06/2023 23:42:30 - INFO - \t Model Parameters: 590.0M, Transformer: 434.6M, Coref head: 155.4M\n",
      "11/06/2023 23:42:30 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/1 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d9cd532938944171aaeb7035e50824e9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/06/2023 23:42:30 - INFO - \t ***** Running Inference on 1 texts *****\n"
     ]
    },
    {
     "data": {
      "text/plain": "Inference:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3ea572aee39d44a2aa67ccff83a30515"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Sadiq Khan', 'his', \"Sadiq Khan's\", 'London mayor', \"Khan's\", \"the capital's mayor Sadiq Khan\", 'Mr Khan', \"London mayor Sadiq Khan's\", 'Mr Khan', \"Sir Keir Starmer's mayor\", 'Sadiq Khan', \"Mr Khan's\", 'Mr Khan', 'I', 'I', 'Sir Keir', 'the Labour leader', 'Mr Khan', 'Keir', 'he', 'Sadiq', 'He', 'he', 'his', 'he', 'Sir Keir', 'your mayor', 'Sir Keir', 'the Labour mayor', 'his', \"Mr Khan's\", \"the mayor's\", 'his', 'Mr Khan', 'Mr Khan', 'the mayor', \"the mayor's\"], ['Rishi Sunak', 'Rishi Sunak', 'Rishi Sunak', 'Rishi Sunak (pictured)'], ['Keir Starmer', 'Starmer', 'Keir Starmer', 'him', 'he', 'Sir Keir', 'He', 'the Labour leader', 'Keir Starmer (left)', 'him', 'the Labour leader', \"Sir Keir Starmer's\", 'Starmer'], ['Ulez', 'ULEZ', 'Ulez', 'Ulez', 'Ulez', 'ULEZ', 'Ulez', 'the Ulez', 'Ulez', 'Ulez', 'the policy', 'Ulez', 'Ulez', 'Ulez', 'Ulez', 'Ulez', 'Ulez', 'Ulez', 'the scheme', 'Ulez'], ['London', \"London's\", \"the capital's\", 'London', 'London', \"London's\", 'London', 'London', \"London's\", 'London', \"London's\", 'London'], ['the levy', 'the Ulez levy', 'the £12.50 daily levy for older, more polluting vehicles', 'it', 'the £12.50 daily levy for older, more polluting vehicles', 'it', 'this Ulez tax', '£12.50 daily charge', 'the daily charge', 'the charge'], ['High Court', 'The High Court', 'High Court', 'the High Court', 'High Court'], [\"London's clean air zone\", 'the zone', 'The zone', \"London's clean air zone\"], ['yesterday', \"yesterday's\", \"yesterday's\"], ['last night', 'last night', 'last night', 'last night'], ['Labour', 'Labour', 'Labour', 'Labour', 'Labour', 'their', 'We', 'the Labour party', 'We', 'Labour', \"The party's\", 'us'], [\"the expansion of London's clean air zone\", \"the expansion of London mayor Sadiq Khan's (right) Ulez\"], ['August 29', '29 August'], ['extend', 'It'], ['ordinary people', 'they', 'their', 'their'], ['pay', 'that'], ['some families and tradesmen who rely on their vehicles', 'their'], ['his hated low emission zone expansion plan', 'the expanded Ulez zone', 'the Ulez extension', 'expand', 'it', 'the Ulez expansion', \"the planned expansion of London's ultra-low emission zone\", 'his Ulez expansion', 'It', 'this', 'this', 'expand', 'expand', \"the expansion of London's clean air zone\", 'the change'], [\"yesterday's ruling\", 'This unambiguous decision today in the High Court', 'the ruling', 'the court ruling', \"the ruling on the expansion of London's clean air zone\", 'the judgment'], ['end', 'that'], ['Shadow Chancellor Rachel Reeves', 'Rachel Reeves'], ['answers', 'that'], ['Auto Trader, an online marketplace,', 'Auto Trader', \"The company's\"], ['raising', 'that', 'that'], ['people', 'they'], ['Transport Secretary Mark Harper', 'He'], ['The expansion of Ulez to the whole of Greater London', 'it'], ['Mr Justice Swift', 'I', 'He'], ['the Ulez area', 'the Ulez area'], ['the London boroughs of Bexley, Bromley, Harrow and Hillingdon along with Surrey County Council', 'the five councils'], ['England', 'England'], ['Chris Clarkson, the Tory MP for Heywood and Middleton in Greater Manchester,', 'his'], ['drivers', 'them'], ['the RAC', 'the RAC'], [\"access to the mayor's £110million scheme which provides grants supporting the scrapping of non Ulez-compliant vehicles in London\", 'It'], ['MailOnline', 'We']]\n"
     ]
    }
   ],
   "source": [
    "from fastcoref import LingMessCoref\n",
    "\n",
    "model = LingMessCoref(device='cpu')\n",
    "\n",
    "preds = model.predict(\n",
    "   texts=[article_text]\n",
    ")\n",
    "\n",
    "print(preds[0].get_clusters()) "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T23:42:35.617953Z",
     "start_time": "2023-11-06T23:42:24.538204Z"
    }
   },
   "id": "94845f90532cce8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "But to my current knowledge: The spaCy library itself does not natively support processing an array of text documents in a single call in parallel.\n",
    "So consider applying fastCoref directly then using spaCy or alternative to extract sentences"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "734c78ff67183220"
  },
  {
   "cell_type": "markdown",
   "source": [
    "https://medium.com/quantrium-tech/top-3-packages-for-named-entity-recognition-e9e14f6f0a2a\n",
    "    \n",
    "And so decided to play around with flair"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d519a462ba302ca9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Spacy NER recommended so applied here:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b3d30f6b6e9aa159"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Possible NER outcomes: CARDINAL, DATE, EVENT, FAC, GPE, LANGUAGE, LAW, LOC, MONEY, NORP, ORDINAL, ORG, PERCENT, PERSON, PRODUCT, QUANTITY, TIME, WORK_OF_ART\n",
    "https://huggingface.co/spacy/en_core_web_sm"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3893f71904635dcf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "NER could be confused and additional mentions could be used to determine \"correct\" category"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T23:43:47.275302Z",
     "start_time": "2023-11-06T23:43:47.252760Z"
    }
   },
   "id": "7fe2ad238fecc72"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "'''The insert_intervals function enables the segmentation of sentences provided by TextBlob (as shown in the cell below) to be further divided into smaller boundaries. This division is based on the identification of specific points where it is deemed necessary to split sentences, particularly within the context of news articles.'''\n",
    "\n",
    "def insert_intervals(initial_list, new_values):\n",
    "    def insert_recursive(intervals, values):\n",
    "        if not values:\n",
    "            return intervals # Base case: Return the intervals when there are no more values to insert.\n",
    "\n",
    "        value = values[0]\n",
    "        result = []\n",
    "        for interval in intervals:\n",
    "            if interval[0] <= value <= interval[1]:\n",
    "                # If the value falls within an existing interval, split the interval into two parts.\n",
    "                # The first part goes from the interval's start to the value (inclusive), and\n",
    "                # the second part goes from the value+1 to the interval's end.\n",
    "                if interval[0] < value:\n",
    "                    result.append((interval[0], value))         # To mess around with intervals change value + - offset here.\n",
    "                if value < interval[1]:\n",
    "                    result.append((value+1, interval[1]))       # To mess around with intervals change value + - offset here.\n",
    "            else:\n",
    "                # If the value doesn't fall within the interval, keep the interval as is.\n",
    "                result.append(interval)\n",
    "        return insert_recursive(result, values[1:]) # Recursively process other values.\n",
    "\n",
    "    updated_list = insert_recursive(initial_list, new_values) # Recursive function call\n",
    "    return updated_list"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T23:43:47.833731Z",
     "start_time": "2023-11-06T23:43:47.811057Z"
    }
   },
   "id": "1c3b8e91b8e4074d"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "'''Process the article text, tokenize by sentence and add custom adjustments to the tokenization using insert intervals above'''\n",
    "\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "\n",
    "# spaCy was not satisfactory for accurately tokenizing for sentence start / end characters. Trying textblob instead...\n",
    "def textblob_custom_sentences(article_text):\n",
    "    # Process the article text and adjust tokenization\n",
    "    blob = TextBlob(article_text)\n",
    "    sentences = blob.sentences\n",
    "\n",
    "    # Determine custom tokenization:\n",
    "    hyphen_sentences = re.split(r'\\n-', article_text)\n",
    "    hyphen_nl_pos = [pos for pos, char in enumerate(article_text) if article_text[pos:pos+2] == '\\n-']  # In testing article a new line and '-' hyphen use typically means consider a different sentence.\n",
    "    extra_split = hyphen_nl_pos\n",
    "\n",
    "    sentence_bounds = [(int(sentence.start), int(sentence.end)) for sentence in sentences]\n",
    "\n",
    "    # Insert custom intervals\n",
    "    updated_list = insert_intervals(sentence_bounds, extra_split)\n",
    "\n",
    "    return updated_list"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T23:43:48.857032Z",
     "start_time": "2023-11-06T23:43:48.848245Z"
    }
   },
   "id": "3e4bb256988b12fe"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 138), (139, 223), (224, 301), (302, 489), (490, 626), (627, 764), (765, 874), (875, 925), (926, 1108), (1109, 1139), (1140, 1605), (1606, 1963), (1964, 2098), (2099, 2298), (2299, 2468), (2469, 2768), (2769, 2937), (2938, 3074), (3075, 3220), (3221, 3273), (3274, 3447), (3448, 3548), (3549, 3710), (3711, 3736), (3737, 3770), (3771, 3820), (3821, 3887), (3888, 3931), (3932, 4009), (4010, 4198), (4199, 4396), (4397, 4607), (4608, 4805), (4806, 4981), (4982, 5077), (5078, 5163), (5164, 5240), (5241, 5371), (5372, 5415), (5416, 5621), (5622, 5738), (5739, 5954), (5955, 6166), (6167, 6254), (6255, 6391), (6392, 6553), (6554, 6613), (6614, 6770), (6771, 6956), (6957, 7268), (7269, 7444), (7445, 7604), (7605, 7691), (7692, 7735), (7736, 7856), (7857, 7909)]\n",
      "56\n",
      "Coreference Threshold: 3\n",
      "Entity Type: CARDINAL\n",
      "Entity: nearly 700,000\n",
      "Positions: [[2131, 2145]]\n",
      "Label: CARDINAL\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: More than 1.5million\n",
      "Positions: [[2299, 2319]]\n",
      "Label: CARDINAL\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: nine\n",
      "Positions: [[2520, 2524]]\n",
      "Label: CARDINAL\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: ten\n",
      "Positions: [[2532, 2535]]\n",
      "Label: CARDINAL\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: five\n",
      "Positions: [[6309, 6313]]\n",
      "Label: CARDINAL\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: Seven\n",
      "Positions: [[6392, 6397]]\n",
      "Label: CARDINAL\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity Type: DATE\n",
      "Entity: yesterday\n",
      "Positions: [[642, 651], [900, 909], [2483, 2492]]\n",
      "Label: DATE\n",
      "Number of Positions: 3\n",
      "\n",
      "Entity: daily\n",
      "Positions: [[1041, 1046], [1395, 1400], [1768, 1773], [2181, 2186], [4268, 4273], [4628, 4633]]\n",
      "Label: DATE\n",
      "Number of Positions: 6\n",
      "\n",
      "Entity: August 29\n",
      "Positions: [[1129, 1138]]\n",
      "Label: DATE\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: 29 August\n",
      "Positions: [[1786, 1795]]\n",
      "Label: DATE\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: weekly\n",
      "Positions: [[1946, 1952]]\n",
      "Label: DATE\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: this year\n",
      "Positions: [[2458, 2467]]\n",
      "Label: DATE\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: today\n",
      "Positions: [[2796, 2801]]\n",
      "Label: DATE\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: last week's\n",
      "Positions: [[2997, 3008]]\n",
      "Label: DATE\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: last weekend\n",
      "Positions: [[3089, 3101]]\n",
      "Label: DATE\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: Wednesday\n",
      "Positions: [[3281, 3290]]\n",
      "Label: DATE\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: this week\n",
      "Positions: [[3480, 3489]]\n",
      "Label: DATE\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: only a year\n",
      "Positions: [[4548, 4559]]\n",
      "Label: DATE\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: September 2015\n",
      "Positions: [[4728, 4742]]\n",
      "Label: DATE\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: 2005\n",
      "Positions: [[4784, 4788]]\n",
      "Label: DATE\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: 2016\n",
      "Positions: [[4891, 4895]]\n",
      "Label: DATE\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: 2015\n",
      "Positions: [[4946, 4950]]\n",
      "Label: DATE\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: next year\n",
      "Positions: [[6760, 6769]]\n",
      "Label: DATE\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: Monday\n",
      "Positions: [[7450, 7456]]\n",
      "Label: DATE\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity Type: EVENT\n",
      "Entity Type: FAC\n",
      "Entity: Sadiq Khan's\n",
      "Positions: [[187, 199]]\n",
      "Label: FAC\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: Dundee\n",
      "Positions: [[6740, 6746]]\n",
      "Label: FAC\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity Type: GPE\n",
      "Entity: London\n",
      "Positions: [[268, 274], [465, 471], [692, 698], [1230, 1236], [2342, 2348], [2550, 2556], [2891, 2897], [3067, 3073], [3541, 3547], [3763, 3769], [4309, 4315], [5790, 5796], [7066, 7072], [7580, 7586]]\n",
      "Label: GPE\n",
      "Number of Positions: 14\n",
      "\n",
      "Entity: Britain\n",
      "Positions: [[756, 763]]\n",
      "Label: GPE\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: Ulez\n",
      "Positions: [[2388, 2392], [3533, 3537], [4657, 4661], [6034, 6038]]\n",
      "Label: GPE\n",
      "Number of Positions: 4\n",
      "\n",
      "Entity: Uxbridge\n",
      "Positions: [[3024, 3032]]\n",
      "Label: GPE\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: Sadiq\n",
      "Positions: [[3741, 3746]]\n",
      "Label: GPE\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: UK\n",
      "Positions: [[4856, 4858]]\n",
      "Label: GPE\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: Greater London\n",
      "Positions: [[5671, 5685]]\n",
      "Label: GPE\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: Bexley\n",
      "Positions: [[5809, 5815]]\n",
      "Label: GPE\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: Bromley\n",
      "Positions: [[5817, 5824]]\n",
      "Label: GPE\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: Harrow\n",
      "Positions: [[5826, 5832]]\n",
      "Label: GPE\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: England\n",
      "Positions: [[6456, 6463], [6916, 6923]]\n",
      "Label: GPE\n",
      "Number of Positions: 2\n",
      "\n",
      "Entity: Bradford\n",
      "Positions: [[6483, 6491]]\n",
      "Label: GPE\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: Sheffield\n",
      "Positions: [[6514, 6523]]\n",
      "Label: GPE\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: Newcastle\n",
      "Positions: [[6529, 6538]]\n",
      "Label: GPE\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: Manchester\n",
      "Positions: [[6602, 6612]]\n",
      "Label: GPE\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: Scotland\n",
      "Positions: [[6617, 6625]]\n",
      "Label: GPE\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: Glasgow\n",
      "Positions: [[6683, 6690]]\n",
      "Label: GPE\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: Edinburgh\n",
      "Positions: [[6729, 6738]]\n",
      "Label: GPE\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: Aberdeen\n",
      "Positions: [[6751, 6759]]\n",
      "Label: GPE\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: Heywood\n",
      "Positions: [[6803, 6810]]\n",
      "Label: GPE\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: Greater Manchester\n",
      "Positions: [[6828, 6846]]\n",
      "Label: GPE\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity Type: LANGUAGE\n",
      "Entity Type: LAW\n",
      "Entity Type: LOC\n",
      "Entity: Greater London\n",
      "Positions: [[2158, 2172]]\n",
      "Label: LOC\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity Type: MONEY\n",
      "Entity: 12.50\n",
      "Positions: [[1035, 1040], [1389, 1394], [1762, 1767], [1864, 1869], [4622, 4627]]\n",
      "Label: MONEY\n",
      "Number of Positions: 5\n",
      "\n",
      "Entity: over £3,000\n",
      "Positions: [[4472, 4483]]\n",
      "Label: MONEY\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: 13,046\n",
      "Positions: [[4918, 4924]]\n",
      "Label: MONEY\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: 3,601\n",
      "Positions: [[4928, 4933]]\n",
      "Label: MONEY\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: 9,445\n",
      "Positions: [[4975, 4980]]\n",
      "Label: MONEY\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: 110million\n",
      "Positions: [[7481, 7491]]\n",
      "Label: MONEY\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity Type: NORP\n",
      "Entity: Brits\n",
      "Positions: [[54, 59]]\n",
      "Label: NORP\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: Tory\n",
      "Positions: [[3207, 3211]]\n",
      "Label: NORP\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity Type: ORDINAL\n",
      "Entity: first\n",
      "Positions: [[4705, 4710]]\n",
      "Label: ORDINAL\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity Type: ORG\n",
      "Entity: Ulez\n",
      "Positions: [[29, 33], [200, 204], [337, 341], [616, 620], [676, 680], [1573, 1577], [1639, 1643], [2627, 2631], [2967, 2971], [3342, 3346], [4182, 4186], [5356, 5360], [5650, 5654], [5925, 5929], [7350, 7354]]\n",
      "Label: ORG\n",
      "Number of Positions: 15\n",
      "\n",
      "Entity: Starmer\n",
      "Positions: [[253, 260]]\n",
      "Label: ORG\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: High Court\n",
      "Positions: [[367, 377], [1802, 1812]]\n",
      "Label: ORG\n",
      "Number of Positions: 2\n",
      "\n",
      "Entity: The High Court\n",
      "Positions: [[627, 641], [2805, 2819], [5739, 5753]]\n",
      "Label: ORG\n",
      "Number of Positions: 3\n",
      "\n",
      "Entity: Labour\n",
      "Positions: [[984, 990], [1338, 1344], [3169, 3175], [3295, 3301], [5330, 5336], [6554, 6560]]\n",
      "Label: ORG\n",
      "Number of Positions: 6\n",
      "\n",
      "Entity: RAC\n",
      "Positions: [[2212, 2215], [7202, 7205]]\n",
      "Label: ORG\n",
      "Number of Positions: 2\n",
      "\n",
      "Entity: Driver\n",
      "Positions: [[2413, 2419]]\n",
      "Label: ORG\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: Vehicle Licensing Agency\n",
      "Positions: [[2424, 2448]]\n",
      "Label: ORG\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: Times Radio\n",
      "Positions: [[3599, 3610]]\n",
      "Label: ORG\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: Energy\n",
      "Positions: [[4010, 4016]]\n",
      "Label: ORG\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: Londoners\n",
      "Positions: [[5404, 5413]]\n",
      "Label: ORG\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: Surrey County Council\n",
      "Positions: [[5859, 5880]]\n",
      "Label: ORG\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: Mr Justice Swift\n",
      "Positions: [[5955, 5971]]\n",
      "Label: ORG\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: Bristol\n",
      "Positions: [[6493, 6500]]\n",
      "Label: ORG\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: Portsmouth\n",
      "Positions: [[6502, 6512]]\n",
      "Label: ORG\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: Gateshead\n",
      "Positions: [[6543, 6552]]\n",
      "Label: ORG\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: The Royal Courts of Justice\n",
      "Positions: [[6998, 7025]]\n",
      "Label: ORG\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: MailOnline\n",
      "Positions: [[7845, 7855]]\n",
      "Label: ORG\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity Type: PERCENT\n",
      "Entity Type: PERSON\n",
      "Entity: Rishi\n",
      "Positions: [[62, 67], [1269, 1274]]\n",
      "Label: PERSON\n",
      "Number of Positions: 2\n",
      "\n",
      "Entity: Sadiq Khan\n",
      "Positions: [[81, 91], [569, 579], [1243, 1253], [1735, 1745]]\n",
      "Label: PERSON\n",
      "Number of Positions: 4\n",
      "\n",
      "Entity: Keir Starmer\n",
      "Positions: [[141, 153], [382, 394], [1140, 1152], [1483, 1495]]\n",
      "Label: PERSON\n",
      "Number of Positions: 4\n",
      "\n",
      "Entity: Tory\n",
      "Positions: [[215, 219]]\n",
      "Label: PERSON\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: Khan\n",
      "Positions: [[360, 364], [1009, 1013], [1363, 1367], [2102, 2106], [2506, 2510], [3396, 3400], [5918, 5922], [6345, 6349], [7106, 7110]]\n",
      "Label: PERSON\n",
      "Number of Positions: 9\n",
      "\n",
      "Entity: Keir\n",
      "Positions: [[769, 773], [3079, 3083], [3613, 3617], [4090, 4094], [5283, 5287]]\n",
      "Label: PERSON\n",
      "Number of Positions: 5\n",
      "\n",
      "Entity: Rishi Sunak\n",
      "Positions: [[926, 937]]\n",
      "Label: PERSON\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: Starmer\n",
      "Positions: [[1688, 1695]]\n",
      "Label: PERSON\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: Boris Johnson's\n",
      "Positions: [[3034, 3049]]\n",
      "Label: PERSON\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: Rachel Reeves\n",
      "Positions: [[3466, 3479], [3696, 3709]]\n",
      "Label: PERSON\n",
      "Number of Positions: 2\n",
      "\n",
      "Entity: Wes Streeting\n",
      "Positions: [[3579, 3592]]\n",
      "Label: PERSON\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: Grant Shapps\n",
      "Positions: [[4027, 4039]]\n",
      "Label: PERSON\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: Ulez\n",
      "Positions: [[4200, 4204]]\n",
      "Label: PERSON\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: Trader\n",
      "Positions: [[4811, 4817]]\n",
      "Label: PERSON\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: Ian Plummer\n",
      "Positions: [[4996, 5007]]\n",
      "Label: PERSON\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: Lucy Frazer\n",
      "Positions: [[5259, 5270]]\n",
      "Label: PERSON\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: Mark Harper\n",
      "Positions: [[5436, 5447]]\n",
      "Label: PERSON\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: Hillingdon\n",
      "Positions: [[5837, 5847]]\n",
      "Label: PERSON\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: Andy Burnham\n",
      "Positions: [[6567, 6579]]\n",
      "Label: PERSON\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: Chris Clarkson\n",
      "Positions: [[6771, 6785]]\n",
      "Label: PERSON\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: Middleton\n",
      "Positions: [[6815, 6824]]\n",
      "Label: PERSON\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity Type: PRODUCT\n",
      "Entity: Shadow\n",
      "Positions: [[3448, 3454]]\n",
      "Label: PRODUCT\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: Londoners\n",
      "Positions: [[3810, 3819]]\n",
      "Label: PRODUCT\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: Volkswagen Golf\n",
      "Positions: [[4875, 4890]]\n",
      "Label: PRODUCT\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity Type: QUANTITY\n",
      "Entity Type: TIME\n",
      "Entity: last night\n",
      "Positions: [[413, 423], [938, 948], [1178, 1188], [1292, 1302]]\n",
      "Label: TIME\n",
      "Number of Positions: 4\n",
      "\n",
      "Entity Type: WORK_OF_ART\n"
     ]
    }
   ],
   "source": [
    "'''Merge all instances of the same entity into a single entry with multiple positions. Where same means a lowercase word match. Place them into an 'entities' dictionary.'''\n",
    "\n",
    "def merge_positions(entities, word):\n",
    "    entity_key = (word.text + word.label_).lower()\n",
    "    if entity_key in entities:\n",
    "        entities[entity_key][1].append([word.start_char, word.end_char])\n",
    "    else:\n",
    "        entities[entity_key] = [word.text, [[word.start_char, word.end_char]], word.label_]\n",
    "    return entities"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T23:43:50.060727Z",
     "start_time": "2023-11-06T23:43:49.399118Z"
    }
   },
   "id": "94af99c84ed0d3ce"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity Type Contained in Dict: PERSON\n",
      "Entity: Rishi\n",
      "Positions: [[62, 67], [1269, 1274]]\n",
      "Label: PERSON\n",
      "Number of Positions: 2\n",
      "\n",
      "Entity: Sadiq Khan\n",
      "Positions: [[81, 91], [569, 579], [1243, 1253], [1735, 1745]]\n",
      "Label: PERSON\n",
      "Number of Positions: 4\n",
      "\n",
      "Entity: Keir Starmer\n",
      "Positions: [[141, 153], [382, 394], [1140, 1152], [1483, 1495]]\n",
      "Label: PERSON\n",
      "Number of Positions: 4\n",
      "\n",
      "Entity: Tory\n",
      "Positions: [[215, 219]]\n",
      "Label: PERSON\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: Khan\n",
      "Positions: [[360, 364], [1009, 1013], [1363, 1367], [2102, 2106], [2506, 2510], [3396, 3400], [5918, 5922], [6345, 6349], [7106, 7110]]\n",
      "Label: PERSON\n",
      "Number of Positions: 9\n",
      "\n",
      "Entity: Keir\n",
      "Positions: [[769, 773], [3079, 3083], [3613, 3617], [4090, 4094], [5283, 5287]]\n",
      "Label: PERSON\n",
      "Number of Positions: 5\n",
      "\n",
      "Entity: Rishi Sunak\n",
      "Positions: [[926, 937]]\n",
      "Label: PERSON\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: Starmer\n",
      "Positions: [[1688, 1695]]\n",
      "Label: PERSON\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: Boris Johnson's\n",
      "Positions: [[3034, 3049]]\n",
      "Label: PERSON\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: Rachel Reeves\n",
      "Positions: [[3466, 3479], [3696, 3709]]\n",
      "Label: PERSON\n",
      "Number of Positions: 2\n",
      "\n",
      "Entity: Wes Streeting\n",
      "Positions: [[3579, 3592]]\n",
      "Label: PERSON\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: Grant Shapps\n",
      "Positions: [[4027, 4039]]\n",
      "Label: PERSON\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: Ulez\n",
      "Positions: [[4200, 4204]]\n",
      "Label: PERSON\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: Trader\n",
      "Positions: [[4811, 4817]]\n",
      "Label: PERSON\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: Ian Plummer\n",
      "Positions: [[4996, 5007]]\n",
      "Label: PERSON\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: Lucy Frazer\n",
      "Positions: [[5259, 5270]]\n",
      "Label: PERSON\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: Mark Harper\n",
      "Positions: [[5436, 5447]]\n",
      "Label: PERSON\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: Hillingdon\n",
      "Positions: [[5837, 5847]]\n",
      "Label: PERSON\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: Andy Burnham\n",
      "Positions: [[6567, 6579]]\n",
      "Label: PERSON\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: Chris Clarkson\n",
      "Positions: [[6771, 6785]]\n",
      "Label: PERSON\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: Middleton\n",
      "Positions: [[6815, 6824]]\n",
      "Label: PERSON\n",
      "Number of Positions: 1\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import math\n",
    "from functools import reduce\n",
    "\n",
    "NER = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "article_text = [article_text][0]\n",
    "article = NER(article_text)\n",
    "\n",
    "# Store sentence positions for use with coreference mentions and news sentiment later\n",
    "sentence_bounds = textblob_custom_sentences(article_text)\n",
    "num_sentences = len(list(sentence_bounds))\n",
    "print(sentence_bounds)\n",
    "print(num_sentences)\n",
    "\n",
    "# NER sentence results comparison\n",
    "# for sent in article.sents:\n",
    "#     print(sent.start_char)\n",
    "#     print(sent.end_char)\n",
    "# \n",
    "# num_sentences = len(list(article.sents))\n",
    "# print(num_sentences)\n",
    "\n",
    "'''# Recommended mention - 'Discard a cluster c in a document d if |Mc| ≤ 0.2|Sd|, where |...| is the \n",
    "number of mentions of a cluster (Mc) and sentences in a document (Sd) (NEWS-MTSC approach)'''\n",
    "\n",
    "# MENTION_REQ_PER = 0.20 \n",
    "\n",
    "MENTION_REQ_PER = 0.07 # Keep as this for ULEZ research article findings later on.\n",
    "threshold = math.floor(num_sentences * MENTION_REQ_PER)\n",
    "print(f\"Coreference Threshold: {threshold}\")\n",
    "\n",
    "# num_sentences = len(list(article.sents))\n",
    "# sentence_positions = [(sent.start_char, sent.end_char) for sent in article.sents]\n",
    "\n",
    "# # Print sentences + start and end pos.\n",
    "# for i, (start, end) in enumerate(sentence_positions):\n",
    "#     sentence_text = article[start:end].text\n",
    "#     print(f\"Sentence {i+1}: {sentence_text}\")\n",
    "#     print(f\"Start Position: {start}, End Position: {end}\")\n",
    "#     print()\n",
    "\n",
    "entity_types = [\"CARDINAL\", \"DATE\", \"EVENT\", \"FAC\", \"GPE\", \"LANGUAGE\", \"LAW\", \"LOC\", \"MONEY\", \"NORP\", \"ORDINAL\", \"ORG\", \"PERCENT\", \"PERSON\", \"PRODUCT\", \"QUANTITY\", \"TIME\", \"WORK_OF_ART\"]\n",
    "\n",
    "entity_type_to_entities = {\n",
    "    entity_type: [\n",
    "        [\n",
    "            entity_text,\n",
    "            positions,\n",
    "            label,\n",
    "            len(positions)  # Count of positions\n",
    "        ] for entity_text, positions, label in reduce(\n",
    "            merge_positions,\n",
    "            filter(lambda word: word.label_ == entity_type, article.ents),\n",
    "            {}\n",
    "        ).values()\n",
    "    ] for entity_type in entity_types\n",
    "}\n",
    "\n",
    "for entity_type, entities in entity_type_to_entities.items():\n",
    "    print(f\"Entity Type: {entity_type}\")\n",
    "    for entity in entities:\n",
    "        entity_text, positions, label, num_positions = entity\n",
    "        print(f\"Entity: {entity_text}\")\n",
    "        print(f\"Positions: {positions}\")\n",
    "        print(f\"Label: {label}\")\n",
    "        print(f\"Number of Positions: {num_positions}\")\n",
    "        print()\n",
    "        \n",
    "people_entities = {entity_type: entity_info for entity_type, entity_info in entity_type_to_entities.items() if entity_type == 'PERSON'}\n",
    "\n",
    "for entity in entity_type_to_entities[entity_type]:\n",
    "    entity_text, positions, label, num_positions = entity\n",
    "    if entity_type not in people_entities:\n",
    "        people_entities[entity_type] = []\n",
    "    people_entities[entity_type].append({\n",
    "        'Entity': entity_text,\n",
    "        'Positions': positions,\n",
    "        'Label': label,\n",
    "        'Number of Positions': num_positions\n",
    "    })"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T23:44:39.535734Z",
     "start_time": "2023-11-06T23:44:39.519137Z"
    }
   },
   "id": "8c13a4012c742ad3"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "{'PERSON': [['Rishi', [[62, 67], [1269, 1274]], 'PERSON', 2], ['Sadiq Khan', [[81, 91], [569, 579], [1243, 1253], [1735, 1745]], 'PERSON', 4], ['Keir Starmer', [[141, 153], [382, 394], [1140, 1152], [1483, 1495]], 'PERSON', 4], ['Tory', [[215, 219]], 'PERSON', 1], ['Khan', [[360, 364], [1009, 1013], [1363, 1367], [2102, 2106], [2506, 2510], [3396, 3400], [5918, 5922], [6345, 6349], [7106, 7110]], 'PERSON', 9], ['Keir', [[769, 773], [3079, 3083], [3613, 3617], [4090, 4094], [5283, 5287]], 'PERSON', 5], ['Rishi Sunak', [[926, 937]], 'PERSON', 1], ['Starmer', [[1688, 1695]], 'PERSON', 1], [\"Boris Johnson's\", [[3034, 3049]], 'PERSON', 1], ['Rachel Reeves', [[3466, 3479], [3696, 3709]], 'PERSON', 2], ['Wes Streeting', [[3579, 3592]], 'PERSON', 1], ['Grant Shapps', [[4027, 4039]], 'PERSON', 1], ['Ulez', [[4200, 4204]], 'PERSON', 1], ['Trader', [[4811, 4817]], 'PERSON', 1], ['Ian Plummer', [[4996, 5007]], 'PERSON', 1], ['Lucy Frazer', [[5259, 5270]], 'PERSON', 1], ['Mark Harper', [[5436, 5447]], 'PERSON', 1], ['Hillingdon', [[5837, 5847]], 'PERSON', 1], ['Andy Burnham', [[6567, 6579]], 'PERSON', 1], ['Chris Clarkson', [[6771, 6785]], 'PERSON', 1], ['Middleton', [[6815, 6824]], 'PERSON', 1]]}\n"
     ]
    }
   ],
   "source": [
    "for entity_type, entities in people_entities.items():\n",
    "    print(f\"Entity Type Contained in Dict: {entity_type}\")\n",
    "    for entity in entities:\n",
    "        entity_text = entity[0]\n",
    "        positions = entity[1]\n",
    "        label = entity[2]\n",
    "        num_positions = entity[3]\n",
    "        print(f\"Entity: {entity_text}\")\n",
    "        print(f\"Positions: {positions}\")\n",
    "        print(f\"Label: {label}\")\n",
    "        print(f\"Number of Positions: {num_positions}\")\n",
    "        print()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T23:44:46.284955Z",
     "start_time": "2023-11-06T23:44:46.263236Z"
    }
   },
   "id": "6293f0a06ea523d7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(sorted_combined_clusters[1][2])\n",
    "print(people_entities)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "736d69146bb2a530"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T23:45:22.402453Z",
     "start_time": "2023-11-06T23:45:22.385247Z"
    }
   },
   "id": "4dcd58dbafa966c0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(f\"Number of Sentences: {num_sentences}\")\n",
    "print(\"Coreference threshold: {:.2f} ({}% detected cluster mentions versus sentences)\".format(threshold, int(MENTION_REQ_PER * 100)))\n",
    "print(\"Cluster Entities before filtration: \", len(sorted_combined_clusters))\n",
    "\n",
    "filtered_clusters = [(i, cluster) for i, cluster in enumerate(sorted_combined_clusters) if cluster[2] >= threshold]\n",
    "\n",
    "print(\"Cluster Entities after filtration: \", len(filtered_clusters))\n",
    "\n",
    "print(filtered_clusters)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "77f6c6572967d079"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "First draft - too simplistic see below markdown cell.\n",
    "'''\n",
    "# entity_to_cluster_mapping = {}\n",
    "# \n",
    "# for entity_type, entities in people_entities.items():\n",
    "#     for entity in entities:\n",
    "#         entity_name = entity[0]\n",
    "#         for cluster in filtered_clusters:\n",
    "#             cluster_text = cluster[0]\n",
    "#             for coref_entry in cluster_text:\n",
    "#                 if entity_name in coref_entry:\n",
    "#                     if entity_name not in entity_to_cluster_mapping:\n",
    "#                         entity_to_cluster_mapping[entity_name] = []\n",
    "# \n",
    "#                     entity_to_cluster_mapping[entity_name].append((cluster_text, cluster[1]))\n",
    "#                     break\n",
    "# print(entity_to_cluster_mapping) \n",
    "# Replaced by beneath code "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d631aef8727a939d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Entity to cluster mapping initially was is this NER entity containined in any coreference cluster.\n",
    "This was too simplistic for example:\n",
    "\n",
    "{'Entity Name': 'Keir Starmer', 'Positions': [[141, 153], [382, 394], [1140, 1152], [1483, 1495]], 'Label': 'PERSON', 'Num Positions': 4, 'Cluster Info': [{'Cluster Text': ['Sadiq Khan', 'his', \"Sadiq Khan's\", 'London mayor', \"Khan's\", \"the capital's mayor Sadiq Khan\", 'Mr Khan', \"London mayor Sadiq Khan's\", 'Mr Khan', \"Sir Keir Starmer's mayor\", 'Sadiq Khan', \"Mr Khan's\", 'Mr Khan', 'I', 'I', 'Mr Khan', 'Sadiq', 'He', 'he', 'his', 'he', 'your mayor', 'the Labour mayor', 'his', \"Mr Khan's\", \"the mayor's\", 'his', 'Mr Khan', 'Mr Khan', 'the mayor', \"the mayor's\"]. \n",
    "- Yes 'Keir' is in the coreference mention 'Sir Keir Starmer's mayor' but on balance wouldn't you agree these coreferences are not referring to him?\n",
    "\n",
    "\n",
    "Solutions I have thought about: \n",
    "1. Removing pronouns (his, her, he, I etc) then doing a % match rate on entity text across the cluster. In the example above we would expect Sadiq / Khan to win out.\n",
    "\n",
    "2. Removing pronouns (his, her, he, I etc) then doing a % match rate on entity position values across the cluster positions."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b1cea1d3add52e65"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "entity_to_cluster_mapping = []\n",
    "\n",
    "# For matching NER people extractions with coreference these references are not useful.\n",
    "undesired_words = [\"i\", \"he\", \"his\", \"she\", \"they\", \"it\", \"this\", \"that\", \"these\", \"those\", \"the\", \"a\", \"an\", \"of\"]\n",
    "\n",
    "\n",
    "def cleanse_cluster_text(cluster_text):\n",
    "    return [word.strip() for word in cluster_text if word.lower().strip() not in undesired_words]\n",
    "\n",
    "threshold_percentage = 0.15 # If \n",
    "\n",
    "for entity_type, entities in people_entities.items():\n",
    "    for entity in entities:\n",
    "        entity_name, positions, label, num_positions = entity\n",
    "        entity_entry = {\n",
    "            'Entity Name': entity_name,\n",
    "            'Positions': positions,\n",
    "            'Label': label,\n",
    "            'Num Positions': num_positions,\n",
    "            'Cluster Info': []\n",
    "        }\n",
    "\n",
    "        for index, (cluster_text, cluster_positions, _) in filtered_clusters:\n",
    "\n",
    "            # for coref_entry in cluster_text:\n",
    "            #     if entity_name in coref_entry:\n",
    "            #         entity_entry['Cluster Info'] = cluster_entry\n",
    "            #         break\n",
    "           \n",
    "            cleaned_cluster_text = cleanse_cluster_text(cluster_text)\n",
    "            total_coref_words = \" \".join(cleaned_cluster_text)\n",
    "            entity_count = total_coref_words.count(entity_name)\n",
    "            percentage = entity_count / len(cleaned_cluster_text)\n",
    "            if percentage > 0.00:\n",
    "                print(entity_name)\n",
    "                print(cleaned_cluster_text)\n",
    "                print(percentage)\n",
    "                print('------------------------------------------')\n",
    "        \n",
    "            if percentage >= threshold_percentage:\n",
    "                cluster_entry = {                \n",
    "                'Cluster Text': cluster_text,\n",
    "                'Cluster Positions': cluster_positions\n",
    "            }\n",
    "                \n",
    "                entity_entry['Cluster Info'] = cluster_entry\n",
    "                break\n",
    "\n",
    "        entity_to_cluster_mapping.append(entity_entry)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "67377b3b671cd43"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Evaluation for threshold:\n",
    "\n",
    "Rishi\n",
    "['Rishi Sunak', 'Rishi Sunak', 'Rishi Sunak', 'Rishi Sunak (pictured)']\n",
    "1.0 Correct\n",
    "------------------------------------------\n",
    "Sadiq Khan\n",
    "['Sadiq Khan', \"Sadiq Khan's\", 'London mayor', \"Khan's\", \"the capital's mayor Sadiq Khan\", 'Mr Khan', \"London mayor Sadiq Khan's\", 'Mr Khan', \"Sir Keir Starmer's mayor\", 'Sadiq Khan', \"Mr Khan's\", 'Mr Khan', 'Mr Khan', 'Sadiq', 'your mayor', 'the Labour mayor', \"Mr Khan's\", \"the mayor's\", 'Mr Khan', 'Mr Khan', 'the mayor', \"the mayor's\"]\n",
    "0.22727272727272727 Correct\n",
    "------------------------------------------\n",
    "Sadiq Khan\n",
    "[\"the expansion of London's clean air zone\", \"the expansion of London mayor Sadiq Khan's (right) Ulez\", 'expansion', 'the Ulez extension', 'the policy', 'expand', 'the Ulez expansion', 'his Ulez expansion', 'expand', 'expand', \"the expansion of London's clean air zone\", 'the change', 'Ulez expansion']\n",
    "0.07692307692307693 Not correct\n",
    "------------------------------------------\n",
    "Sadiq Khan\n",
    "[\"London's clean air zone\", 'Ulez', 'the zone', 'The zone', \"London mayor Sadiq Khan's (right) Ulez\", 'the expanded Ulez zone', 'the Ulez', 'Ulez', 'Ulez', 'Ulez', 'Ulez', 'Ulez', 'Ulez', 'the scheme', \"London's clean air zone\", 'Ulez']\n",
    "0.0625 Not correct\n",
    "------------------------------------------\n",
    "Keir Starmer\n",
    "['Sadiq Khan', \"Sadiq Khan's\", 'London mayor', \"Khan's\", \"the capital's mayor Sadiq Khan\", 'Mr Khan', \"London mayor Sadiq Khan's\", 'Mr Khan', \"Sir Keir Starmer's mayor\", 'Sadiq Khan', \"Mr Khan's\", 'Mr Khan', 'Mr Khan', 'Sadiq', 'your mayor', 'the Labour mayor', \"Mr Khan's\", \"the mayor's\", 'Mr Khan', 'Mr Khan', 'the mayor', \"the mayor's\"]\n",
    "0.045454545454545456 Not correct\n",
    "------------------------------------------\n",
    "Keir Starmer\n",
    "['Keir Starmer', 'Starmer', 'Keir Starmer', 'him', 'Sir Keir', 'the Labour leader', 'Keir Starmer (left)', 'him', 'the Labour leader', \"Sir Keir Starmer's\", 'Starmer', 'Sir Keir', 'the Labour leader', 'Keir', 'Sir Keir', 'Sir Keir']\n",
    "0.25 Correct\n",
    "------------------------------------------\n",
    "Khan\n",
    "['Sadiq Khan', \"Sadiq Khan's\", 'London mayor', \"Khan's\", \"the capital's mayor Sadiq Khan\", 'Mr Khan', \"London mayor Sadiq Khan's\", 'Mr Khan', \"Sir Keir Starmer's mayor\", 'Sadiq Khan', \"Mr Khan's\", 'Mr Khan', 'Mr Khan', 'Sadiq', 'your mayor', 'the Labour mayor', \"Mr Khan's\", \"the mayor's\", 'Mr Khan', 'Mr Khan', 'the mayor', \"the mayor's\"]\n",
    "0.6363636363636364 Correct\n",
    "------------------------------------------\n",
    "Keir\n",
    "['Sadiq Khan', \"Sadiq Khan's\", 'London mayor', \"Khan's\", \"the capital's mayor Sadiq Khan\", 'Mr Khan', \"London mayor Sadiq Khan's\", 'Mr Khan', \"Sir Keir Starmer's mayor\", 'Sadiq Khan', \"Mr Khan's\", 'Mr Khan', 'Mr Khan', 'Sadiq', 'your mayor', 'the Labour mayor', \"Mr Khan's\", \"the mayor's\", 'Mr Khan', 'Mr Khan', 'the mayor', \"the mayor's\"]\n",
    "0.045454545454545456 Not correct\n",
    "------------------------------------------\n",
    "Keir\n",
    "['Keir Starmer', 'Starmer', 'Keir Starmer', 'him', 'Sir Keir', 'the Labour leader', 'Keir Starmer (left)', 'him', 'the Labour leader', \"Sir Keir Starmer's\", 'Starmer', 'Sir Keir', 'the Labour leader', 'Keir', 'Sir Keir', 'Sir Keir']\n",
    "0.5625 Correct\n",
    "------------------------------------------\n",
    "Rishi Sunak\n",
    "['Rishi Sunak', 'Rishi Sunak', 'Rishi Sunak', 'Rishi Sunak (pictured)']\n",
    "1.0 Correct\n",
    "------------------------------------------\n",
    "Starmer\n",
    "['Sadiq Khan', \"Sadiq Khan's\", 'London mayor', \"Khan's\", \"the capital's mayor Sadiq Khan\", 'Mr Khan', \"London mayor Sadiq Khan's\", 'Mr Khan', \"Sir Keir Starmer's mayor\", 'Sadiq Khan', \"Mr Khan's\", 'Mr Khan', 'Mr Khan', 'Sadiq', 'your mayor', 'the Labour mayor', \"Mr Khan's\", \"the mayor's\", 'Mr Khan', 'Mr Khan', 'the mayor', \"the mayor's\"]\n",
    "0.045454545454545456 Not correct\n",
    "------------------------------------------\n",
    "Starmer\n",
    "['Keir Starmer', 'Starmer', 'Keir Starmer', 'him', 'Sir Keir', 'the Labour leader', 'Keir Starmer (left)', 'him', 'the Labour leader', \"Sir Keir Starmer's\", 'Starmer', 'Sir Keir', 'the Labour leader', 'Keir', 'Sir Keir', 'Sir Keir']\n",
    "0.375 Correct\n",
    "------------------------------------------\n",
    "Rachel Reeves\n",
    "['Shadow Chancellor Rachel Reeves', 'Rachel Reeves', 'Mr Justice Swift']\n",
    "0.6666666666666666 Correct\n",
    "------------------------------------------\n",
    "Ulez\n",
    "[\"the expansion of London's clean air zone\", \"the expansion of London mayor Sadiq Khan's (right) Ulez\", 'expansion', 'the Ulez extension', 'the policy', 'expand', 'the Ulez expansion', 'his Ulez expansion', 'expand', 'expand', \"the expansion of London's clean air zone\", 'the change', 'Ulez expansion']\n",
    "0.38461538461538464 Correct\n",
    "------------------------------------------\n",
    "Ulez\n",
    "[\"London's clean air zone\", 'Ulez', 'the zone', 'The zone', \"London mayor Sadiq Khan's (right) Ulez\", 'the expanded Ulez zone', 'the Ulez', 'Ulez', 'Ulez', 'Ulez', 'Ulez', 'Ulez', 'Ulez', 'the scheme', \"London's clean air zone\", 'Ulez']\n",
    "0.6875 Correct\n",
    "------------------------------------------\n",
    "On this whole entity name approach with the above data I figure 0.15. Let's see if we can get a higher threshold value somehow... "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "80de2edae36b28f5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let us try spliting the entity names into part words e.g. Sadiq Khan will be split into Sadiq and Khan for evaluation purposes."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "89353bfe2f7ead41"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "entity_to_cluster_mapping = []\n",
    "\n",
    "undesired_words = [\"i\", \"he\", \"his\", \"she\", \"they\", \"it\", \"this\", \"that\", \"these\", \"those\", \"the\", \"a\", \"an\", \"of\"]\n",
    "\n",
    "\n",
    "def cleanse_cluster_text(cluster_text):\n",
    "    return [word.strip() for word in cluster_text if word.lower().strip() not in undesired_words]\n",
    "\n",
    "threshold_percentage = 0.30\n",
    "\n",
    "for entity_type, entities in people_entities.items():\n",
    "    for entity in entities:\n",
    "        entity_name, positions, label, num_positions = entity\n",
    "        entity_entry = {\n",
    "            'Entity Name': entity_name,\n",
    "            'Positions': positions,\n",
    "            'Label': label,\n",
    "            'Num Positions': num_positions,\n",
    "            'Cluster Info': []\n",
    "        }\n",
    "\n",
    "        cluster_id = 0\n",
    "\n",
    "        for index, (cluster_text, cluster_positions, _) in filtered_clusters:\n",
    "            \n",
    "            cluster_id += 1\n",
    "            \n",
    "            cleaned_cluster_text = cleanse_cluster_text(cluster_text)\n",
    "            total_coref_words = \" \".join(cleaned_cluster_text)\n",
    "            \n",
    "            entity_parts = entity_name.split()\n",
    "            max_percentage = 0.0\n",
    "\n",
    "            for entity_part in entity_parts:\n",
    "                entity_count = total_coref_words.count(entity_part)\n",
    "                percentage = entity_count / len(cleaned_cluster_text) #  How well the entity name matches the cluster. \n",
    "            \n",
    "                if percentage > max_percentage: # Is this a better match than the previous best match? If so make this the new best match.\n",
    "                    max_percentage = percentage\n",
    "                    \n",
    "                if percentage > 0.00:\n",
    "                    print(\"Entity Part: \" + entity_part)\n",
    "                    print(cleaned_cluster_text)\n",
    "                    print(percentage)\n",
    "                    print('------------------------------------------')\n",
    "\n",
    "            if max_percentage > 0.00:\n",
    "                print(\"Entity Name: \" + entity_name)\n",
    "                print(cleaned_cluster_text)\n",
    "                print(\"Max % Match: \" + str(max_percentage))\n",
    "                if max_percentage > threshold_percentage:\n",
    "                    print(\"PREDICTION:  MATCH\")\n",
    "                else:\n",
    "                    print (\"PREDICTION: INVALID\")\n",
    "                print('------------------------------------------')\n",
    "                   \n",
    "                    \n",
    "            cleaned_cluster_text = cleanse_cluster_text(cluster_text)\n",
    "            total_coref_words = \" \".join(cleaned_cluster_text)\n",
    "            entity_count = total_coref_words.count(entity_name)\n",
    "            percentage = entity_count / len(cleaned_cluster_text)\n",
    "            \n",
    "        \n",
    "            if percentage >= threshold_percentage:\n",
    "                cluster_entry = {  \n",
    "                'Cluster ID' : cluster_id,\n",
    "                'Cluster Text': cluster_text,\n",
    "                'Cluster Positions': cluster_positions\n",
    "            }\n",
    "                \n",
    "                entity_entry['Cluster Info'] = cluster_entry\n",
    "                entity_to_cluster_mapping.append(entity_entry)\n",
    "                break\n",
    "\n",
    "        \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7cc2e6f77a939c22"
  },
  {
   "cell_type": "markdown",
   "source": [
    "On this new evaluation will double threshold now to p = 0.3 -- could further explore requirement on the start/end position values in text"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "58c9d2ce99bcc7fb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print(entity_to_cluster_mapping)\n",
    "\n",
    "# Bin entities that were not successful in coreference combination stage i.e their cluster info is blank.\n",
    "\n",
    "for entry in entity_to_cluster_mapping:\n",
    "    print(entry)\n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c8e8c5d3dc73aca0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We need to consolidate some entries that have been determined as the same cluster ID.\n",
    "\n",
    "Two types identified so far:\n",
    "    1. where an entity is the substring of another e.g. Rishi is a substring of Rishi Sunak so should be merged into the 'better' later version with full name.\n",
    "    2. where an entity first and second name are separate entity lists but one is the first name e.g., Keir and the other is the second name Starmer. I propose trying a combination of 2 of the entities under the same cluster ID and checking if they match any coreference values in cluster text \n",
    "    (validating that first / second name combination) so then we can break out create a new combined entity and remove the first / second name instance.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "428400bcb6cd49c2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "Purposes of below code:\n",
    "\n",
    "1. Substring Matching: If one entity is a substring of another entity, they are considered as candidates for consolidation. For example, if \"Rishi\" is a substring of \"Rishi Sunak,\" the code merges them into the longer version, \"Rishi Sunak.\"\n",
    "\n",
    "2. First and Second Name Combination: If there are two entities, one representing the first name and the other representing the last name, and they share the same coreference cluster, the code attempts to combine them into a single entity.'''\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "cluster_dict = defaultdict(list)\n",
    "\n",
    "for entry in entity_to_cluster_mapping:\n",
    "    entity_name = entry['Entity Name']\n",
    "    cluster_id = entry['Cluster Info']['Cluster ID']\n",
    "    \n",
    "    cluster_dict[cluster_id].append(entry)\n",
    "\n",
    "\n",
    "def is_substring(entity1, entity2):\n",
    "    return entity1 in entity2 or entity2 in entity1\n",
    "\n",
    "def combine_entities(entities, cluster_text):\n",
    "    combined_entity = None\n",
    "\n",
    "    for entity1 in entities:\n",
    "        for entity2 in entities:\n",
    "            if entity1 != entity2:\n",
    "                combined1 = entity1 + ' ' + entity2\n",
    "                combined2 = entity2 + ' ' + entity1\n",
    "\n",
    "                if combined1 in cluster_text or combined2 in cluster_text:\n",
    "                    combined_entity = combined1 if combined1 in cluster_text else combined2\n",
    "                    break\n",
    "\n",
    "    return combined_entity\n",
    "\n",
    "for cluster_id, entries in cluster_dict.items():\n",
    "    if len(entries) > 1:\n",
    "        combined_entry = None\n",
    "        for i, entry1 in enumerate(entries):\n",
    "            for j, entry2 in enumerate(entries):\n",
    "                if i < j:\n",
    "                    entity_1_name = entry1['Entity Name']\n",
    "                    entity_2_name = entry2['Entity Name']\n",
    "                    \n",
    "                    cluster_text = entries[0]['Cluster Info']['Cluster Text']\n",
    "                    combined_entity = combine_entities([entity_1_name, entity_2_name], cluster_text)\n",
    "                    \n",
    "                    \n",
    "                    if is_substring(entity_1_name, entity_2_name):\n",
    "                        if len(entity_1_name) > len(entity_2_name):\n",
    "                            entries.remove(entry2)\n",
    "                            entry1['Num Positions'] = int(-200) # Coreference cluster will be used anyway -200 to indicate merge via removal.\n",
    "                            entry1['Positions'] = int(-200) \n",
    "                        else:\n",
    "                            entries.remove(entry1)\n",
    "                            entry2['Num Positions'] = int(-200) # Coreference cluster will be used anyway -200 to indicate merge via removal.\n",
    "                            entry2['Positions'] = int(-200) \n",
    "                            \n",
    "                    elif combined_entity in cluster_text:\n",
    "                        combined_entry = {\n",
    "                            'Entity Name': combined_entity,\n",
    "                            'Positions': int(-100), # Coreference cluster will be used anyway -100 to indicate merge via combined entity.\n",
    "                            'Label': entry1['Label'], \n",
    "                            'Num Positions': int(-100),  # Coreference cluster will be used anyway -100 to indicate merge via combined entity.\n",
    "                            'Cluster Info': entry1['Cluster Info'] \n",
    "                        }\n",
    "                        entries.remove(entry1)\n",
    "                        entries.remove(entry2)\n",
    "                        break\n",
    "        if combined_entry:\n",
    "            entries.append(combined_entry)                \n",
    "\n",
    "clustered_entities = [entry for entries in cluster_dict.values() for entry in entries]\n",
    "\n",
    "for entry in clustered_entities:\n",
    "    print(entry)\n",
    "    print()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9a4469ae201a3df5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "With Khan may be worth searching the cluster text for two word mentions that ft. Khan to extract Sadiq Khan as a \"better\" entity name."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a79a062e9d3b5bda"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# !pip install textblob"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e3cc8ec0db789d93"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ***Replaced with TextBlob code below as it had a convenient sentence.start and sentence.end)\n",
    "\n",
    "\n",
    "# import nltk\n",
    "# from nltk.tokenize import sent_tokenize\n",
    "# \n",
    "# sentences = sent_tokenize(article_text)\n",
    "# sentence_boundaries = [(0, len(sentences[0]))] \n",
    "# for i in range(1, len(sentences)):\n",
    "#     start = sentence_boundaries[-1][1] + 1\n",
    "#     end = start + len(sentences[i])\n",
    "#     sentence_boundaries.append((start, end))\n",
    "# \n",
    "# \n",
    "# # print(sentence_boundaries)\n",
    "# # Cluster positions (assuming they are character positions within the text)\n",
    "# cluster_positions = [(20, 35)]\n",
    "# \n",
    "# # Initialise a list to store the selected sentences\n",
    "# selected_sentences = []\n",
    "# \n",
    "# # Map cluster positions to sentences\n",
    "# for start, end in cluster_positions:\n",
    "#     for i, (sentence_start, sentence_end) in enumerate(sentence_boundaries):\n",
    "#         if sentence_start <= start < sentence_end or sentence_start <= end < sentence_end:\n",
    "#             selected_sentences.append(sentences[i])\n",
    "# \n",
    "# # Print the selected sentences\n",
    "# for sentence in selected_sentences:\n",
    "#     print(sentence)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a2932c40738918d5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Old Boundaries Code:\n",
    "\n",
    "\n",
    "# def insert_intervals(initial_list, new_values):\n",
    "#     def insert_recursive(intervals, values):\n",
    "#         if not values:\n",
    "#             return intervals\n",
    "# \n",
    "#         value = values[0]\n",
    "#         result = []\n",
    "#         for interval in intervals:\n",
    "#             if interval[0] <= value <= interval[1]:\n",
    "#                 if interval[0] < value:\n",
    "#                     result.append((interval[0], value - 1))\n",
    "#                 if value < interval[1]:\n",
    "#                     result.append((value, interval[1]))\n",
    "#             else:\n",
    "#                 result.append(interval)\n",
    "#         return insert_recursive(result, values[1:])\n",
    "# \n",
    "#     updated_list = insert_recursive(initial_list, new_values)\n",
    "#     return updated_list"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f8ef663fc44398d2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(sentence_bounds)\n",
    "print(extra_split)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a60e6b713c945187"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a06d2c442c2a6c44"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''The purpose of this graph is to illustrate the impact of different exponents on the scaling of sentiment probabilities.'''\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Probabilities from 0 to 1\n",
    "probabilities = np.linspace(0, 1, 100)\n",
    "\n",
    "exponents = range(11)\n",
    "\n",
    "# Create subplots for each exponent\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for exponent in exponents:\n",
    "    points = (probabilities ** exponent) * 100\n",
    "    plt.plot(probabilities, points, label=f'Exponent = {exponent}')\n",
    "\n",
    "plt.xlabel('Probability')\n",
    "plt.ylabel('Points')\n",
    "plt.title('Exponential Scaling of Sentiment Probabilities')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "94a6c035725ef728"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def scaling_old(probabilities, k=3, linear=False):\n",
    "    neutral_points = positive_points = negative_points = 0\n",
    "\n",
    "    for prob_data in probabilities:\n",
    "        class_prob = prob_data['class_prob']\n",
    "        class_label = prob_data['class_label']\n",
    "        \n",
    "        if linear:\n",
    "            points = class_prob * 100\n",
    "        else:\n",
    "            points = (class_prob ** k) * 100\n",
    "\n",
    "        if class_label == 'neutral':\n",
    "            neutral_points += points\n",
    "        elif class_label == 'positive':\n",
    "            positive_points += points\n",
    "        elif class_label == 'negative':\n",
    "            negative_points += points\n",
    "\n",
    "    return [neutral_points, positive_points, negative_points]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "51289762547bf653"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def scaling(avg_array_list, k=3, linear=False):\n",
    "    neutral_points = positive_points = negative_points = 0\n",
    "\n",
    "    for avg_array in avg_array_list:\n",
    "        for i, avg_value in enumerate(avg_array):\n",
    "            \n",
    "            if linear:\n",
    "                points = avg_value * 100\n",
    "            else:\n",
    "                points = (avg_value ** k) * 100\n",
    "    \n",
    "            if i == 0:\n",
    "                neutral_points += points\n",
    "            elif i == 1:\n",
    "                positive_points += points\n",
    "            elif i == 2:\n",
    "                negative_points += points\n",
    "\n",
    "    return [neutral_points, positive_points, negative_points]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c230e09f1cc40da6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def average_array(probabilities):\n",
    "    num_probabilities = len(probabilities)\n",
    "    neutral_total = positive_total = negative_total = 0\n",
    "\n",
    "    for prob_data in probabilities:\n",
    "        class_prob = prob_data['class_prob']\n",
    "        class_label = prob_data['class_label']\n",
    "\n",
    "        if class_label == 'neutral':\n",
    "            neutral_total += class_prob\n",
    "        elif class_label == 'positive':\n",
    "            positive_total += class_prob\n",
    "        elif class_label == 'negative':\n",
    "            negative_total += class_prob\n",
    "\n",
    "    neutral_avg = neutral_total / num_probabilities if num_probabilities > 0 else 0\n",
    "    positive_avg = positive_total / num_probabilities if num_probabilities > 0 else 0\n",
    "    negative_avg = negative_total / num_probabilities if num_probabilities > 0 else 0\n",
    "\n",
    "    return [neutral_avg, positive_avg, negative_avg]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "97280b1502714c91"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# !pip install NewsSentiment\n",
    "\n",
    "from NewsSentiment import TargetSentimentClassifier\n",
    "tsc = TargetSentimentClassifier()\n",
    "\n",
    "def bounds_sentiment(mention_start, mention_end, sentence_start, sentence_end):\n",
    "    \n",
    "    sentiment = tsc.infer_from_text(article_text[sentence_start:mention_start] , article_text[mention_start:mention_end], article_text[mention_end:sentence_end])\n",
    "    print(sentiment[0])\n",
    "    return sentiment[0]\n",
    "    \n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "104174efe03cb89c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Code to identify sentence and highlight target - sending first instance of entity in a sentence to NewsSentiment. \n",
    "# Misses some sentences that are un-punctuated in the ULEZ research article.\n",
    "\n",
    "from intervaltree import Interval, IntervalTree\n",
    "\n",
    "print(clustered_entities[1]['Entity Name'])\n",
    "\n",
    "START_HIGHLIGHT = '\\033[0m'\n",
    "END_HIGHLIGHT = '\\033[94m'\n",
    "\n",
    "GREEN = '\\033[92m'\n",
    "END_COLOR = '\\033[0m'\n",
    "\n",
    "cluster_positions = [(81, 91), (96, 99), (187, 199), (268, 280), (360, 366), (549, 579), (1006, 1013), (1230, 1255), (1360, 1367), (1479, 1503), (1735, 1745), (2099, 2108), (2503, 2510), (2669, 2670), (2688, 2689), (3393, 3400), (3741, 3746), (3771, 3773), (3796, 3798), (3866, 3869), (3936, 3938), (4136, 4146), (5326, 5342), (5352, 5355), (5915, 5924), (5999, 6010), (6154, 6157), (6342, 6349), (7103, 7110), (7216, 7225), (7468, 7479)]\n",
    "\n",
    "\n",
    "\n",
    "bounds_tree = IntervalTree(Interval(start, end) for start, end in sentence_bounds)\n",
    "\n",
    "processed_bounds = {}\n",
    "\n",
    "for mention_start, mention_end in cluster_positions:\n",
    "    overlap = bounds_tree.overlap(mention_start, mention_end)\n",
    "    if overlap:\n",
    "        first_unprocessed = None\n",
    "        for interval in overlap:\n",
    "            print(\" \")\n",
    "            sentence_start, sentence_end = interval.begin, interval.end\n",
    "            bounds_key = (sentence_start, sentence_end)\n",
    "            \n",
    "            highlighted_text = (\n",
    "                START_HIGHLIGHT + article_text[sentence_start:mention_start] + END_HIGHLIGHT +\n",
    "                article_text[mention_start:mention_end] +\n",
    "                START_HIGHLIGHT + article_text[mention_end:sentence_end] + END_HIGHLIGHT)\n",
    "            print(START_HIGHLIGHT + f\"Mention ({mention_start}, {mention_end}) is within bounds ({sentence_start}, {sentence_end})\")\n",
    "            print(highlighted_text)\n",
    "            \n",
    "            \n",
    "            if bounds_key not in processed_bounds:\n",
    "                first_unprocessed = interval\n",
    "                processed_bounds[bounds_key] = True\n",
    "                bounds_sentiment(mention_start, mention_end, sentence_start, sentence_end)\n",
    "                print(GREEN + \"NewsSentiment Candidate as first bound appearance\" + END_COLOR)\n",
    "                break"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae0432b3a3266ee"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bounds_data = {}\n",
    "\n",
    "for entity in clustered_entities:\n",
    "    entity_name = entity['Entity Name']\n",
    "    cluster_positions = entity['Cluster Info']['Cluster Positions']\n",
    "\n",
    "\n",
    "    for mention_start, mention_end in cluster_positions:\n",
    "        overlap = bounds_tree.overlap(mention_start, mention_end)\n",
    "        if overlap:\n",
    "            for interval in overlap:\n",
    "                print(\" \")\n",
    "                sentence_start, sentence_end = interval.begin, interval.end\n",
    "                bounds_key = (sentence_start, sentence_end)\n",
    "\n",
    "                if bounds_key not in bounds_data:\n",
    "                    bounds_data[bounds_key] = {}\n",
    "\n",
    "                if entity_name not in bounds_data[bounds_key]:\n",
    "                    bounds_data[bounds_key][entity_name] = []\n",
    "\n",
    "                highlighted_text = (\n",
    "                    START_HIGHLIGHT + article_text[sentence_start:mention_start] + END_HIGHLIGHT +\n",
    "                    article_text[mention_start:mention_end] +\n",
    "                    START_HIGHLIGHT + article_text[mention_end:sentence_end] + END_HIGHLIGHT)\n",
    "                print(START_HIGHLIGHT + f\"{entity_name} - Mention ({mention_start}, {mention_end}) is within bounds ({sentence_start}, {sentence_end})\")\n",
    "                print(highlighted_text)\n",
    "\n",
    "                result = bounds_sentiment(mention_start, mention_end, sentence_start, sentence_end)\n",
    "\n",
    "                bounds_data[bounds_key][entity_name].append(result)\n",
    "\n",
    "                print(GREEN + f\"NewsSentiment Candidate appearance {len(bounds_data[bounds_key][entity_name])}\" + END_COLOR)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4dc2cf6ca4154b14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def percentage_contribution(elements):\n",
    "    total = sum(elements)\n",
    "    percentage_contributions = [(element / total) * 100 for element in elements]\n",
    "    return percentage_contributions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fac5a78ed6af9a41"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "entity_averages = {}\n",
    "\n",
    "for bounds_key, entity_results in bounds_data.items():\n",
    "    for entity_name, results in entity_results.items():\n",
    "        print(entity_name + \" in:\")\n",
    "        print(article_text[bounds_key[0]:bounds_key[1]])\n",
    "        print(results)\n",
    "        avg = average_array(results)\n",
    "        \n",
    "        if entity_name not in entity_averages:\n",
    "            entity_averages[entity_name] = []\n",
    "        entity_averages[entity_name].append(avg)\n",
    "    \n",
    "    \n",
    "for entity_name, averages in entity_averages.items():\n",
    "    print(f\"Averages for {entity_name}:\")\n",
    "    for avg in averages:\n",
    "        print(avg)\n",
    "    scaled_classification =scaling(averages, k=3)\n",
    "    percentage_values = percentage_contribution(scaled_classification)\n",
    "    \n",
    "    linear_scaled_classification =scaling(averages, linear=True)\n",
    "    linear_percentage_values = percentage_contribution(linear_scaled_classification)\n",
    "    \n",
    "    print(\"Under exponential points system: \")\n",
    "    print(scaled_classification)\n",
    "    print(percentage_values)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if scaled_classification[0] > scaled_classification[1] and scaled_classification[0] > scaled_classification[2]:\n",
    "        print(\"Exponential Neutral Majority\")\n",
    "        \n",
    "    elif scaled_classification[1] > scaled_classification[0] and scaled_classification[1] > scaled_classification[2]:\n",
    "        print(\"Exponential Positive Majority\")\n",
    "        \n",
    "    elif scaled_classification[2] > scaled_classification[0] and scaled_classification[2] > scaled_classification[1]:\n",
    "        print(\"Exponential Negative Majority\")\n",
    "        \n",
    "    print(\"Under linear points system: \")\n",
    "    print(linear_scaled_classification)\n",
    "    print(linear_percentage_values)\n",
    "    \n",
    "    if linear_scaled_classification[0] > linear_scaled_classification[1] and linear_scaled_classification[0] > linear_scaled_classification[2]:\n",
    "        print(\"Linear Neutral Majority\")\n",
    "        \n",
    "    elif linear_scaled_classification[1] > linear_scaled_classification[0] and linear_scaled_classification[1] > linear_scaled_classification[2]:\n",
    "        print(\"Linear Positive Majority\")\n",
    "        \n",
    "    elif linear_scaled_classification[2] > linear_scaled_classification[0] and linear_scaled_classification[2] > linear_scaled_classification[1]:\n",
    "        print(\"Linear Negative Majority\")\n",
    "\n",
    "    print(\"----------------\")\n",
    "    \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "96c21d75bd7c2c6d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "collated_entities = {}\n",
    "\n",
    "for entity_type, entities in entity_type_to_entities.items():\n",
    "    for entity_info in entities:\n",
    "        entity_text = entity_info[0]\n",
    "        positions = entity_info[1]\n",
    "        label = entity_info[2]\n",
    "\n",
    "        if entity_text in collated_entities:\n",
    "            if label in collated_entities[entity_text]:\n",
    "                collated_entities[entity_text][label]['occurrences'] += len(positions)\n",
    "                collated_entities[entity_text][label]['positions'].extend(positions)\n",
    "            else:\n",
    "                collated_entities[entity_text][label] = {'occurrences': len(positions), 'positions': positions}\n",
    "        else:\n",
    "            collated_entities[entity_text] = {label: {'occurrences': len(positions), 'positions': positions}}\n",
    "\n",
    "\n",
    "entities_with_multiple_labels = {}\n",
    "\n",
    "# for entity_text, label_occurrences in collated_entities.items():\n",
    "#     output = [f\"Entity Text: {entity_text}\"]\n",
    "#     for label, data in label_occurrences.items():\n",
    "#         output.append(f\"Label: {label}, Occurrences: {data['occurrences']}, Positions: {data['positions']}\")\n",
    "#     print(\" | \".join(output))\n",
    "    \n",
    "\n",
    "# Print results where there are duplicate / in dispute NER labels:\n",
    "\n",
    "# for entity_text, label_occurrences in collated_entities.items():\n",
    "#     # Check if there are multiple labels associated with the entity text\n",
    "#     if len(label_occurrences) > 1:\n",
    "#         output = [f\"Entity Text: {entity_text}\"]\n",
    "#         for label, data in label_occurrences.items():\n",
    "#              output.append(f\"Label: {label}, Occurrences: {data['occurrences']}, Positions: {data['positions']}\")\n",
    "#              # output.append(f\"Label: {label}, Occurrences: {data['occurrences']}\")\n",
    "#         entities_with_multiple_labels[entity_text] = \" | \".join(output)\n",
    "#         print(\" | \".join(output))\n",
    "#         \n",
    "# \n",
    "# \n",
    "# for entity_text, data in entities_with_multiple_labels.items():\n",
    "#     print(f\" {data}\")\n",
    "\n",
    "# Store results where there are duplicate / in dispute NER labels:\n",
    "for entity_text, label_occurrences in collated_entities.items():\n",
    "    if len(label_occurrences) > 1:\n",
    "        entities_with_multiple_labels[entity_text] = []\n",
    "        for label, data in label_occurrences.items():\n",
    "            entity_info = {\n",
    "                \"Label\": label,\n",
    "                \"Occurrences\": data['occurrences'],\n",
    "                \"Positions\": data['positions']\n",
    "            }\n",
    "            entities_with_multiple_labels[entity_text].append(entity_info)\n",
    "\n",
    "# Print the stored alike entity text values that have multiple NER labels:\n",
    "for entity_text, label_data in entities_with_multiple_labels.items():\n",
    "    print(\"---------------------------\")\n",
    "    print(f\"Entity Text: {entity_text}\")\n",
    "    for label_info in label_data:\n",
    "        print(f\"Label: {label_info['Label']}, Occurrences: {label_info['Occurrences']}, Positions: {label_info['Positions']}\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5b95cee8380c735d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Appears using the majority case wouldn't always work. \n",
    "\n",
    "Let us park these inconclusive NER classifications here ^^^. After we have connected the NER to the coreference resolution output...."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b9e119e34e5a9368"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "coreference_matrix = [\n",
    "    'Sadiq Khan', 'his', \"Sadiq Khan's\", 'London mayor', \"Khan's\", \"the capital's mayor Sadiq Khan\",\n",
    "    'Mr Khan', \"London mayor Sadiq Khan's\", 'Mr Khan', \"Sir Keir Starmer's mayor\", 'Sadiq Khan',\n",
    "    \"Mr Khan's\", 'Mr Khan', 'I', 'I', 'Mr Khan', 'Sadiq', 'He', 'he', 'his', 'he', 'your mayor',\n",
    "    'the Labour mayor', 'his', \"Mr Khan's\", \"the mayor's\", 'his', 'Mr Khan', 'Mr Khan', 'the mayor', \"the mayor's\"\n",
    "]\n",
    "\n",
    "split_matrix = [element.split() for element in coreference_matrix]\n",
    "\n",
    "nouns = []\n",
    "for words in split_matrix:\n",
    "    for word in words:\n",
    "        doc = nlp(word)\n",
    "        for token in doc:\n",
    "            if token.pos_ == \"NOUN\":\n",
    "                nouns.append(token.text)\n",
    "\n",
    "print(nouns)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6b5b844c5b7ade2a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Seems flawed approach as Keir has been brought out from the phrase \"Sir Keir Starmer's mayor\" even when we are referring to Sadiq Khan - additionally Khan was discarded too."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8651bfcefbaf80ea"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for cluster in clustersText:\n",
    "    print(\"-----------------------------\")\n",
    "    most_specific_mention = None\n",
    "   \n",
    "    text = \" \".join(cluster)\n",
    "    doc = nlp(text)\n",
    "\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            most_specific_mention = most_specific_mention or ent.text\n",
    "\n",
    "    if most_specific_mention:\n",
    "        print(cluster)\n",
    "        print(\"Most Specific PERSON Mention:\", most_specific_mention)\n",
    "    else:\n",
    "        print(cluster)\n",
    "        print(\"No specific PERSON name found in the cluster.\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "67b2853ae3d71ebf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: Resolve conflict code above to identify prevalence of NER unsure.\n",
    "# TODO: Match co-reference resolution outcomes with NER identification to increase positions code.\n",
    "# TODO: WIKIPEDIA Resolution? (Nice to have? for summaries?)\n",
    "# TODO: Extract sentences for all identified mentions\n",
    "# TODO: Apply NewsSentiment with correct start/end positions and formatting resolution"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bc7006348fca5a0c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "9b736954602caad"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "14c810297faab945"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
