{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "from decimal import Decimal, ROUND_HALF_UP\n",
    "from constants import EXPONENTIAL_K_VALUE\n",
    "from intervaltree import Interval, IntervalTree\n",
    "from NewsSentiment import TargetSentimentClassifier\n",
    "\n",
    "\n",
    "def scaling(avg_array_list, k=3, linear=False):\n",
    "    neutral_points = positive_points = negative_points = 0\n",
    "\n",
    "    for avg_array in avg_array_list:\n",
    "        for i, avg_value in enumerate(avg_array):\n",
    "\n",
    "            if linear:\n",
    "                points = avg_value * 100\n",
    "            else:\n",
    "                points = (avg_value ** k) * 100\n",
    "\n",
    "            if i == 0:\n",
    "                neutral_points += points\n",
    "            elif i == 1:\n",
    "                positive_points += points\n",
    "            elif i == 2:\n",
    "                negative_points += points\n",
    "    return [neutral_points, positive_points, negative_points]\n",
    "\n",
    "\n",
    "def average_array(probabilities):\n",
    "    num_probabilities = len(probabilities)\n",
    "    neutral_total = positive_total = negative_total = 0\n",
    "\n",
    "    for prob_data in probabilities:\n",
    "        if not prob_data:\n",
    "            continue\n",
    "        class_prob = prob_data['class_prob']\n",
    "        class_label = prob_data['class_label']\n",
    "\n",
    "        if class_label == 'neutral':\n",
    "            neutral_total += class_prob\n",
    "        elif class_label == 'positive':\n",
    "            positive_total += class_prob\n",
    "        elif class_label == 'negative':\n",
    "            negative_total += class_prob\n",
    "\n",
    "    neutral_avg = neutral_total / num_probabilities if num_probabilities > 0 else 0\n",
    "    positive_avg = positive_total / num_probabilities if num_probabilities > 0 else 0\n",
    "    negative_avg = negative_total / num_probabilities if num_probabilities > 0 else 0\n",
    "\n",
    "    return [neutral_avg, positive_avg, negative_avg]\n",
    "\n",
    "\n",
    "def round_array_to_1dp(arr):\n",
    "    decimal_array = [Decimal(str(x)) for x in arr]\n",
    "    rounded_array = [x.quantize(Decimal('0.0'), rounding=ROUND_HALF_UP) for x in decimal_array]\n",
    "    rounded_sum = sum(rounded_array)\n",
    "    adjustment = Decimal('100') - rounded_sum\n",
    "    rounded_array[-1] += adjustment\n",
    "    rounded_array = [float(x) for x in rounded_array]\n",
    "    return rounded_array\n",
    "\n",
    "\n",
    "def percentage_contribution(elements):\n",
    "    total = sum(elements)\n",
    "    percentage_contributions = [(element / total) * 100 for element in elements]\n",
    "    return round_array_to_1dp(percentage_contributions)\n",
    "\n",
    "\n",
    "class SentimentAnalyser:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.tsc = TargetSentimentClassifier()\n",
    "\n",
    "    def bounds_sentiment(self, mention_start, mention_end, sentence_start, sentence_end,\n",
    "                         article_text, database_id):\n",
    "        try:\n",
    "            left_segment = article_text[sentence_start:mention_start]\n",
    "            mention_segment = article_text[mention_start:mention_end]\n",
    "            right_segment = article_text[mention_end:sentence_end]\n",
    "\n",
    "            # Could add logging here to see the quality of sentence segmentation.\n",
    "            start_time = time.time()\n",
    "            sentiment = self.tsc.infer_from_text(left_segment, mention_segment, right_segment)\n",
    "            elapsed_time = time.time() - start_time\n",
    "\n",
    "            if elapsed_time > 5:\n",
    "                print(f\"News Sentiment Time > 5 seconds so: {elapsed_time} seconds\")\n",
    "            # print(sentiment[0])\n",
    "\n",
    "            return sentiment[0]\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during sentiment analysis: {e}\")\n",
    "            print(f\"LEFT: {left_segment}\")\n",
    "            print(f\"MENTION: {mention_segment}\")\n",
    "            print(f\"RIGHT: {right_segment}\")\n",
    "\n",
    "            BoundError.objects.create(\n",
    "                article_id=database_id,\n",
    "                bound_start=mention_start,\n",
    "                bound_end=mention_end,\n",
    "                left_segment=left_segment,\n",
    "                mention_segment=mention_segment,\n",
    "                right_segment=right_segment,\n",
    "                error_message=f\"Exception during sentiment analysis\"\n",
    "            )\n",
    "\n",
    "            return None\n",
    "\n",
    "    def process_clustered_entities(self, clustered_entities, sentence_bounds, article_text,\n",
    "                                   database_id,\n",
    "                                   debug):\n",
    "        START_HIGHLIGHT = '\\033[0m'\n",
    "        END_HIGHLIGHT = '\\033[94m'\n",
    "        GREEN = '\\033[92m'\n",
    "        END_COLOR = '\\033[0m'\n",
    "\n",
    "        bounds_tree = IntervalTree(Interval(start, end) for start, end in\n",
    "                                   sentence_bounds)\n",
    "\n",
    "        bounds_sentiment = {}\n",
    "\n",
    "        ''' Some entities at this point may not have been fully consolidated.\n",
    "            Running the model is the most intensive part of this process.\n",
    "            Since non consolidated entities likely have the same coref cluster.\n",
    "            Running model over save cluster more than once is wasteful.'''\n",
    "\n",
    "        cluster_id_mapping = {}  # Map cluster_id to bounds_sentiment\n",
    "\n",
    "        for entity in clustered_entities:\n",
    "            entity_name = entity['Entity Name']\n",
    "            if 'entity_db_id' in entity:\n",
    "                entity_db_id = entity['entity_db_id']\n",
    "            else:\n",
    "                print(\"Process clustered entities would skip...\")\n",
    "                print(entity_name)\n",
    "                continue\n",
    "            cluster_positions = entity['Cluster Info']['Cluster Positions']\n",
    "            cluster_id = entity['Cluster Info']['Cluster ID']\n",
    "\n",
    "            # Check if the cluster_id has been seen before\n",
    "            if cluster_id in cluster_id_mapping:\n",
    "                # print('Using cached bounds sentiment')\n",
    "                # If so, use the cached bounds_sentiment\n",
    "                for entry in cluster_id_mapping[cluster_id]:\n",
    "                    bounds_key = entry['bounds_key']\n",
    "\n",
    "                    if entity_name not in bounds_sentiment[bounds_key]:\n",
    "                        bounds_sentiment[bounds_key][entity_name] = {}\n",
    "\n",
    "                    if entity_db_id not in bounds_sentiment[bounds_key][entity_name]:\n",
    "                        bounds_sentiment[bounds_key][entity_name][entity_db_id] = []\n",
    "\n",
    "                    bounds_sentiment[bounds_key][entity_name][entity_db_id].append(entry['result'])\n",
    "\n",
    "            else:\n",
    "                cluster_id_mapping[cluster_id] = []\n",
    "\n",
    "                for mention_start, mention_end in cluster_positions:\n",
    "                    overlap = bounds_tree.overlap(mention_start, mention_end)\n",
    "                    if overlap:\n",
    "                        for interval in overlap:\n",
    "                            sentence_start, sentence_end = interval.begin, interval.end\n",
    "                            bounds_key = (sentence_start, sentence_end)\n",
    "\n",
    "                            if bounds_key not in bounds_sentiment:\n",
    "                                bounds_sentiment[bounds_key] = {}\n",
    "\n",
    "                            if entity_name not in bounds_sentiment[bounds_key]:\n",
    "                                bounds_sentiment[bounds_key][entity_name] = {}\n",
    "\n",
    "                            if entity_db_id not in bounds_sentiment[bounds_key][entity_name]:\n",
    "                                bounds_sentiment[bounds_key][entity_name][entity_db_id] = []\n",
    "\n",
    "                            highlighted_text = (\n",
    "                                    START_HIGHLIGHT +\n",
    "                                    article_text[sentence_start:mention_start] + END_HIGHLIGHT +\n",
    "                                    article_text[mention_start:mention_end] + START_HIGHLIGHT +\n",
    "                                    article_text[mention_end:sentence_end] + END_HIGHLIGHT)\n",
    "\n",
    "                            result = self.bounds_sentiment(mention_start, mention_end,\n",
    "                                                           sentence_start, sentence_end,\n",
    "                                                           article_text, database_id)\n",
    "\n",
    "                            bounds_sentiment[bounds_key][entity_name][entity_db_id].append(\n",
    "                                result)\n",
    "\n",
    "                            cluster_id_mapping[cluster_id].append({\n",
    "                                'bounds_key': bounds_key,\n",
    "                                'result': result\n",
    "                            })\n",
    "\n",
    "                            if debug:\n",
    "                                print(\n",
    "                                    START_HIGHLIGHT + f\"{entity_name} - Mention ({mention_start}, {mention_end}) is within bounds ({sentence_start}, {sentence_end})\")\n",
    "                                print(highlighted_text)\n",
    "                                print(\n",
    "                                    GREEN + f\"NewsSentiment Candidateappearance{len(bounds_sentiment[bounds_key][entity_name][entity_db_id])}\" + END_COLOR)\n",
    "\n",
    "        return bounds_sentiment\n",
    "\n",
    "    @staticmethod\n",
    "    def average_sentiment_results(source_article_id, bounds_sentiment, article_text):\n",
    "        if bounds_sentiment is None:\n",
    "            print(\"Error: bounds_sentiment is None\")\n",
    "            return\n",
    "        entity_averages = {}\n",
    "        for bounds_key, entity_results in bounds_sentiment.items():\n",
    "            for entity_name, entity_db_ids in entity_results.items():\n",
    "                # print(\"Entity DB IDs: \")\n",
    "                # print(entity_db_ids)\n",
    "                for entity_db_id, results in entity_db_ids.items():\n",
    "\n",
    "                    if not results:  # Empty results for an entity? Skip...\n",
    "                        continue\n",
    "                    # print(results)\n",
    "                    avg = average_array(results)\n",
    "\n",
    "                    # Store entity - bound mention - bound text - average result in database\n",
    "\n",
    "                    if entity_name not in entity_averages:\n",
    "                        entity_averages[entity_name] = {\n",
    "                            \"entity_db_ids\": [entity_db_id],\n",
    "                            \"bounds_keys\": [bounds_key],\n",
    "                            \"sentiment_scores\": [avg],\n",
    "                            \"text\": [article_text[bounds_key[0]:bounds_key[1]]],\n",
    "                        }\n",
    "                    else:\n",
    "                        entity_averages[entity_name][\"entity_db_ids\"].append(entity_db_id)\n",
    "                        entity_averages[entity_name][\"bounds_keys\"].append(bounds_key)\n",
    "                        entity_averages[entity_name][\"sentiment_scores\"].append(avg)\n",
    "                        entity_averages[entity_name][\"text\"].append(\n",
    "                            article_text[bounds_key[0]:bounds_key[1]])\n",
    "\n",
    "        # print('Sentiment Scores Format: [Neutral, Positive, Negative]')\n",
    "        for entity_name, averages in entity_averages.items():\n",
    "            entity_db_id = averages['entity_db_ids'][0]\n",
    "            # print(f\"Averages for {entity_name} (Entity DB ID: {entity_db_id}):\")\n",
    "            sentiment_scores = averages['sentiment_scores']\n",
    "            text = averages['text']\n",
    "            bounds_keys = averages['bounds_keys']\n",
    "\n",
    "            for i, scores in enumerate(sentiment_scores):\n",
    "                print(\"Sentiment Scores:\", scores)\n",
    "                print(\"Text:\", text[i])\n",
    "                print(\"Bounds Keys:\", bounds_keys[i])\n",
    "                print()\n",
    "\n",
    "                DatabaseUtils.insert_bound_mention_data(entity_name, source_article_id,\n",
    "                                                        entity_db_id,\n",
    "                                                        scores, text[i],\n",
    "                                                        bounds_keys[i])\n",
    "\n",
    "            num_bound = len(averages['sentiment_scores'])\n",
    "            scaled_classification = scaling(averages['sentiment_scores'],\n",
    "                                            k=EXPONENTIAL_K_VALUE)\n",
    "\n",
    "            # Can't scale an array of [0, 0, 0] -> Divide by zero error.\n",
    "            if sum(scaled_classification) == 0:\n",
    "                # print(scaled_classification)\n",
    "                continue\n",
    "\n",
    "            exp_percent = percentage_contribution(scaled_classification)\n",
    "\n",
    "            linear_scaled_classification = scaling(averages['sentiment_scores'],\n",
    "                                                   linear=True)\n",
    "            linear_percent = percentage_contribution(\n",
    "                linear_scaled_classification)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-25T01:18:27.403285Z",
     "start_time": "2024-02-25T01:18:27.401523Z"
    }
   },
   "id": "7d13e6016e10f60c"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "import nltk\n",
    "import ppdeep\n",
    "from lexicalrichness import LexicalRichness\n",
    "\n",
    "\n",
    "def calculate_statistics(text_body):\n",
    "    lex = LexicalRichness(text_body)\n",
    "    common_stop_words = [\"the\", \"and\", \"is\", \"of\", \"in\", \"it\", \"that\", \"to\", \"with\"]\n",
    "    tokens = nltk.word_tokenize(text_body)\n",
    "    stop_word_counts = {word: tokens.count(word) for word in common_stop_words}\n",
    "\n",
    "    try:\n",
    "        vocd_int = lex.vocd()\n",
    "    except ValueError:\n",
    "        vocd_int = None\n",
    "\n",
    "    linguistic_stats = {\n",
    "        \"fuzzy_hash\": ppdeep.hash(text_body),\n",
    "        \"word_count\": len(tokens),\n",
    "        \"terms_count\": lex.terms,\n",
    "        \"vocd\": vocd_int,\n",
    "        \"yulek\": lex.yulek,\n",
    "        \"simpsond\": lex.simpsond,\n",
    "        \"the_count\": stop_word_counts[\"the\"],\n",
    "        \"and_count\": stop_word_counts[\"and\"],\n",
    "        \"is_count\": stop_word_counts[\"is\"],\n",
    "        \"of_count\": stop_word_counts[\"of\"],\n",
    "        \"in_count\": stop_word_counts[\"in\"],\n",
    "        \"it_count\": stop_word_counts[\"it\"],\n",
    "        \"that_count\": stop_word_counts[\"that\"],\n",
    "        \"to_count\": stop_word_counts[\"to\"],\n",
    "        \"with_count\": stop_word_counts[\"with\"],\n",
    "    }\n",
    "\n",
    "    return linguistic_stats\n",
    "\n",
    "\n",
    "def calculate_percentage_difference(value1, value2):\n",
    "    if value1 is not None and value2 is not None and max(value1, value2) != 0:\n",
    "        return abs((value1 - value2) / max(value1, value2)) * 100\n",
    "    elif value1 is not None or value2 is not None:\n",
    "        return 100\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def calculate_all_percentage_differences(pair):\n",
    "    stat1 = pair.article1\n",
    "    stat2 = pair.article2\n",
    "    pair.words_diff = calculate_percentage_difference(stat1.word_count, stat2.word_count)\n",
    "    pair.terms_diff = calculate_percentage_difference(stat1.terms_count, stat2.terms_count)\n",
    "    pair.vocd_diff = calculate_percentage_difference(stat1.vocd, stat2.vocd)\n",
    "    pair.yulek_diff = calculate_percentage_difference(stat1.yulek, stat2.yulek)\n",
    "    pair.simpsond_diff = calculate_percentage_difference(stat1.simpsond, stat2.simpsond)\n",
    "    pair.the_diff = calculate_percentage_difference(stat1.the_count, stat2.the_count)\n",
    "    pair.and_diff = calculate_percentage_difference(stat1.and_count, stat2.and_count)\n",
    "    pair.is_diff = calculate_percentage_difference(stat1.is_count, stat2.is_count)\n",
    "    pair.of_diff = calculate_percentage_difference(stat1.of_count, stat2.of_count)\n",
    "    pair.in_diff = calculate_percentage_difference(stat1.in_count, stat2.in_count)\n",
    "    pair.to_diff = calculate_percentage_difference(stat1.to_count, stat2.to_count)\n",
    "    pair.it_diff = calculate_percentage_difference(stat1.it_count, stat2.it_count)\n",
    "    pair.that_diff = calculate_percentage_difference(stat1.that_count, stat2.that_count)\n",
    "    pair.with_diff = calculate_percentage_difference(stat1.with_count, stat2.with_count)\n",
    "\n",
    "    pair.avg_count_diff = pair.calculate_average_diff()\n",
    "    pair.save()\n",
    "\n",
    "\n",
    "class ArticleUpdate:\n",
    "    def __init__(self, text_body, article_model):\n",
    "        try:\n",
    "            nltk.data.find('tokenizers/punkt')\n",
    "        except LookupError:\n",
    "            nltk.download('punkt')\n",
    "\n",
    "        self.text_body = text_body  # Added by trafilatura\n",
    "        self.linguistic_stats = None\n",
    "        self.article_model = article_model\n",
    "\n",
    "    def get_statistics(self):\n",
    "        self.linguistic_stats = calculate_statistics(self.text_body)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-25T01:18:28.075601Z",
     "start_time": "2024-02-25T01:18:28.071289Z"
    }
   },
   "id": "133bfd2155d0083d"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d0a74e9292e2ac54"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "import urllib.robotparser\n",
    "from collections import defaultdict\n",
    "from functools import reduce\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from textblob import TextBlob\n",
    "from constants import (ENTITY_THRESHOLD_PERCENT,\n",
    "                        MENTION_REQ_PER,\n",
    "                        MERGE_REMOVAL_INDICATOR,\n",
    "                        COMBINED_REMOVAL_INDICATOR,\n",
    "                        COMBINED_CLUSTER_ID_SEPARATOR,\n",
    "                        SIMILAR_SEARCH_DAYS,\n",
    "                        PREVIEW_IMG_TIMEOUT)\n",
    "\n",
    "\n",
    "def can_fetch_url(url_to_check):\n",
    "    \"\"\"Determine if the URL can be fetched by all crawlers - adding politeness / adherence to robot\n",
    "    policy.\"\"\"\n",
    "    parsed_url = urlparse(url_to_check)\n",
    "    base_url = parsed_url.scheme + \"://\" + parsed_url.netloc\n",
    "    rules = urllib.robotparser.RobotFileParser()\n",
    "    rules.set_url(base_url + \"/robots.txt\")\n",
    "    rules.read()\n",
    "    return rules.can_fetch(\"*\", url_to_check)\n",
    "\n",
    "\n",
    "def get_preview_image_url(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=PREVIEW_IMG_TIMEOUT)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            og_image = soup.find('meta', property='og:image')\n",
    "            if og_image:\n",
    "                return og_image['content']\n",
    "\n",
    "            # Twitter Card image tag\n",
    "            twitter_image = soup.find(name='twitter:image')\n",
    "            if twitter_image:\n",
    "                return twitter_image['content']\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching preview image URL: {e}\")\n",
    "\n",
    "\n",
    "def merge_positions(entities, word):\n",
    "    \"\"\"Merge all instances of the same entity into a single entry with multiple\n",
    "    positions. Where same means a lowercase word match. Place them into an\n",
    "    'entities' dictionary.\"\"\"\n",
    "    entity_key = (word.text + word.label_).lower()\n",
    "    if entity_key in entities:\n",
    "        entities[entity_key][1].append([word.start_char, word.end_char])\n",
    "    else:\n",
    "        entities[entity_key] = [word.text, [[word.start_char, word\n",
    "        .end_char]], word.label_]\n",
    "    return entities\n",
    "\n",
    "\n",
    "def cleanse_cluster_text(cluster_text):\n",
    "    return [word.strip() for word in cluster_text if word.lower().strip() not\n",
    "            in undesired_words]\n",
    "\n",
    "\n",
    "undesired_words = [\"i\", \"he\", \"his\", \"she\", \"they\", \"it\", \"this\", \"that\",\n",
    "                   \"these\", \"those\", \"the\", \"a\", \"an\", \"of\"]\n",
    "\n",
    "\n",
    "def remove_titles(text):\n",
    "    title_pattern = r\"^(Mr|Mrs|Ms|Miss|Dr|Prof|Rev|Capt|Sir|Madam|Mx|Esq|Hon|Gen|Col|Sgt|Fr|Sr|Jr|Lord|Lady)\\s\"\n",
    "    text = re.sub(title_pattern, \"\", text)\n",
    "\n",
    "    '''10th Nov adding as:\n",
    "        {'Entity Name': 'Keith', 'Positions': [[11664, 11669], [12301, 12306], [14453, 14458], \n",
    "        [15005, 15010], [15286, 15291]], 'Label': 'PERSON', 'Num Positions': 5, 'Cluster Info': \n",
    "        {'Cluster ID': 12, 'Cluster Text': ['Keith', 'Keith', 'Keith', 'Keith', 'Hugo Keith KC',\n",
    "        'Keith', 'Keith', 'Keith'], 'Cluster Positions': [(11664, 11669), (12301, 12306), \n",
    "        (12312, 12314), (13026, 13031), (13464, 13469), (14374, 14387), (14453, 14458),\n",
    "        (15005, 15010), (15286, 15291)]}}\n",
    "        would have been updated to Hugo Keith had it not been for KC which made it 3 words'''\n",
    "\n",
    "    # Pattern for titles at the end\n",
    "    title_pattern_end = r\"\\s*(KC|QC)\\s*$\"\n",
    "    text = re.sub(title_pattern_end, \"\", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def insert_intervals(initial_list, new_values):\n",
    "    \"\"\"The insert_intervals function enables the segmentation of sentences provided by\n",
    "    TextBlob (as shown in the cell below) to be further divided into smaller boundaries. This\n",
    "     division is based on the identification of specific points where it is deemed necessary\n",
    "     to split sentences, particularly within the context of news articles.\"\"\"\n",
    "\n",
    "    def insert_recursive(intervals, values):\n",
    "        if not values:\n",
    "            return intervals  # Base case: Return the intervals when there are no more values to insert.\n",
    "\n",
    "        value = values[0]\n",
    "        result = []\n",
    "        for interval in intervals:\n",
    "            if interval[0] <= value <= interval[1]:\n",
    "                # If the value falls within an existing interval, split the interval into two parts.\n",
    "                # The first part goes from the interval's start to the value (inclusive), and\n",
    "                # the second part goes from the value+1 to the interval's end.\n",
    "                if interval[0] < value:\n",
    "                    # To mess around with intervals change value + - offset here.\n",
    "                    result.append((interval[0], value))\n",
    "                if value < interval[1]:\n",
    "                    # To mess around with intervals change value + - offset here.\n",
    "                    result.append((value + 1, interval[1]))\n",
    "            else:\n",
    "                # If the value doesn't fall within the interval, keep the interval as is.\n",
    "                result.append(interval)\n",
    "        # Recursively process other values.\n",
    "        return insert_recursive(result, values[1:])\n",
    "\n",
    "        # Recursive function call\n",
    "\n",
    "    updated_list = insert_recursive(initial_list, new_values)\n",
    "    return updated_list\n",
    "\n",
    "\n",
    "def is_substring(entity1, entity2):\n",
    "    return entity1.lower() in entity2.lower() or entity2.lower() in entity1.lower()\n",
    "\n",
    "\n",
    "def combine_entities(entities, cluster_text):\n",
    "    combined_entity = None\n",
    "\n",
    "    for entity1 in entities:\n",
    "        for entity2 in entities:\n",
    "            if entity1 != entity2:\n",
    "                combined1 = entity1 + ' ' + entity2\n",
    "                combined2 = entity2 + ' ' + entity1\n",
    "\n",
    "                if combined1 in cluster_text or combined2 in cluster_text:\n",
    "                    combined_entity = combined1 if combined1 in cluster_text \\\n",
    "                        else combined2\n",
    "                    break\n",
    "\n",
    "    return combined_entity\n",
    "\n",
    "\n",
    "def update_entity_name(entry):\n",
    "    \"\"\"Calls remove titles and removes possessives before checking if an entity name is a\n",
    "     substring of a 2 word entry in the coref cluster e.g. Johnson should become Boris Johnson\"\"\"\n",
    "\n",
    "    entity_name = entry['Entity Name']\n",
    "    cluster_text = entry['Cluster Info']['Cluster Text']\n",
    "\n",
    "    for text in cluster_text:\n",
    "        # Remove titles as not relevant\n",
    "        text = remove_titles(text)\n",
    "        # Replaces left / right quotation mark with standard single quotation mark\n",
    "        text = text.replace('’', \"'\").replace('‘', \"'\")\n",
    "        # Remove possessive markers for comparison\n",
    "        text = text.replace(\"’s\", \"\")\n",
    "        # Remove space and quote\n",
    "        text = text.replace(\" '\", \"\")\n",
    "        # Check if the current entity name is a substring of a 2-word cluster text entry\n",
    "        if len(text.split()) == 2 and entity_name in text:\n",
    "            entry['Entity Name'] = text\n",
    "            break\n",
    "    return entry\n",
    "\n",
    "\n",
    "def clean_up_substrings(clustered_entities):\n",
    "    longest_names = {}\n",
    "    entities_to_keep = []\n",
    "\n",
    "    # Identify longest names in cluster ID and remove shorter ones\n",
    "    for entity in clustered_entities:\n",
    "        cluster_id = entity['Cluster Info']['Cluster ID']\n",
    "        entity_name = entity['Entity Name']\n",
    "        current_longest = longest_names.get(cluster_id, \"\")\n",
    "\n",
    "        if len(entity_name) > len(current_longest):\n",
    "            # Remove the shorter entity without adding it to the list of entities to keep\n",
    "            if current_longest:\n",
    "                entities_to_keep = [e for e in clustered_entities if\n",
    "                                    e['Cluster Info']['Cluster ID'] != cluster_id]\n",
    "            longest_names[cluster_id] = entity_name\n",
    "\n",
    "            # Add the entity to the list of entities to keep\n",
    "            entities_to_keep.append(entity)\n",
    "\n",
    "    # Set merge indicator for entities with more than one associated name in the original list\n",
    "    for entity in clustered_entities:\n",
    "        cluster_id = entity['Cluster Info']['Cluster ID']\n",
    "        if len([e for e in entities_to_keep if\n",
    "                e['Cluster Info']['Cluster ID'] == cluster_id]) > 1:\n",
    "            entity['Num Positions'] = int(MERGE_REMOVAL_INDICATOR)\n",
    "            entity['Positions'] = int(MERGE_REMOVAL_INDICATOR)\n",
    "    return entities_to_keep\n",
    "\n",
    "\n",
    "def create_entity_entry(entity_name, positions, label, num_positions):\n",
    "    return {\n",
    "        'Entity Name': entity_name,\n",
    "        'Positions': positions,\n",
    "        'Label': label,\n",
    "        'Num Positions': num_positions,\n",
    "        'Cluster Info': []\n",
    "    }\n",
    "\n",
    "\n",
    "class Article:\n",
    "\n",
    "    def __init__(self, url, headline, text_body, NER, date, author, site_name):\n",
    "        self.url = url\n",
    "        self.NER = NER\n",
    "        self.headline = headline\n",
    "        self.image_url = None\n",
    "        self.description = None\n",
    "        self.text_body = text_body  # Added by trafilatura\n",
    "        self.coref_clusters = None\n",
    "        self.people_entities = None  # NER results here.\n",
    "        self.sentence_bounds = None\n",
    "        self.num_sentences = None\n",
    "        self.mention_threshold = None\n",
    "        self.entity_to_cluster_mapping = []\n",
    "        self.clustered_entities = None\n",
    "        self.database_candidate = False\n",
    "        self.database_id = None\n",
    "        self.bounds_sentiment = None\n",
    "        self.sentiment_analyser = None\n",
    "        self.publication_date = date\n",
    "        self.author = author\n",
    "        self.site_name = site_name\n",
    "        self.linguistic_stats = None\n",
    "\n",
    "    def set_sentiment_analyser(self, sa):\n",
    "\n",
    "        if sa is None:\n",
    "            self.sentiment_analyser = SentimentAnalyser()\n",
    "        else:\n",
    "            self.sentiment_analyser = sa\n",
    "\n",
    "    def get_bounds_sentiment(self):\n",
    "            self.bounds_sentiment = self.sentiment_analyser.process_clustered_entities(\n",
    "                clustered_entities=self.clustered_entities, sentence_bounds=self.sentence_bounds,\n",
    "                article_text=self.text_body, database_id=self.database_id,\n",
    "                debug=False)\n",
    "\n",
    "    def print_clustered_entities(self):\n",
    "        for entry in self.clustered_entities:\n",
    "            print(entry)\n",
    "            print()\n",
    "\n",
    "    def print_entity_to_cluster_mapping(self):\n",
    "        for entry in self.entity_to_cluster_mapping:\n",
    "            print(entry)\n",
    "            print()\n",
    "\n",
    "    def determine_entity_to_cluster_mapping(self):\n",
    "\n",
    "        \"\"\"\n",
    "        Removing pronouns & other 'useless words' (his, her, he, I etc) then doing a % match rate on\n",
    "        entity text across cluster entries for each entity. If match rate % exceeds threshold\n",
    "        pair them.\n",
    "\n",
    "        Improvement: Spliting the entity names into part words e.g. Sadiq Khan will be split into\n",
    "        Sadiq and Khan for evaluation purposes. This way if they are mostly mentioned by first\n",
    "        or second name the match still has an opportunity to take place.\n",
    "        \"\"\"\n",
    "\n",
    "        for entity_type, entities in self.people_entities.items():\n",
    "            for entity in entities:\n",
    "                entity_name, positions, label, num_positions = entity\n",
    "                entity_entry = create_entity_entry(entity_name, positions, label,\n",
    "                                                   num_positions)\n",
    "                self.process_clusters_for_entity(entity_entry, entity_name)\n",
    "\n",
    "    def process_clusters_for_entity(self, entity_entry, entity_name):\n",
    "        cluster_id = 0\n",
    "        for index, (cluster_text, cluster_positions, _) in self.coref_clusters:\n",
    "            cluster_id += 1\n",
    "\n",
    "            cluster_text = cleanse_cluster_text(cluster_text)\n",
    "            cleaned_cluster_text = [remove_titles(text) for text in cluster_text]\n",
    "\n",
    "            # Check if the length of cleaned_cluster_text is less than 4 elements\n",
    "            if len(cleaned_cluster_text) < 4:\n",
    "                continue  # Skip this cluster and move to the next one\n",
    "\n",
    "            total_coref_words = \" \".join(cleaned_cluster_text)\n",
    "            entity_parts = entity_name.split()\n",
    "            max_percentage = 0.0\n",
    "\n",
    "            winning_entity_part = None\n",
    "            for entity_part in entity_parts:\n",
    "                entity_count = total_coref_words.count(entity_part)\n",
    "                #  How well the entity name matches the cluster.\n",
    "                percentage = entity_count / len(cleaned_cluster_text)\n",
    "\n",
    "                # Is this a better match than the previous best match? If so\n",
    "                # make this the new best match.\n",
    "                if percentage > max_percentage:\n",
    "                    max_percentage = percentage\n",
    "\n",
    "            if max_percentage >= ENTITY_THRESHOLD_PERCENT:\n",
    "                cluster_entry = {\n",
    "                    'Cluster ID': cluster_id,\n",
    "                    'Cluster Text': cluster_text,\n",
    "                    'Cluster Positions': cluster_positions\n",
    "                }\n",
    "\n",
    "                entity_entry['Cluster Info'] = cluster_entry\n",
    "                self.entity_to_cluster_mapping.append(entity_entry)\n",
    "                break\n",
    "\n",
    "    def set_coref_clusters(self, sorted_combined_clusters):\n",
    "        # Add an ID to each cluster\n",
    "        self.coref_clusters = list(enumerate(sorted_combined_clusters))\n",
    "\n",
    "    def source_ner_people(self):\n",
    "\n",
    "        \"\"\"SpaCy is a popular NLP library that offers pre-trained models for various languages, and\n",
    "            its NER component is capable of recognising and categorising named entities within text.\n",
    "            It is utilised here to identify PERSON entities\"\"\"\n",
    "\n",
    "        NER = self.NER\n",
    "        article_text = self.text_body\n",
    "        article = NER(article_text)\n",
    "\n",
    "        '''# Recommended mention - 'Discard a cluster c in a document d if |Mc| ≤ 0.2|Sd|,  \n",
    "        where |...| is the number of mentions of a cluster (Mc) and sentences in a document (Sd)\n",
    "        (NEWS-MTSC approach)'''\n",
    "\n",
    "        entity_types = [\"CARDINAL\", \"DATE\", \"EVENT\", \"FAC\", \"GPE\", \"LANGUAGE\", \"LAW\",\n",
    "                        \"LOC\", \"MONEY\", \"NORP\", \"ORDINAL\", \"ORG\", \"PERCENT\",\n",
    "                        \"PERSON\", \"PRODUCT\", \"QUANTITY\", \"TIME\", \"WORK_OF_ART\"]\n",
    "\n",
    "        entity_type_to_entities = {\n",
    "            entity_type: [\n",
    "                [\n",
    "                    entity_text,\n",
    "                    positions,\n",
    "                    label,\n",
    "                    len(positions)\n",
    "                ] for entity_text, positions, label in reduce(\n",
    "                    merge_positions,\n",
    "                    filter(lambda word: word.label_ == entity_type, article.ents),\n",
    "                    {}\n",
    "                ).values()\n",
    "            ] for entity_type in entity_types\n",
    "        }\n",
    "\n",
    "        for entity_type, entities in entity_type_to_entities.items():\n",
    "            print(f\"Entity Type: {entity_type}\")\n",
    "            for entity in entities:\n",
    "                entity_text, positions, label, num_positions = entity\n",
    "                print(f\"Entity: {entity_text}\")\n",
    "                print(f\"Positions: {positions}\")\n",
    "                print(f\"Label: {label}\")\n",
    "                print(f\"Number of Positions: {num_positions}\")\n",
    "                print()\n",
    "\n",
    "        people_entities = {entity_type: entity_info for entity_type, entity_info in\n",
    "                           entity_type_to_entities.items() if entity_type == 'PERSON'}\n",
    "\n",
    "        for entity in entity_type_to_entities[entity_type]:\n",
    "            entity_text, positions, label, num_positions = entity\n",
    "            if entity_type not in people_entities:\n",
    "                people_entities[entity_type] = []\n",
    "            people_entities[entity_type].append({\n",
    "                'Entity': entity_text,\n",
    "                'Positions': positions,\n",
    "                'Label': label,\n",
    "                'Number of Positions': num_positions\n",
    "            })\n",
    "\n",
    "        self.people_entities = people_entities\n",
    "\n",
    "    def determine_sentences(self):\n",
    "\n",
    "        \"\"\"\n",
    "        Process the article text, tokenise by sentence and add custom adjustments to the\n",
    "        tokenisation using insert intervals below.\n",
    "        spaCy was not satisfactory for accurately tokenising for sentence start / end\n",
    "        characters. Trying textblob instead. TextBlob is a Python library for processing textual\n",
    "        data that is bulit upon NLTK.\n",
    "\n",
    "        TextBlob can provide me with the start and end of sentences by using the sentences\n",
    "        attribute of a TextBlob object. This attribute returns a list of Sentence objects, each\n",
    "        of which has a start and end property that indicates the index of the first and last\n",
    "        character of the sentence within the original text.\"\"\"\n",
    "\n",
    "        # Process the article text and adjust tokenization\n",
    "        article_text = self.text_body\n",
    "        blob = TextBlob(article_text)\n",
    "        sentences = blob.sentences\n",
    "\n",
    "        # Determine custom tokenization:\n",
    "        hyphen_sentences = re.split(r'\\n-', article_text)\n",
    "        # In testing article a new line and '-' hyphen use typically means consider\n",
    "        # a different sentence.\n",
    "        hyphen_nl_pos = [pos for pos, char in enumerate(article_text) if\n",
    "                         article_text[pos:pos + 2] == '\\n-']\n",
    "        extra_split = hyphen_nl_pos\n",
    "\n",
    "        sentence_bounds = [(int(sentence.start), int(sentence.end)) for sentence\n",
    "                           in sentences]\n",
    "\n",
    "        # Insert custom intervals\n",
    "        updated_list = insert_intervals(sentence_bounds, extra_split)\n",
    "\n",
    "        self.sentence_bounds = updated_list\n",
    "        self.num_sentences = len(list(sentence_bounds))\n",
    "        self.mention_threshold = math.floor(self.num_sentences * MENTION_REQ_PER)\n",
    "\n",
    "    def entity_cluster_map_consolidation(self):\n",
    "        \"\"\"\n",
    "        Purposes of below code:\n",
    "\n",
    "        1. Substring Matching: If one entity is a substring of another entity, they\n",
    "        are considered as candidates for consolidation. For example, if \"Rishi\" is a\n",
    "        substring of \"Rishi Sunak,\" the code merges them into the longer version, \"Rishi Sunak.\"\n",
    "\n",
    "        2. First and Second Name Combination: If there are two entities, one\n",
    "        representing the first name and the other representing the last name, and\n",
    "        they share the same coreference cluster, the code attempts to combine them\n",
    "        into a single entity.\n",
    "\n",
    "        3. 9th November - add while loop to see if continuing consolidation until no more\n",
    "        consolidation takes place results in a better consolidation as theorised.\n",
    "\n",
    "        4. 16th November - resolve instances of 'King\\n11:43' which should be King.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        cluster_dict = defaultdict(list)\n",
    "        for entry in self.entity_to_cluster_mapping:\n",
    "            entity_name = entry['Entity Name']\n",
    "            cluster_id = entry['Cluster Info']['Cluster ID']\n",
    "            cluster_dict[cluster_id].append(entry)\n",
    "\n",
    "        # While loop to continue consolidation until no more consolidation can be done\n",
    "        consolidation_done = True\n",
    "        while consolidation_done:\n",
    "            consolidation_done = False\n",
    "            for cluster_id, entries in cluster_dict.items():\n",
    "                if len(entries) > 1:\n",
    "                    combined_entry = None\n",
    "                    for i, entry1 in enumerate(entries):\n",
    "                        for j, entry2 in enumerate(entries):\n",
    "                            entry1 = update_entity_name(entry1)\n",
    "                            entry2 = update_entity_name(entry2)\n",
    "\n",
    "                            if i < j:\n",
    "                                entity_1_name = entry1['Entity Name']\n",
    "                                entity_2_name = entry2['Entity Name']\n",
    "\n",
    "                                cluster_text = entries[0]['Cluster Info']['Cluster Text']\n",
    "                                combined_entity = combine_entities([entity_1_name,\n",
    "                                                                    entity_2_name],\n",
    "                                                                   cluster_text)\n",
    "\n",
    "                                if is_substring(entity_1_name, entity_2_name):\n",
    "                                    if len(entity_1_name) > len(entity_2_name):\n",
    "                                        if entry2 in entries:\n",
    "                                            entries.remove(entry2)\n",
    "                                        # Coreference cluster will be used anyway -200 to\n",
    "                                        # indicate merge via removal.\n",
    "                                        entry1['Num Positions'] = int(MERGE_REMOVAL_INDICATOR)\n",
    "                                        entry1['Positions'] = int(MERGE_REMOVAL_INDICATOR)\n",
    "\n",
    "                                    else:\n",
    "                                        if entry1 in entries:\n",
    "                                            entries.remove(entry1)\n",
    "                                        # Coreference cluster will be used anyway -200 to\n",
    "                                        # indicate merge via removal.\n",
    "                                        entry2['Num Positions'] = int(MERGE_REMOVAL_INDICATOR)\n",
    "                                        entry2['Positions'] = int(MERGE_REMOVAL_INDICATOR)\n",
    "\n",
    "                                    consolidation_done = True\n",
    "                                    # exit inner for loop\n",
    "                                    break\n",
    "                                elif combined_entity in cluster_text:\n",
    "                                    combined_entry = {\n",
    "                                        'Entity Name': combined_entity,\n",
    "                                        # Coreference cluster will be used anyway -100 to\n",
    "                                        # indicate merge via combined entity.\n",
    "                                        'Positions': int(COMBINED_REMOVAL_INDICATOR),\n",
    "                                        'Label': entry1['Label'],\n",
    "                                        # Coreference cluster will be used anyway -100 to\n",
    "                                        # indicate merge via combined entity.\n",
    "                                        'Num Positions': int(COMBINED_REMOVAL_INDICATOR),\n",
    "                                        'Cluster Info': entry1['Cluster Info']\n",
    "                                    }\n",
    "                                    entries.remove(entry1)\n",
    "                                    entries.remove(entry2)\n",
    "                                    consolidation_done = True\n",
    "                                    # exit inner for loop\n",
    "                                    break\n",
    "                        if combined_entry:\n",
    "                            entries.append(combined_entry)\n",
    "                        if consolidation_done:\n",
    "                            # exit outer for loop\n",
    "                            break\n",
    "\n",
    "            '''\n",
    "            Now look across cluster ids (above we stayed within a cluster id) for substrings of\n",
    "            entity names and merge together cluster ids, positions and text.\n",
    "            '''\n",
    "            for cluster_id1, entries1 in cluster_dict.items():\n",
    "                for cluster_id2, entries2 in cluster_dict.items():\n",
    "                    if cluster_id1 != cluster_id2:  # Prevents comparing entries within the same\n",
    "                        # cluster\n",
    "                        for entry1 in entries1:\n",
    "                            for entry2 in entries2:\n",
    "                                entry1 = update_entity_name(entry1)\n",
    "                                entry2 = update_entity_name(entry2)\n",
    "\n",
    "                                entity_1_name = entry1['Entity Name']\n",
    "                                entity_2_name = entry2['Entity Name']\n",
    "                                # Check if an entity name is a substring of another.\n",
    "                                if entity_1_name in entity_2_name or entity_2_name in entity_1_name:\n",
    "                                    print('cross cluster merge triggered!')\n",
    "                                    print(entity_1_name)\n",
    "                                    print(entity_2_name)\n",
    "                                    # Combine cluster IDs with '0000' in between as a strong\n",
    "                                    # indicator.\n",
    "                                    combined_cluster_id = f\"{entry1['Cluster Info']['Cluster ID']}{COMBINED_CLUSTER_ID_SEPARATOR}{entry2['Cluster Info']['Cluster ID']}\"\n",
    "\n",
    "                                    # Set the new cluster ID\n",
    "                                    entry1['Cluster Info']['Cluster ID'] = combined_cluster_id\n",
    "                                    entry2['Cluster Info']['Cluster ID'] = combined_cluster_id\n",
    "\n",
    "                                    # Append cluster texts and positions to entry 1.\n",
    "                                    entry1['Cluster Info']['Cluster Text'].extend(\n",
    "                                        entry2['Cluster Info']['Cluster Text'])\n",
    "                                    entry1['Cluster Info']['Cluster Positions'].extend(\n",
    "                                        entry2['Cluster Info']['Cluster Positions'])\n",
    "\n",
    "                                    if len(entity_2_name) > len(entity_1_name):\n",
    "                                        entry1['Entity Name'] = entity_2_name\n",
    "                                    entries2.remove(entry2)\n",
    "                                    consolidation_done = True\n",
    "\n",
    "        clustered_entities = [entry for entries in cluster_dict.values() for entry in entries]\n",
    "\n",
    "        before_len = len(clustered_entities)\n",
    "\n",
    "        cleaned_entities = []\n",
    "\n",
    "        # Resolve \\n in entity name instances and any below mention threshold.\n",
    "        for entity in clustered_entities:\n",
    "            entity_name = entity['Entity Name']\n",
    "            cleaned_name = re.split(r'\\n\\d*:', entity_name)[0].strip()\n",
    "\n",
    "            entity_name = remove_titles(cleaned_name)\n",
    "\n",
    "            '''Found 'Entity Name': 'Reginald D. Hunter’s' - handle removing 's from last word'''\n",
    "\n",
    "            # Replaces left / right quotation mark with standard single quotation mark\n",
    "            entity_name = entity_name.replace('’', \"'\").replace('‘', \"'\")\n",
    "\n",
    "            # Handle the relatively common case of Meghan Markle '  i.e the space then quote mark\n",
    "            entity_name = entity_name.rstrip(\"'\")\n",
    "\n",
    "            # Split the entity name into words\n",
    "            words = entity_name.split()\n",
    "\n",
    "            # Check if the last word ends with 's, and if so, remove it\n",
    "            if words and words[-1].endswith(\"'s\"):\n",
    "                words[-1] = words[-1][:-2]  # Remove 's from the last word\n",
    "\n",
    "            # Join the words back into the entity name\n",
    "            cleaned_name = ' '.join(words)\n",
    "\n",
    "            # Capitalise the first letter of each word and make the rest lowercase\n",
    "            cleaned_name = ' '.join(word.capitalize() for word in words)\n",
    "\n",
    "            # Remove spaces @ start and end string\n",
    "            cleaned_name = cleaned_name.strip()\n",
    "\n",
    "            # Replace the original entity name with the cleaned name\n",
    "            entity['Entity Name'] = cleaned_name\n",
    "\n",
    "            cluster_positions = entity['Cluster Info']['Cluster Positions']\n",
    "            cluster_id = entity['Cluster Info']['Cluster ID']\n",
    "\n",
    "            # Count the number of entries in cluster_positions\n",
    "            num_entries = len(cluster_positions)\n",
    "\n",
    "            # Check if the number of entries is below the threshold\n",
    "            if num_entries < self.mention_threshold:\n",
    "                print(f\"Cluster ID {cluster_id} has {num_entries} entries, which is below the \"\n",
    "                      f\"threshold of {self.mention_threshold}.\")\n",
    "                # print(f\"Removing entity {entity['Entity Name']} with Cluster ID {cluster_id} due \"\n",
    "                #       f\"to low mention count:\")\n",
    "                # print(entity['Cluster Info'])\n",
    "                clustered_entities.remove(entity)\n",
    "                continue\n",
    "\n",
    "            # At most, an entity should have the first, middle and last name.\n",
    "            if len(cleaned_name.split()) > 3:\n",
    "                print(f\"Cluster ID {cluster_id} has {num_entries} entries, which is below the \"\n",
    "                      f\"threshold of {self.mention_threshold}.\")\n",
    "                print(\n",
    "                    f\"Removing entity {entity['Entity Name']} with Cluster ID {cluster_id} due \"\n",
    "                    f\"to low mention count:\")\n",
    "                print(entity['Cluster Info'])\n",
    "                clustered_entities.remove(entity)\n",
    "                continue\n",
    "\n",
    "            # If the entity is not removed, add it to the cleaned list\n",
    "            cleaned_entities.append(entity)\n",
    "\n",
    "        clustered_entities = cleaned_entities\n",
    "        new_length = len(clustered_entities)\n",
    "\n",
    "        print(f\"number of entities before mention threshold: {before_len}\")\n",
    "        print(f\"number of entities after mention threshold: {new_length}\")\n",
    "\n",
    "        clustered_entities = clean_up_substrings(clustered_entities)\n",
    "\n",
    "        self.clustered_entities = clustered_entities\n",
    "\n",
    "        # Is this article going to go on the web app? If clustered_entities > 0 then yes so get\n",
    "        # article parts and insert into database.\n",
    "        if new_length > 0:\n",
    "            self.set_database_candidate_true()\n",
    "\n",
    "    def set_database_candidate_true(self):\n",
    "        self.database_candidate = True\n",
    "\n",
    "    def get_average_sentiment_results(self):\n",
    "        self.sentiment_analyser.average_sentiment_results(self.database_id, self.bounds_sentiment,\n",
    "                                                          self\n",
    "                                                          .text_body)\n",
    "\n",
    "    def save_to_database(self):\n",
    "\n",
    "        self.image_url = get_preview_image_url(self.url)\n",
    "\n",
    "        print(f\"\"\"\n",
    "                fuzzy_hash={self.linguistic_stats[\"fuzzy_hash\"]},\n",
    "                word_count={self.linguistic_stats[\"word_count\"]},\n",
    "                terms_count={self.linguistic_stats[\"terms_count\"]},\n",
    "                vocd={self.linguistic_stats[\"vocd\"]},\n",
    "                yulek={self.linguistic_stats[\"yulek\"]},\n",
    "                simpsond={self.linguistic_stats[\"simpsond\"]},\n",
    "                the_count={self.linguistic_stats[\"the_count\"]},\n",
    "                and_count={self.linguistic_stats[\"and_count\"]},\n",
    "                is_count={self.linguistic_stats[\"is_count\"]},\n",
    "                of_count={self.linguistic_stats[\"of_count\"]},\n",
    "                in_count={self.linguistic_stats[\"in_count\"]},\n",
    "                to_count={self.linguistic_stats[\"to_count\"]},\n",
    "                it_count={self.linguistic_stats[\"it_count\"]},\n",
    "                that_count={self.linguistic_stats[\"that_count\"]},\n",
    "                with_count={self.linguistic_stats[\"with_count\"]},\n",
    "                \"\"\")\n",
    "\n",
    "    def get_statistics(self):\n",
    "        self.linguistic_stats = calculate_statistics(self.text_body)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-25T01:18:29.284867Z",
     "start_time": "2024-02-25T01:18:29.269337Z"
    }
   },
   "id": "76ea9ff938a16e0f"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import concurrent.futures\n",
    "import json\n",
    "import os\n",
    "import queue\n",
    "import urllib.robotparser\n",
    "import trafilatura\n",
    "import spacy\n",
    "import time\n",
    "import logging\n",
    "import urllib.request\n",
    "import socket\n",
    "\n",
    "\n",
    "from fastcoref import FCoref\n",
    "from urllib.parse import urlparse\n",
    "from urllib.robotparser import RobotFileParser\n",
    "\n",
    "\n",
    "def can_fetch_url(url_to_check):\n",
    "    \"\"\"Determine if the URL can be fetched by all crawlers - adding politeness / adherence to\n",
    "        robot policy.\"\"\"\n",
    "    parsed_url = urlparse(url_to_check)\n",
    "    base_url = parsed_url.scheme + \"://\" + parsed_url.netloc\n",
    "\n",
    "    rules = RobotFileParser()\n",
    "    try:\n",
    "        with urllib.request.urlopen(base_url + \"/robots.txt\", timeout=5) as response:\n",
    "            rules.parse(response.read().decode('utf-8').splitlines())\n",
    "        return rules.can_fetch(\"*\", url_to_check)\n",
    "    except urllib.error.URLError as e:\n",
    "        print(f\"Error accessing robots.txt: {e}\")\n",
    "    except socket.timeout as e:\n",
    "        print(f\"Timeout occurred: {e}\")\n",
    "\n",
    "    # Default to False as if Robot can't be checked then not compliant + the site may timeout.\n",
    "    return False\n",
    "\n",
    "\n",
    "def perform_coreference_resolution(article_texts, batch_size=100):\n",
    "    model = FCoref(device='mps')\n",
    "    predictions = model.predict(texts=article_texts, max_tokens_in_batch=batch_size)\n",
    "\n",
    "    # Empty list to store clusters for each article\n",
    "    article_text_clusters = []\n",
    "\n",
    "    for prediction in predictions:\n",
    "        clusters_text = prediction.get_clusters()\n",
    "        clusters_positions = prediction.get_clusters(as_strings=False)\n",
    "        combined_clusters = [(text, positions, len(text)) for text, positions in\n",
    "                             zip(clusters_text, clusters_positions)]\n",
    "        sorted_combined_clusters = sorted(combined_clusters, key=lambda x: x[2], reverse=True)\n",
    "\n",
    "        article_text_clusters.append(sorted_combined_clusters)\n",
    "\n",
    "    return article_text_clusters\n",
    "\n",
    "\n",
    "def check_similarity_with_timeout(article_obj):\n",
    "    \"\"\"Wrapper function for check_similarity with a timeout.\"\"\"\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        future = executor.submit(article_obj.check_similarity)\n",
    "        try:\n",
    "            return future.result(timeout=30)  # Returns True/False based on similarity check\n",
    "        except concurrent.futures.TimeoutError:\n",
    "            print(\"Check Similarity timed out!\")\n",
    "            return False  # If we timeout, assume not too similar\n",
    "\n",
    "\n",
    "class Command:\n",
    "    help = 'Scrape articles and trigger NLP flow'\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(Command, self).__init__(*args, **kwargs)\n",
    "\n",
    "        self.sa_queue = queue.Queue()\n",
    "\n",
    "        self.max_concurrent_threads = 1\n",
    "        # Create 5 SentimentAnalyser objects (these take quite a bit of time to define!)\n",
    "        # Previously declared for each article object so quite a wasteful operation\n",
    "        # Queue for the 5 threads\n",
    "        for _ in range(self.max_concurrent_threads):\n",
    "            sa = SentimentAnalyser()\n",
    "            self.sa_queue.put(sa)\n",
    "\n",
    "    def process_articles(start=0, end=None):\n",
    "        \n",
    "        #  ***Demo notebook fill in with article of interest***\n",
    "        url =  'https://www.thenational.scot/politics/24129165.will-keir-starmer-run-questions-prime-minister/'\n",
    "        headline  = 'Will <b>Keir Starmer</b> run from questions as prime minister too?'\n",
    "        article_list = []\n",
    "        try:\n",
    "            if can_fetch_url(url):\n",
    "                downloaded = trafilatura.fetch_url(url)\n",
    "                # Extract metadata\n",
    "                metadata = trafilatura.extract_metadata(downloaded)\n",
    "                # print(metadata.date)\n",
    "\n",
    "                # Extract publication date\n",
    "                date_str = metadata.date\n",
    "                naive_datetime = datetime.strptime(date_str, '%Y-%m-%d')\n",
    "\n",
    "                # datetime aware to a satisfy model\n",
    "                publication_date = naive_datetime\n",
    "\n",
    "                # Extract some useful trafilatura metadata.\n",
    "                author = metadata.author\n",
    "\n",
    "                site_name = metadata.sitename\n",
    "                article_text = trafilatura.extract(downloaded, favour_recall=True,\n",
    "                                                   include_comments=False, include_images=False,\n",
    "                                                   include_tables=False)\n",
    "\n",
    "                if article_text and len(article_text) > 249:\n",
    "                    article_obj = Article(url, headline, article_text, None,  publication_date,\n",
    "                                          author, site_name)\n",
    "                    article_list.append(article_obj)\n",
    "                elif article_text is None:\n",
    "                    print('article text is None')\n",
    "                elif len(article_text) < 250:\n",
    "                    print(article_text)\n",
    "                    \n",
    "        \n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing article: {url}\")\n",
    "            print(f\"Error message: {str(e)}\")\n",
    "            \n",
    "        return article_list\n",
    "\n",
    "    def process_file(self):\n",
    "        \n",
    "        return Command.process_articles()\n",
    "\n",
    "    def process_article(self, article, sa):\n",
    "        \n",
    "        start_time = time.time()\n",
    "        article.source_ner_people()\n",
    "        # print(f\"NER People Time: {time.time() - start_time} seconds\")\n",
    "\n",
    "        start_time = time.time()\n",
    "        article.determine_sentences()\n",
    "        # print(f\"Sentence determined Time: {time.time() - start_time} seconds\")\n",
    "\n",
    "        start_time = time.time()\n",
    "        article.determine_entity_to_cluster_mapping()\n",
    "        # print(f\"Entity to cluster map time: {time.time() - start_time} seconds\")\n",
    "\n",
    "        start_time = time.time()\n",
    "        article.entity_cluster_map_consolidation()\n",
    "        # print(f\"Entity cluster map consolidation Time: {time.time() - start_time} seconds\")\n",
    "\n",
    "        time.time()\n",
    "        if article.database_candidate:\n",
    "            # article.save_to_database() already done earlier in similar check now\n",
    "\n",
    "            if article.database_id != -1:\n",
    "                for entity_data in article.clustered_entities:\n",
    "                    entity_name = entity_data['Entity Name']\n",
    "                    entity_db_id = DatabaseUtils.insert_entity(entity_name, article.database_id)\n",
    "                    entity_data['entity_db_id'] = entity_db_id\n",
    "\n",
    "                article.set_sentiment_analyser(sa)\n",
    "\n",
    "                start_time = time.time()\n",
    "                article.get_bounds_sentiment()\n",
    "                print(f\"Bounds sentiment time: {time.time() - start_time} seconds\")\n",
    "\n",
    "                start_time = time.time()\n",
    "                article.get_average_sentiment_results()\n",
    "                # print(f\"Average results time: {time.time() - start_time} seconds\")\n",
    "                # < 0.5 seconds\n",
    "\n",
    "                article.set_db_processed(True, similar_rejection=False)\n",
    "\n",
    "        elif not article.database_candidate:\n",
    "            # print(\"Not enough mentions to add\")\n",
    "            article.set_db_processed(True, similar_rejection=False)\n",
    "        else:\n",
    "            print(\"Article already exists in the database\")\n",
    "\n",
    "        # Save memory by deleting properly!\n",
    "        # article.reset_attributes()\n",
    "        del article\n",
    "\n",
    "    def process_article_wrapper(self, args):\n",
    "        article, i, total_objects = args\n",
    "        print(f\"Current Progress: {i} of {total_objects}\")\n",
    "\n",
    "        sa = self.sa_queue.get()\n",
    "        try:\n",
    "            self.process_article(article, sa)\n",
    "\n",
    "        finally:\n",
    "            # Putting the SentimentAnalyser back in the queue, even if exception takes place\n",
    "            self.sa_queue.put(sa)\n",
    "\n",
    "    def go(self):\n",
    "        articles_from_file = self.process_file()\n",
    "\n",
    "        print()\n",
    "\n",
    "        # Extend article_objects with articles from the current file\n",
    "        article_objects = articles_from_file\n",
    "\n",
    "        article_texts = [article.text_body for article in article_objects]\n",
    "        article_text_clusters = perform_coreference_resolution(article_texts)\n",
    "\n",
    "        for article, clusters in zip(article_objects, article_text_clusters):\n",
    "            article.set_coref_clusters(clusters)\n",
    "\n",
    "        print(\"Length Article Objects: \")\n",
    "\n",
    "        total_objects = len(article_objects)\n",
    "        print(f\"Total objects: {total_objects}\")\n",
    "        logging.info(f\"Processing {total_objects} article objects\")\n",
    "\n",
    "        with concurrent.futures.ThreadPoolExecutor(\n",
    "                max_workers=self.max_concurrent_threads) as executor:\n",
    "            list(executor.map(self.process_article_wrapper,\n",
    "                              ((article, i + 1, len(article_objects)) for i, article in\n",
    "                               enumerate(article_objects))))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-25T01:18:29.931451Z",
     "start_time": "2024-02-25T01:18:29.928749Z"
    }
   },
   "id": "c8960c3d31a766c8"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/25/2024 01:18:33 - INFO - \t missing_keys: []\n",
      "02/25/2024 01:18:33 - INFO - \t unexpected_keys: []\n",
      "02/25/2024 01:18:33 - INFO - \t mismatched_keys: []\n",
      "02/25/2024 01:18:33 - INFO - \t error_msgs: []\n",
      "02/25/2024 01:18:33 - INFO - \t Model Parameters: 90.5M, Transformer: 82.1M, Coref head: 8.4M\n",
      "02/25/2024 01:18:33 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/1 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6b0b4c33b0a94c4b803271b376b2beb4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/25/2024 01:18:33 - INFO - \t ***** Running Inference on 1 texts *****\n"
     ]
    },
    {
     "data": {
      "text/plain": "Inference:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e01dc6b467a248d0b2002dbce6ce7ee1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article 1:\n",
      "  Cluster 1:\n",
      "    Text: ['Keir Starmer', 'his', 'the Labour leader', 'Mr Starmer', 'He', 'he', 'I', 'he', 'he', 'Mr Starmer', 'he', 'He', 'he', 'I', 'He', 'he', 'he', 'I', 'Mr Starmer', 'I', 'he', 'I', 'Mr Starmer', 'he', 'he', 'I', 'He', 'he', 'The Labour leader', 'he', 'he', 'Mr Starmer, who dealt with cases of assisted dying when he was Director of Public Prosecutions,', 'he', 'I', 'I', 'he', 'he', 'Mr Starmer', 'I', 'I', 'he', 'his', 'Mr Starmer', 'He', 'The Labour leader', 'I', 'He', 'he', 'his']\n",
      "    Positions: [(0, 12), (132, 135), (174, 191), (298, 308), (434, 436), (442, 444), (543, 544), (574, 576), (1169, 1171), (1203, 1213), (1475, 1477), (1547, 1549), (1808, 1810), (1974, 1975), (2098, 2100), (2269, 2271), (2308, 2310), (2318, 2319), (2447, 2457), (2600, 2601), (2703, 2705), (2713, 2714), (2894, 2904), (2910, 2912), (3095, 3097), (3105, 3106), (3295, 3297), (3489, 3491), (3593, 3610), (3631, 3633), (3679, 3681), (3705, 3800), (3761, 3763), (3808, 3809), (3906, 3907), (4006, 4008), (4033, 4035), (4191, 4201), (4209, 4210), (4243, 4244), (4289, 4291), (4297, 4300), (4515, 4525), (4626, 4628), (4757, 4774), (4829, 4830), (4838, 4840), (4992, 4994), (5014, 5017)]\n",
      "\n",
      "  Cluster 2:\n",
      "    Text: ['Labour', 'Labour’s', 'We', 'We', 'the Labour Party', 'ourselves', 'we', 'we', 'We', 'we', 'Labour', 'We', 'Labour', 'the Labour Party', 'we', 'Labour', 'it', \"Labour's\", 'Labour', 'Labour', 'Labour', 'Labour', 'Labour', 'Labour']\n",
      "    Positions: [(178, 184), (598, 606), (629, 631), (642, 644), (709, 725), (734, 743), (764, 766), (814, 816), (1221, 1223), (1234, 1236), (2150, 2156), (2390, 2392), (2555, 2561), (2684, 2700), (2744, 2746), (2780, 2786), (2864, 2866), (3055, 3063), (3343, 3349), (3500, 3506), (3538, 3544), (3597, 3603), (4612, 4618), (4761, 4767)]\n",
      "\n",
      "  Cluster 3:\n",
      "    Text: ['Rishi Sunak', 'the cowardly Prime Minister', 'he', 'Mr Sunak', 'he', 'Mr Sunak', 'he', 'He', 'he', 'he', 'He', 'he', 'just him', 'he', 'he', 'Mr Sunak', 'the Prime Minister']\n",
      "    Positions: [(26, 37), (314, 341), (404, 406), (466, 474), (502, 504), (1555, 1563), (1612, 1614), (1635, 1637), (1676, 1678), (1756, 1758), (1818, 1820), (1879, 1881), (1916, 1924), (1992, 1994), (2047, 2049), (2181, 2189), (2649, 2667)]\n",
      "\n",
      "  Cluster 4:\n",
      "    Text: ['the country', 'the country', 'The country', 'the country', 'the country', 'this country']\n",
      "    Positions: [(778, 789), (842, 853), (1268, 1279), (2084, 2095), (2766, 2777), (3280, 3292)]\n",
      "\n",
      "  Cluster 5:\n",
      "    Text: ['the Mirror', 'the Mirror', 'your', 'Mirror']\n",
      "    Positions: [(582, 592), (980, 990), (1290, 1294), (1459, 1465)]\n",
      "\n",
      "  Cluster 6:\n",
      "    Text: ['Tory', 'Tory', 'the Tories', 'the Tories']\n",
      "    Positions: [(245, 249), (2111, 2115), (3303, 3313), (4570, 4580)]\n",
      "\n",
      "  Cluster 7:\n",
      "    Text: ['is', 'that', 'that']\n",
      "    Positions: [(1280, 1282), (1306, 1310), (1316, 1320)]\n",
      "\n",
      "  Cluster 8:\n",
      "    Text: ['this Government', 'itself', 'the Government']\n",
      "    Positions: [(1374, 1389), (1405, 1411), (4430, 4444)]\n",
      "\n",
      "  Cluster 9:\n",
      "    Text: ['Nigel Farage-linked outlet Reform UK, who claim they are pushing for Labour votes', 'they', 'them']\n",
      "    Positions: [(2486, 2567), (2534, 2538), (2578, 2582)]\n",
      "\n",
      "  Cluster 10:\n",
      "    Text: ['Parliament', 'Parliament', 'its']\n",
      "    Positions: [(2217, 2227), (3955, 3965), (3987, 3990)]\n",
      "\n",
      "  Cluster 11:\n",
      "    Text: ['the ex-Post Office chief Paula Vennells', 'her', 'her']\n",
      "    Positions: [(4127, 4166), (4182, 4185), (4235, 4238)]\n",
      "\n",
      "  Cluster 12:\n",
      "    Text: ['those who lent their vote to the Tories in 2019', 'their', 'them']\n",
      "    Positions: [(4541, 4588), (4556, 4561), (4598, 4602)]\n",
      "\n",
      "  Cluster 13:\n",
      "    Text: ['it', 'it']\n",
      "    Positions: [(48, 50), (537, 539)]\n",
      "\n",
      "  Cluster 14:\n",
      "    Text: ['the summer', 'the summer']\n",
      "    Positions: [(1049, 1059), (1148, 1158)]\n",
      "\n",
      "  Cluster 15:\n",
      "    Text: ['A recent poll for the Mirror', 'your poll']\n",
      "    Positions: [(962, 990), (1290, 1299)]\n",
      "\n",
      "  Cluster 16:\n",
      "    Text: ['Mirror readers', 'your']\n",
      "    Positions: [(1459, 1473), (1534, 1538)]\n",
      "\n",
      "  Cluster 17:\n",
      "    Text: ['is', 'that']\n",
      "    Positions: [(1995, 1997), (2032, 2036)]\n",
      "\n",
      "  Cluster 18:\n",
      "    Text: ['legislation Mr Sunak is trying to force through Parliament to revive the failing scheme', 'the legislation']\n",
      "    Positions: [(2169, 2256), (2284, 2299)]\n",
      "\n",
      "  Cluster 19:\n",
      "    Text: ['wasting', 'it']\n",
      "    Positions: [(2337, 2344), (2405, 2407)]\n",
      "\n",
      "  Cluster 20:\n",
      "    Text: ['threats from Nigel Farage-linked outlet Reform UK, who claim they are pushing for Labour votes', 'that']\n",
      "    Positions: [(2473, 2567), (2613, 2617)]\n",
      "\n",
      "  Cluster 21:\n",
      "    Text: ['the general election', 'the next election']\n",
      "    Positions: [(2872, 2892), (3425, 3442)]\n",
      "\n",
      "  Cluster 22:\n",
      "    Text: ['assisted dying', 'it']\n",
      "    Positions: [(3661, 3675), (3948, 3950)]\n",
      "\n",
      "  Cluster 23:\n",
      "    Text: ['a free vote on it', 'it']\n",
      "    Positions: [(3933, 3950), (4029, 4031)]\n",
      "\n",
      "  Cluster 24:\n",
      "    Text: ['give', 'it']\n",
      "    Positions: [(4174, 4178), (4280, 4282)]\n",
      "\n",
      "  Cluster 25:\n",
      "    Text: ['his predecessor Jeremy Corbyn', 'his']\n",
      "    Positions: [(5014, 5043), (5052, 5055)]\n",
      "\n",
      "Length Article Objects: \n",
      "Total objects: 1\n",
      "Entity Type: PERSON\n",
      "Entity: Keir Starmer\n",
      "Positions: [[0, 12]]\n",
      "Label: PERSON\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: Rishi Sunak\n",
      "Positions: [[26, 37]]\n",
      "Label: PERSON\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: Starmer\n",
      "Positions: [[301, 308], [1206, 1213], [2450, 2457], [2897, 2904], [3708, 3715], [4194, 4201], [4518, 4525]]\n",
      "Label: PERSON\n",
      "Number of Positions: 7\n",
      "\n",
      "Entity: Sunak\n",
      "Positions: [[1558, 1563], [2184, 2189]]\n",
      "Label: PERSON\n",
      "Number of Positions: 2\n",
      "\n",
      "Entity: Nigel Farage\n",
      "Positions: [[2486, 2498]]\n",
      "Label: PERSON\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: Jeremy Hunt\n",
      "Positions: [[3367, 3378]]\n",
      "Label: PERSON\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: Paula Vennells\n",
      "Positions: [[4152, 4166]]\n",
      "Label: PERSON\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: Boris Johnson\n",
      "Positions: [[4878, 4891]]\n",
      "Label: PERSON\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: Lord Cameron\n",
      "Positions: [[4896, 4908]]\n",
      "Label: PERSON\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: Jeremy Corbyn\n",
      "Positions: [[5030, 5043]]\n",
      "Label: PERSON\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity is not in the expected format\n",
      "Article has this many sentences based on length of sentence bounds:  43\n",
      "So the mention threshold is:  8\n",
      "After mapping, before consolidation steps: \n",
      "{'Entity Name': 'Keir Starmer', 'Positions': [[0, 12]], 'Label': 'PERSON', 'Num Positions': 1, 'Cluster Info': {'Cluster ID': 1, 'Cluster Text': ['Keir Starmer', 'the Labour leader', 'Mr Starmer', 'Mr Starmer', 'Mr Starmer', 'Mr Starmer', 'The Labour leader', 'Mr Starmer, who dealt with cases of assisted dying when he was Director of Public Prosecutions,', 'Mr Starmer', 'Mr Starmer', 'The Labour leader'], 'Cluster Positions': [(0, 12), (132, 135), (174, 191), (298, 308), (434, 436), (442, 444), (543, 544), (574, 576), (1169, 1171), (1203, 1213), (1475, 1477), (1547, 1549), (1808, 1810), (1974, 1975), (2098, 2100), (2269, 2271), (2308, 2310), (2318, 2319), (2447, 2457), (2600, 2601), (2703, 2705), (2713, 2714), (2894, 2904), (2910, 2912), (3095, 3097), (3105, 3106), (3295, 3297), (3489, 3491), (3593, 3610), (3631, 3633), (3679, 3681), (3705, 3800), (3761, 3763), (3808, 3809), (3906, 3907), (4006, 4008), (4033, 4035), (4191, 4201), (4209, 4210), (4243, 4244), (4289, 4291), (4297, 4300), (4515, 4525), (4626, 4628), (4757, 4774), (4829, 4830), (4838, 4840), (4992, 4994), (5014, 5017)]}}\n",
      "\n",
      "{'Entity Name': 'Rishi Sunak', 'Positions': [[26, 37]], 'Label': 'PERSON', 'Num Positions': 1, 'Cluster Info': {'Cluster ID': 3, 'Cluster Text': ['Rishi Sunak', 'the cowardly Prime Minister', 'Mr Sunak', 'Mr Sunak', 'just him', 'Mr Sunak', 'the Prime Minister'], 'Cluster Positions': [(26, 37), (314, 341), (404, 406), (466, 474), (502, 504), (1555, 1563), (1612, 1614), (1635, 1637), (1676, 1678), (1756, 1758), (1818, 1820), (1879, 1881), (1916, 1924), (1992, 1994), (2047, 2049), (2181, 2189), (2649, 2667)]}}\n",
      "\n",
      "{'Entity Name': 'Starmer', 'Positions': [[301, 308], [1206, 1213], [2450, 2457], [2897, 2904], [3708, 3715], [4194, 4201], [4518, 4525]], 'Label': 'PERSON', 'Num Positions': 7, 'Cluster Info': {'Cluster ID': 1, 'Cluster Text': ['Keir Starmer', 'the Labour leader', 'Mr Starmer', 'Mr Starmer', 'Mr Starmer', 'Mr Starmer', 'The Labour leader', 'Mr Starmer, who dealt with cases of assisted dying when he was Director of Public Prosecutions,', 'Mr Starmer', 'Mr Starmer', 'The Labour leader'], 'Cluster Positions': [(0, 12), (132, 135), (174, 191), (298, 308), (434, 436), (442, 444), (543, 544), (574, 576), (1169, 1171), (1203, 1213), (1475, 1477), (1547, 1549), (1808, 1810), (1974, 1975), (2098, 2100), (2269, 2271), (2308, 2310), (2318, 2319), (2447, 2457), (2600, 2601), (2703, 2705), (2713, 2714), (2894, 2904), (2910, 2912), (3095, 3097), (3105, 3106), (3295, 3297), (3489, 3491), (3593, 3610), (3631, 3633), (3679, 3681), (3705, 3800), (3761, 3763), (3808, 3809), (3906, 3907), (4006, 4008), (4033, 4035), (4191, 4201), (4209, 4210), (4243, 4244), (4289, 4291), (4297, 4300), (4515, 4525), (4626, 4628), (4757, 4774), (4829, 4830), (4838, 4840), (4992, 4994), (5014, 5017)]}}\n",
      "\n",
      "{'Entity Name': 'Sunak', 'Positions': [[1558, 1563], [2184, 2189]], 'Label': 'PERSON', 'Num Positions': 2, 'Cluster Info': {'Cluster ID': 3, 'Cluster Text': ['Rishi Sunak', 'the cowardly Prime Minister', 'Mr Sunak', 'Mr Sunak', 'just him', 'Mr Sunak', 'the Prime Minister'], 'Cluster Positions': [(26, 37), (314, 341), (404, 406), (466, 474), (502, 504), (1555, 1563), (1612, 1614), (1635, 1637), (1676, 1678), (1756, 1758), (1818, 1820), (1879, 1881), (1916, 1924), (1992, 1994), (2047, 2049), (2181, 2189), (2649, 2667)]}}\n",
      "\n",
      "number of entities before mention threshold: 2\n",
      "number of entities after mention threshold: 2\n",
      "After consolidation steps: \n",
      "{'Entity Name': 'Keir Starmer', 'Positions': -200, 'Label': 'PERSON', 'Num Positions': -200, 'Cluster Info': {'Cluster ID': 1, 'Cluster Text': ['Keir Starmer', 'the Labour leader', 'Mr Starmer', 'Mr Starmer', 'Mr Starmer', 'Mr Starmer', 'The Labour leader', 'Mr Starmer, who dealt with cases of assisted dying when he was Director of Public Prosecutions,', 'Mr Starmer', 'Mr Starmer', 'The Labour leader'], 'Cluster Positions': [(0, 12), (132, 135), (174, 191), (298, 308), (434, 436), (442, 444), (543, 544), (574, 576), (1169, 1171), (1203, 1213), (1475, 1477), (1547, 1549), (1808, 1810), (1974, 1975), (2098, 2100), (2269, 2271), (2308, 2310), (2318, 2319), (2447, 2457), (2600, 2601), (2703, 2705), (2713, 2714), (2894, 2904), (2910, 2912), (3095, 3097), (3105, 3106), (3295, 3297), (3489, 3491), (3593, 3610), (3631, 3633), (3679, 3681), (3705, 3800), (3761, 3763), (3808, 3809), (3906, 3907), (4006, 4008), (4033, 4035), (4191, 4201), (4209, 4210), (4243, 4244), (4289, 4291), (4297, 4300), (4515, 4525), (4626, 4628), (4757, 4774), (4829, 4830), (4838, 4840), (4992, 4994), (5014, 5017)]}}\n",
      "\n",
      "{'Entity Name': 'Rishi Sunak', 'Positions': -200, 'Label': 'PERSON', 'Num Positions': -200, 'Cluster Info': {'Cluster ID': 3, 'Cluster Text': ['Rishi Sunak', 'the cowardly Prime Minister', 'Mr Sunak', 'Mr Sunak', 'just him', 'Mr Sunak', 'the Prime Minister'], 'Cluster Positions': [(26, 37), (314, 341), (404, 406), (466, 474), (502, 504), (1555, 1563), (1612, 1614), (1635, 1637), (1676, 1678), (1756, 1758), (1818, 1820), (1879, 1881), (1916, 1924), (1992, 1994), (2047, 2049), (2181, 2189), (2649, 2667)]}}\n",
      "\n",
      "Would now insert Keir Starmer into entities database\n",
      "Would now insert Rishi Sunak into entities database\n",
      "\u001B[0mKeir Starmer - Mention (0, 12) is within bounds (0, 128)\n",
      "\u001B[0m\u001B[94mKeir Starmer\u001B[0m tonight told Rishi Sunak to “bring it on” and warned voters can't afford to wait any longer for a general election.\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidate appearance 1\u001B[0m\n",
      "\u001B[0mKeir Starmer - Mention (132, 135) is within bounds (129, 293)\n",
      "\u001B[0mIn \u001B[94mhis\u001B[0m first interview of the election year, the Labour leader said voters can’t afford to wait for another year of Tory misery as mortgages rise and bills pile up.\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidate appearance 1\u001B[0m\n",
      "\u001B[0mKeir Starmer - Mention (174, 191) is within bounds (129, 293)\n",
      "\u001B[0mIn his first interview of the election year, \u001B[94mthe Labour leader\u001B[0m said voters can’t afford to wait for another year of Tory misery as mortgages rise and bills pile up.\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidate appearance 2\u001B[0m\n",
      "\u001B[0mKeir Starmer - Mention (298, 308) is within bounds (294, 433)\n",
      "\u001B[0mBut \u001B[94mMr Starmer\u001B[0m said the cowardly Prime Minister is running scared from calling an election as “everyone knows he’s been an abject failure”.\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidate appearance 1\u001B[0m\n",
      "\u001B[0mKeir Starmer - Mention (434, 436) is within bounds (434, 529)\n",
      "\u001B[0m\u001B[94mHe\u001B[0m said he was ready to take on Mr Sunak and rubbished reports that he wanted to duck scrutiny.\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidate appearance 1\u001B[0m\n",
      "\u001B[0mKeir Starmer - Mention (442, 444) is within bounds (434, 529)\n",
      "\u001B[0mHe said \u001B[94mhe\u001B[0m was ready to take on Mr Sunak and rubbished reports that he wanted to duck scrutiny.\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidate appearance 2\u001B[0m\n",
      "\u001B[0mKeir Starmer - Mention (543, 544) is within bounds (530, 549)\n",
      "\u001B[0m“Bring it on \u001B[94mI\u001B[0m say.\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidate appearance 1\u001B[0m\n",
      "\u001B[0mKeir Starmer - Mention (574, 576) is within bounds (550, 627)\n",
      "\u001B[0mBring on the campaign,” \u001B[94mhe\u001B[0m told the Mirror from Labour’s London headquarters.\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidate appearance 1\u001B[0m\n",
      "\u001B[0mKeir Starmer - Mention (1169, 1171) is within bounds (1160, 1267)\n",
      "\u001B[0mAsked if \u001B[94mhe\u001B[0m was ready for a snap election, Mr Starmer said: “We’re ready, we’ve been ready for a long time.\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidate appearance 1\u001B[0m\n",
      "\u001B[0mKeir Starmer - Mention (1203, 1213) is within bounds (1160, 1267)\n",
      "\u001B[0mAsked if he was ready for a snap election, \u001B[94mMr Starmer\u001B[0m said: “We’re ready, we’ve been ready for a long time.\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidate appearance 2\u001B[0m\n",
      "\u001B[0mKeir Starmer - Mention (1475, 1477) is within bounds (1312, 1633)\n",
      "\u001B[0mAnd that’s because people can’t afford to wait any longer for this Government to finally put itself before the electorate.”\n",
      "In a direct message to Mirror readers, \u001B[94mhe\u001B[0m said: \"Hope is on the way and the power of change is in your hands.”\n",
      "He said Mr Sunak was afraid to call an election as \"it's obvious he hasn't delivered\".\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidate appearance 1\u001B[0m\n",
      "\u001B[0mKeir Starmer - Mention (1547, 1549) is within bounds (1312, 1633)\n",
      "\u001B[0mAnd that’s because people can’t afford to wait any longer for this Government to finally put itself before the electorate.”\n",
      "In a direct message to Mirror readers, he said: \"Hope is on the way and the power of change is in your hands.”\n",
      "\u001B[94mHe\u001B[0m said Mr Sunak was afraid to call an election as \"it's obvious he hasn't delivered\".\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidate appearance 2\u001B[0m\n",
      "\u001B[0mKeir Starmer - Mention (1808, 1810) is within bounds (1634, 1816)\n",
      "\u001B[0m\"He's now getting into the position where he's cooking the books - the asylum backlog is the latest example of pretending he's solved problems rather than actually solving,\" \u001B[94mhe\u001B[0m said.\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidate appearance 1\u001B[0m\n",
      "\u001B[0mKeir Starmer - Mention (1974, 1975) is within bounds (1974, 2097)\n",
      "\u001B[0m\u001B[94mI\u001B[0m know the reason he is reluctant to call an election and that's because he hasn't got a record to put before the country.\"\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidate appearance 1\u001B[0m\n",
      "\u001B[0mKeir Starmer - Mention (2098, 2100) is within bounds (2098, 2257)\n",
      "\u001B[0m\u001B[94mHe\u001B[0m rubbished Tory Rwanda deportation plans and said Labour would ditch legislation Mr Sunak is trying to force through Parliament to revive the failing scheme.\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidate appearance 1\u001B[0m\n",
      "\u001B[0mKeir Starmer - Mention (2269, 2271) is within bounds (2258, 2389)\n",
      "\u001B[0mAsked what \u001B[94mhe\u001B[0m would do if the legislation passed, he said: \"I don't believe in wasting taxpayers money on gimmicks that don't work.\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidate appearance 1\u001B[0m\n",
      "\u001B[0mKeir Starmer - Mention (2308, 2310) is within bounds (2258, 2389)\n",
      "\u001B[0mAsked what he would do if the legislation passed, \u001B[94mhe\u001B[0m said: \"I don't believe in wasting taxpayers money on gimmicks that don't work.\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidate appearance 2\u001B[0m\n",
      "\u001B[0mKeir Starmer - Mention (2318, 2319) is within bounds (2258, 2389)\n",
      "\u001B[0mAsked what he would do if the legislation passed, he said: \"\u001B[94mI\u001B[0m don't believe in wasting taxpayers money on gimmicks that don't work.\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidate appearance 3\u001B[0m\n",
      "\u001B[0mKeir Starmer - Mention (2447, 2457) is within bounds (2410, 2568)\n",
      "\u001B[0mIf you can't see the poll click here\n",
      "\u001B[94mMr Starmer\u001B[0m also dismissed threats from Nigel Farage-linked outlet Reform UK, who claim they are pushing for Labour votes.\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidate appearance 1\u001B[0m\n",
      "\u001B[0mKeir Starmer - Mention (2600, 2601) is within bounds (2569, 2711)\n",
      "\u001B[0m\"Many of them are ex-Tories so \u001B[94mI\u001B[0m think that that's really about the weakness of the Prime Minister more than about the Labour Party,\" he said.\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidate appearance 1\u001B[0m\n",
      "\u001B[0mKeir Starmer - Mention (2703, 2705) is within bounds (2569, 2711)\n",
      "\u001B[0m\"Many of them are ex-Tories so I think that that's really about the weakness of the Prime Minister more than about the Labour Party,\" \u001B[94mhe\u001B[0m said.\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidate appearance 2\u001B[0m\n",
      "\u001B[0mKeir Starmer - Mention (2713, 2714) is within bounds (2712, 2779)\n",
      "\u001B[0m\"\u001B[94mI\u001B[0m'm confident in the case that we want to put before the country.\"\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidate appearance 1\u001B[0m\n",
      "\u001B[0mKeir Starmer - Mention (2894, 2904) is within bounds (2894, 3074)\n",
      "\u001B[0m\u001B[94mMr Starmer\u001B[0m said he wanted to lower the tax burden for working people but wouldn't be drawn on whether there would be cuts to income tax or national insurance in Labour's manifesto.\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidate appearance 1\u001B[0m\n",
      "\u001B[0mKeir Starmer - Mention (2910, 2912) is within bounds (2894, 3074)\n",
      "\u001B[0mMr Starmer said \u001B[94mhe\u001B[0m wanted to lower the tax burden for working people but wouldn't be drawn on whether there would be cuts to income tax or national insurance in Labour's manifesto.\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidate appearance 2\u001B[0m\n",
      "\u001B[0mKeir Starmer - Mention (3095, 3097) is within bounds (3075, 3294)\n",
      "\u001B[0mOn inheritance tax, \u001B[94mhe\u001B[0m said: \"I think even they [the Tories] know that more tax cuts for those that are better off without any benefit for the vast majority of working people cannot be the way forward for this country.\"\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidate appearance 1\u001B[0m\n",
      "\u001B[0mKeir Starmer - Mention (3105, 3106) is within bounds (3075, 3294)\n",
      "\u001B[0mOn inheritance tax, he said: \"\u001B[94mI\u001B[0m think even they [the Tories] know that more tax cuts for those that are better off without any benefit for the vast majority of working people cannot be the way forward for this country.\"\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidate appearance 2\u001B[0m\n",
      "\u001B[0mKeir Starmer - Mention (3295, 3297) is within bounds (3295, 3484)\n",
      "\u001B[0m\u001B[94mHe\u001B[0m said the Tories were trying to set traps for Labour after Chancellor Jeremy Hunt paved the way for a new austerity drive after the next election, with a major squeeze on public spending.\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidate appearance 1\u001B[0m\n",
      "\u001B[0mKeir Starmer - Mention (3489, 3491) is within bounds (3485, 3537)\n",
      "\u001B[0mBut \u001B[94mhe\u001B[0m added: \"Labour is not the party of austerity.\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidate appearance 1\u001B[0m\n",
      "\u001B[0mKeir Starmer - Mention (3593, 3610) is within bounds (3593, 3704)\n",
      "\u001B[0m\u001B[94mThe Labour leader\u001B[0m also signalled that he could allow a free vote on assisted dying if he gets the keys to No10.\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidate appearance 1\u001B[0m\n",
      "\u001B[0mKeir Starmer - Mention (3631, 3633) is within bounds (3593, 3704)\n",
      "\u001B[0mThe Labour leader also signalled that \u001B[94mhe\u001B[0m could allow a free vote on assisted dying if he gets the keys to No10.\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidate appearance 2\u001B[0m\n",
      "\u001B[0mKeir Starmer - Mention (3679, 3681) is within bounds (3593, 3704)\n",
      "\u001B[0mThe Labour leader also signalled that he could allow a free vote on assisted dying if \u001B[94mhe\u001B[0m gets the keys to No10.\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidate appearance 3\u001B[0m\n",
      "\u001B[0mKeir Starmer - Mention (3705, 3800) is within bounds (3705, 3851)\n",
      "\u001B[0m\u001B[94mMr Starmer, who dealt with cases of assisted dying when he was Director of Public Prosecutions,\u001B[0m said: \"I do think it's time for the law to change.\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidate appearance 1\u001B[0m\n",
      "\u001B[0mKeir Starmer - Mention (3761, 3763) is within bounds (3705, 3851)\n",
      "\u001B[0mMr Starmer, who dealt with cases of assisted dying when \u001B[94mhe\u001B[0m was Director of Public Prosecutions, said: \"I do think it's time for the law to change.\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidate appearance 2\u001B[0m\n",
      "\u001B[0mKeir Starmer - Mention (3808, 3809) is within bounds (3705, 3851)\n",
      "\u001B[0mMr Starmer, who dealt with cases of assisted dying when he was Director of Public Prosecutions, said: \"\u001B[94mI\u001B[0m do think it's time for the law to change.\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidate appearance 3\u001B[0m\n",
      "\u001B[0mKeir Starmer - Mention (3906, 3907) is within bounds (3852, 3996)\n",
      "\u001B[0m\"Any change would have to be carefully considered but \u001B[94mI\u001B[0m do think there should be a free vote on it and Parliament should properly have its say.\"\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidate appearance 1\u001B[0m\n",
      "\u001B[0mKeir Starmer - Mention (4006, 4008) is within bounds (3997, 4094)\n",
      "\u001B[0mAsked if \u001B[94mhe\u001B[0m would make time for it, he said a private members bill is \"a route we could go down\".\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidate appearance 1\u001B[0m\n",
      "\u001B[0mKeir Starmer - Mention (4033, 4035) is within bounds (3997, 4094)\n",
      "\u001B[0mAsked if he would make time for it, \u001B[94mhe\u001B[0m said a private members bill is \"a route we could go down\".\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidate appearance 2\u001B[0m\n",
      "\u001B[0mKeir Starmer - Mention (4191, 4201) is within bounds (4095, 4284)\n",
      "\u001B[0mAmid a growing row over whether the ex-Post Office chief Paula Vennells should give up her CBE, \u001B[94mMr Starmer\u001B[0m said: \"I think it's a matter for her and I can see why people are calling for it.\"\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidate appearance 1\u001B[0m\n",
      "\u001B[0mKeir Starmer - Mention (4209, 4210) is within bounds (4095, 4284)\n",
      "\u001B[0mAmid a growing row over whether the ex-Post Office chief Paula Vennells should give up her CBE, Mr Starmer said: \"\u001B[94mI\u001B[0m think it's a matter for her and I can see why people are calling for it.\"\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidate appearance 2\u001B[0m\n",
      "\u001B[0mKeir Starmer - Mention (4243, 4244) is within bounds (4095, 4284)\n",
      "\u001B[0mAmid a growing row over whether the ex-Post Office chief Paula Vennells should give up her CBE, Mr Starmer said: \"I think it's a matter for her and \u001B[94mI\u001B[0m can see why people are calling for it.\"\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidate appearance 3\u001B[0m\n",
      "\u001B[0mKeir Starmer - Mention (4289, 4291) is within bounds (4285, 4465)\n",
      "\u001B[0mBut \u001B[94mhe\u001B[0m said his priority would be getting compensation for postmasters who were wrongly jailed due to the \"shocking\" Horizon IT scandal and told the Government to \"get on with it\".\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidate appearance 1\u001B[0m\n",
      "\u001B[0mKeir Starmer - Mention (4297, 4300) is within bounds (4285, 4465)\n",
      "\u001B[0mBut he said \u001B[94mhis\u001B[0m priority would be getting compensation for postmasters who were wrongly jailed due to the \"shocking\" Horizon IT scandal and told the Government to \"get on with it\".\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidate appearance 2\u001B[0m\n",
      "\u001B[0mKeir Starmer - Mention (4515, 4525) is within bounds (4466, 4625)\n",
      "\u001B[0mIn a major speech in the South West on Thursday, \u001B[94mMr Starmer\u001B[0m will appeal to those who lent their vote to the Tories in 2019 and urge them to trust Labour again.\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidate appearance 1\u001B[0m\n",
      "\u001B[0mKeir Starmer - Mention (4626, 4628) is within bounds (4626, 4756)\n",
      "\u001B[0m\u001B[94mHe\u001B[0m will vow to clean up politics and crack down on cronyism, with plans for tougher sentences for people who defraud the taxpayer.\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidate appearance 1\u001B[0m\n",
      "\u001B[0mKeir Starmer - Mention (4757, 4774) is within bounds (4757, 4837)\n",
      "\u001B[0m\u001B[94mThe Labour leader\u001B[0m will warn: \"Nobody will be above the law in a Britain I lead.\"\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidate appearance 1\u001B[0m\n",
      "\u001B[0mKeir Starmer - Mention (4829, 4830) is within bounds (4757, 4837)\n",
      "\u001B[0mThe Labour leader will warn: \"Nobody will be above the law in a Britain \u001B[94mI\u001B[0m lead.\"\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidate appearance 2\u001B[0m\n",
      "\u001B[0mKeir Starmer - Mention (4838, 4840) is within bounds (4838, 4987)\n",
      "\u001B[0m\u001B[94mHe\u001B[0m will also hit out at top Tories like Boris Johnson and Lord Cameron, saying politics is not a “hobby” for people who “enjoy the feeling of power”.\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidate appearance 1\u001B[0m\n",
      "\u001B[0mKeir Starmer - Mention (4992, 4994) is within bounds (4988, 5107)\n",
      "\u001B[0mAnd \u001B[94mhe\u001B[0m'll take a swipe at his predecessor Jeremy Corbyn, vowing his premiership won't be \"vanity dressed up as virtue\".\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidate appearance 1\u001B[0m\n",
      "\u001B[0mKeir Starmer - Mention (5014, 5017) is within bounds (4988, 5107)\n",
      "\u001B[0mAnd he'll take a swipe at \u001B[94mhis\u001B[0m predecessor Jeremy Corbyn, vowing his premiership won't be \"vanity dressed up as virtue\".\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidate appearance 2\u001B[0m\n",
      "\u001B[0mRishi Sunak - Mention (26, 37) is within bounds (0, 128)\n",
      "\u001B[0mKeir Starmer tonight told \u001B[94mRishi Sunak\u001B[0m to “bring it on” and warned voters can't afford to wait any longer for a general election.\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidate appearance 1\u001B[0m\n",
      "\u001B[0mRishi Sunak - Mention (314, 341) is within bounds (294, 433)\n",
      "\u001B[0mBut Mr Starmer said \u001B[94mthe cowardly Prime Minister\u001B[0m is running scared from calling an election as “everyone knows he’s been an abject failure”.\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidate appearance 1\u001B[0m\n",
      "\u001B[0mRishi Sunak - Mention (404, 406) is within bounds (294, 433)\n",
      "\u001B[0mBut Mr Starmer said the cowardly Prime Minister is running scared from calling an election as “everyone knows \u001B[94mhe\u001B[0m’s been an abject failure”.\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidate appearance 2\u001B[0m\n",
      "\u001B[0mRishi Sunak - Mention (466, 474) is within bounds (434, 529)\n",
      "\u001B[0mHe said he was ready to take on \u001B[94mMr Sunak\u001B[0m and rubbished reports that he wanted to duck scrutiny.\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidate appearance 1\u001B[0m\n",
      "\u001B[0mRishi Sunak - Mention (502, 504) is within bounds (434, 529)\n",
      "\u001B[0mHe said he was ready to take on Mr Sunak and rubbished reports that \u001B[94mhe\u001B[0m wanted to duck scrutiny.\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidate appearance 2\u001B[0m\n",
      "\u001B[0mRishi Sunak - Mention (1555, 1563) is within bounds (1312, 1633)\n",
      "\u001B[0mAnd that’s because people can’t afford to wait any longer for this Government to finally put itself before the electorate.”\n",
      "In a direct message to Mirror readers, he said: \"Hope is on the way and the power of change is in your hands.”\n",
      "He said \u001B[94mMr Sunak\u001B[0m was afraid to call an election as \"it's obvious he hasn't delivered\".\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidate appearance 1\u001B[0m\n",
      "\u001B[0mRishi Sunak - Mention (1612, 1614) is within bounds (1312, 1633)\n",
      "\u001B[0mAnd that’s because people can’t afford to wait any longer for this Government to finally put itself before the electorate.”\n",
      "In a direct message to Mirror readers, he said: \"Hope is on the way and the power of change is in your hands.”\n",
      "He said Mr Sunak was afraid to call an election as \"it's obvious \u001B[94mhe\u001B[0m hasn't delivered\".\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidate appearance 2\u001B[0m\n",
      "\u001B[0mRishi Sunak - Mention (1635, 1637) is within bounds (1634, 1816)\n",
      "\u001B[0m\"\u001B[94mHe\u001B[0m's now getting into the position where he's cooking the books - the asylum backlog is the latest example of pretending he's solved problems rather than actually solving,\" he said.\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidate appearance 1\u001B[0m\n",
      "\u001B[0mRishi Sunak - Mention (1676, 1678) is within bounds (1634, 1816)\n",
      "\u001B[0m\"He's now getting into the position where \u001B[94mhe\u001B[0m's cooking the books - the asylum backlog is the latest example of pretending he's solved problems rather than actually solving,\" he said.\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidate appearance 2\u001B[0m\n",
      "\u001B[0mRishi Sunak - Mention (1756, 1758) is within bounds (1634, 1816)\n",
      "\u001B[0m\"He's now getting into the position where he's cooking the books - the asylum backlog is the latest example of pretending \u001B[94mhe\u001B[0m's solved problems rather than actually solving,\" he said.\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidate appearance 3\u001B[0m\n",
      "\u001B[0mRishi Sunak - Mention (1818, 1820) is within bounds (1817, 1862)\n",
      "\u001B[0m\"\u001B[94mHe\u001B[0m's not going to convince anyone with this.\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidate appearance 1\u001B[0m\n",
      "\u001B[0mRishi Sunak - Mention (1879, 1881) is within bounds (1863, 1907)\n",
      "\u001B[0mEverybody knows \u001B[94mhe\u001B[0m's been an abject failure.\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidate appearance 1\u001B[0m\n",
      "\u001B[0mRishi Sunak - Mention (1916, 1924) is within bounds (1908, 1973)\n",
      "\u001B[0mBut not \u001B[94mjust him\u001B[0m - the last 14 years have been an abject failure.\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidate appearance 1\u001B[0m\n",
      "\u001B[0mRishi Sunak - Mention (1992, 1994) is within bounds (1974, 2097)\n",
      "\u001B[0mI know the reason \u001B[94mhe\u001B[0m is reluctant to call an election and that's because he hasn't got a record to put before the country.\"\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidate appearance 1\u001B[0m\n",
      "\u001B[0mRishi Sunak - Mention (2047, 2049) is within bounds (1974, 2097)\n",
      "\u001B[0mI know the reason he is reluctant to call an election and that's because \u001B[94mhe\u001B[0m hasn't got a record to put before the country.\"\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidate appearance 2\u001B[0m\n",
      "\u001B[0mRishi Sunak - Mention (2181, 2189) is within bounds (2098, 2257)\n",
      "\u001B[0mHe rubbished Tory Rwanda deportation plans and said Labour would ditch legislation \u001B[94mMr Sunak\u001B[0m is trying to force through Parliament to revive the failing scheme.\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidate appearance 1\u001B[0m\n",
      "\u001B[0mRishi Sunak - Mention (2649, 2667) is within bounds (2569, 2711)\n",
      "\u001B[0m\"Many of them are ex-Tories so I think that that's really about the weakness of \u001B[94mthe Prime Minister\u001B[0m more than about the Labour Party,\" he said.\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidate appearance 1\u001B[0m\n",
      "Bounds sentiment time: 21.722966194152832 seconds\n",
      "Entity Name:  Keir Starmer\n",
      "Sentiment Scores: [0.5014316439628601, 0.0, 0.0]\n",
      "Text: Keir Starmer tonight told Rishi Sunak to “bring it on” and warned voters can't afford to wait any longer for a general election.\n",
      "Bounds Keys: (0, 128)\n",
      "\n",
      "Entity Name:  Keir Starmer\n",
      "Sentiment Scores: [0.7552022635936737, 0.0, 0.0]\n",
      "Text: In his first interview of the election year, the Labour leader said voters can’t afford to wait for another year of Tory misery as mortgages rise and bills pile up.\n",
      "Bounds Keys: (129, 293)\n",
      "\n",
      "Entity Name:  Keir Starmer\n",
      "Sentiment Scores: [0.944856584072113, 0.0, 0.0]\n",
      "Text: But Mr Starmer said the cowardly Prime Minister is running scared from calling an election as “everyone knows he’s been an abject failure”.\n",
      "Bounds Keys: (294, 433)\n",
      "\n",
      "Entity Name:  Keir Starmer\n",
      "Sentiment Scores: [0.0, 0.7329699397087097, 0.0]\n",
      "Text: He said he was ready to take on Mr Sunak and rubbished reports that he wanted to duck scrutiny.\n",
      "Bounds Keys: (434, 529)\n",
      "\n",
      "Entity Name:  Keir Starmer\n",
      "Sentiment Scores: [0.9385808110237122, 0.0, 0.0]\n",
      "Text: “Bring it on I say.\n",
      "Bounds Keys: (530, 549)\n",
      "\n",
      "Entity Name:  Keir Starmer\n",
      "Sentiment Scores: [0.8948442339897156, 0.0, 0.0]\n",
      "Text: Bring on the campaign,” he told the Mirror from Labour’s London headquarters.\n",
      "Bounds Keys: (550, 627)\n",
      "\n",
      "Entity Name:  Keir Starmer\n",
      "Sentiment Scores: [0.25685301423072815, 0.27885743975639343, 0.0]\n",
      "Text: Asked if he was ready for a snap election, Mr Starmer said: “We’re ready, we’ve been ready for a long time.\n",
      "Bounds Keys: (1160, 1267)\n",
      "\n",
      "Entity Name:  Keir Starmer\n",
      "Sentiment Scores: [0.0, 0.24776586890220642, 0.2223050743341446]\n",
      "Text: And that’s because people can’t afford to wait any longer for this Government to finally put itself before the electorate.”\n",
      "In a direct message to Mirror readers, he said: \"Hope is on the way and the power of change is in your hands.”\n",
      "He said Mr Sunak was afraid to call an election as \"it's obvious he hasn't delivered\".\n",
      "Bounds Keys: (1312, 1633)\n",
      "\n",
      "Entity Name:  Keir Starmer\n",
      "Sentiment Scores: [0.0, 0.5442259311676025, 0.0]\n",
      "Text: \"He's now getting into the position where he's cooking the books - the asylum backlog is the latest example of pretending he's solved problems rather than actually solving,\" he said.\n",
      "Bounds Keys: (1634, 1816)\n",
      "\n",
      "Entity Name:  Keir Starmer\n",
      "Sentiment Scores: [0.750167727470398, 0.0, 0.0]\n",
      "Text: I know the reason he is reluctant to call an election and that's because he hasn't got a record to put before the country.\"\n",
      "Bounds Keys: (1974, 2097)\n",
      "\n",
      "Entity Name:  Keir Starmer\n",
      "Sentiment Scores: [0.5865752696990967, 0.0, 0.0]\n",
      "Text: He rubbished Tory Rwanda deportation plans and said Labour would ditch legislation Mr Sunak is trying to force through Parliament to revive the failing scheme.\n",
      "Bounds Keys: (2098, 2257)\n",
      "\n",
      "Entity Name:  Keir Starmer\n",
      "Sentiment Scores: [0.4319673379262288, 0.13055825233459473, 0.0]\n",
      "Text: Asked what he would do if the legislation passed, he said: \"I don't believe in wasting taxpayers money on gimmicks that don't work.\n",
      "Bounds Keys: (2258, 2389)\n",
      "\n",
      "Entity Name:  Keir Starmer\n",
      "Sentiment Scores: [0.5204476118087769, 0.0, 0.0]\n",
      "Text: If you can't see the poll click here\n",
      "Mr Starmer also dismissed threats from Nigel Farage-linked outlet Reform UK, who claim they are pushing for Labour votes.\n",
      "Bounds Keys: (2410, 2568)\n",
      "\n",
      "Entity Name:  Keir Starmer\n",
      "Sentiment Scores: [0.8866260051727295, 0.0, 0.0]\n",
      "Text: \"Many of them are ex-Tories so I think that that's really about the weakness of the Prime Minister more than about the Labour Party,\" he said.\n",
      "Bounds Keys: (2569, 2711)\n",
      "\n",
      "Entity Name:  Keir Starmer\n",
      "Sentiment Scores: [0.0, 0.9111377000808716, 0.0]\n",
      "Text: \"I'm confident in the case that we want to put before the country.\"\n",
      "Bounds Keys: (2712, 2779)\n",
      "\n",
      "Entity Name:  Keir Starmer\n",
      "Sentiment Scores: [0.0, 0.8211084306240082, 0.0]\n",
      "Text: Mr Starmer said he wanted to lower the tax burden for working people but wouldn't be drawn on whether there would be cuts to income tax or national insurance in Labour's manifesto.\n",
      "Bounds Keys: (2894, 3074)\n",
      "\n",
      "Entity Name:  Keir Starmer\n",
      "Sentiment Scores: [0.7302328050136566, 0.0, 0.0]\n",
      "Text: On inheritance tax, he said: \"I think even they [the Tories] know that more tax cuts for those that are better off without any benefit for the vast majority of working people cannot be the way forward for this country.\"\n",
      "Bounds Keys: (3075, 3294)\n",
      "\n",
      "Entity Name:  Keir Starmer\n",
      "Sentiment Scores: [0.9217226505279541, 0.0, 0.0]\n",
      "Text: He said the Tories were trying to set traps for Labour after Chancellor Jeremy Hunt paved the way for a new austerity drive after the next election, with a major squeeze on public spending.\n",
      "Bounds Keys: (3295, 3484)\n",
      "\n",
      "Entity Name:  Keir Starmer\n",
      "Sentiment Scores: [0.9414286017417908, 0.0, 0.0]\n",
      "Text: But he added: \"Labour is not the party of austerity.\n",
      "Bounds Keys: (3485, 3537)\n",
      "\n",
      "Entity Name:  Keir Starmer\n",
      "Sentiment Scores: [0.0, 0.8864459991455078, 0.0]\n",
      "Text: The Labour leader also signalled that he could allow a free vote on assisted dying if he gets the keys to No10.\n",
      "Bounds Keys: (3593, 3704)\n",
      "\n",
      "Entity Name:  Keir Starmer\n",
      "Sentiment Scores: [0.0, 0.5978894233703613, 0.0]\n",
      "Text: Mr Starmer, who dealt with cases of assisted dying when he was Director of Public Prosecutions, said: \"I do think it's time for the law to change.\n",
      "Bounds Keys: (3705, 3851)\n",
      "\n",
      "Entity Name:  Keir Starmer\n",
      "Sentiment Scores: [0.0, 0.6405276656150818, 0.0]\n",
      "Text: \"Any change would have to be carefully considered but I do think there should be a free vote on it and Parliament should properly have its say.\"\n",
      "Bounds Keys: (3852, 3996)\n",
      "\n",
      "Entity Name:  Keir Starmer\n",
      "Sentiment Scores: [0.8615372180938721, 0.0, 0.0]\n",
      "Text: Asked if he would make time for it, he said a private members bill is \"a route we could go down\".\n",
      "Bounds Keys: (3997, 4094)\n",
      "\n",
      "Entity Name:  Keir Starmer\n",
      "Sentiment Scores: [0.26908254623413086, 0.3481914798418681, 0.0]\n",
      "Text: Amid a growing row over whether the ex-Post Office chief Paula Vennells should give up her CBE, Mr Starmer said: \"I think it's a matter for her and I can see why people are calling for it.\"\n",
      "Bounds Keys: (4095, 4284)\n",
      "\n",
      "Entity Name:  Keir Starmer\n",
      "Sentiment Scores: [0.0, 0.7586805522441864, 0.0]\n",
      "Text: But he said his priority would be getting compensation for postmasters who were wrongly jailed due to the \"shocking\" Horizon IT scandal and told the Government to \"get on with it\".\n",
      "Bounds Keys: (4285, 4465)\n",
      "\n",
      "Entity Name:  Keir Starmer\n",
      "Sentiment Scores: [0.0, 0.9327613115310669, 0.0]\n",
      "Text: In a major speech in the South West on Thursday, Mr Starmer will appeal to those who lent their vote to the Tories in 2019 and urge them to trust Labour again.\n",
      "Bounds Keys: (4466, 4625)\n",
      "\n",
      "Entity Name:  Keir Starmer\n",
      "Sentiment Scores: [0.0, 0.8430836200714111, 0.0]\n",
      "Text: He will vow to clean up politics and crack down on cronyism, with plans for tougher sentences for people who defraud the taxpayer.\n",
      "Bounds Keys: (4626, 4756)\n",
      "\n",
      "Entity Name:  Keir Starmer\n",
      "Sentiment Scores: [0.5832965970039368, 0.0, 0.0]\n",
      "Text: The Labour leader will warn: \"Nobody will be above the law in a Britain I lead.\"\n",
      "Bounds Keys: (4757, 4837)\n",
      "\n",
      "Entity Name:  Keir Starmer\n",
      "Sentiment Scores: [0.0, 0.5739161968231201, 0.0]\n",
      "Text: He will also hit out at top Tories like Boris Johnson and Lord Cameron, saying politics is not a “hobby” for people who “enjoy the feeling of power”.\n",
      "Bounds Keys: (4838, 4987)\n",
      "\n",
      "Entity Name:  Keir Starmer\n",
      "Sentiment Scores: [0.0, 0.2279917299747467, 0.21064232289791107]\n",
      "Text: And he'll take a swipe at his predecessor Jeremy Corbyn, vowing his premiership won't be \"vanity dressed up as virtue\".\n",
      "Bounds Keys: (4988, 5107)\n",
      "\n",
      "Source Article ID: None\n",
      "Entity Database ID: 1\n",
      "DEMO DB ENTITY IGNORE - this is: Keir Starmer\n",
      "Number Bound: 30\n",
      "Linear Percent 0: 54.3\n",
      "Linear Percent 1: 43.7\n",
      "Linear Percent 2: 2.0\n",
      "Exp Percent 0: 58.7\n",
      "Exp Percent 1: 41.2\n",
      "Exp Percent 2: 0.1\n",
      "Entity Name:  Rishi Sunak\n",
      "Sentiment Scores: [0.5703337788581848, 0.0, 0.0]\n",
      "Text: Keir Starmer tonight told Rishi Sunak to “bring it on” and warned voters can't afford to wait any longer for a general election.\n",
      "Bounds Keys: (0, 128)\n",
      "\n",
      "Entity Name:  Rishi Sunak\n",
      "Sentiment Scores: [0.0, 0.0, 0.9929583668708801]\n",
      "Text: But Mr Starmer said the cowardly Prime Minister is running scared from calling an election as “everyone knows he’s been an abject failure”.\n",
      "Bounds Keys: (294, 433)\n",
      "\n",
      "Entity Name:  Rishi Sunak\n",
      "Sentiment Scores: [0.0, 0.35228365659713745, 0.2648952305316925]\n",
      "Text: He said he was ready to take on Mr Sunak and rubbished reports that he wanted to duck scrutiny.\n",
      "Bounds Keys: (434, 529)\n",
      "\n",
      "Entity Name:  Rishi Sunak\n",
      "Sentiment Scores: [0.0, 0.0, 0.5586777478456497]\n",
      "Text: And that’s because people can’t afford to wait any longer for this Government to finally put itself before the electorate.”\n",
      "In a direct message to Mirror readers, he said: \"Hope is on the way and the power of change is in your hands.”\n",
      "He said Mr Sunak was afraid to call an election as \"it's obvious he hasn't delivered\".\n",
      "Bounds Keys: (1312, 1633)\n",
      "\n",
      "Entity Name:  Rishi Sunak\n",
      "Sentiment Scores: [0.0, 0.5675618847211202, 0.0]\n",
      "Text: \"He's now getting into the position where he's cooking the books - the asylum backlog is the latest example of pretending he's solved problems rather than actually solving,\" he said.\n",
      "Bounds Keys: (1634, 1816)\n",
      "\n",
      "Entity Name:  Rishi Sunak\n",
      "Sentiment Scores: [0.0, 0.0, 0.9761675596237183]\n",
      "Text: I know the reason he is reluctant to call an election and that's because he hasn't got a record to put before the country.\"\n",
      "Bounds Keys: (1974, 2097)\n",
      "\n",
      "Entity Name:  Rishi Sunak\n",
      "Sentiment Scores: [0.0, 0.0, 0.799013078212738]\n",
      "Text: He rubbished Tory Rwanda deportation plans and said Labour would ditch legislation Mr Sunak is trying to force through Parliament to revive the failing scheme.\n",
      "Bounds Keys: (2098, 2257)\n",
      "\n",
      "Entity Name:  Rishi Sunak\n",
      "Sentiment Scores: [0.0, 0.0, 0.9611873626708984]\n",
      "Text: \"Many of them are ex-Tories so I think that that's really about the weakness of the Prime Minister more than about the Labour Party,\" he said.\n",
      "Bounds Keys: (2569, 2711)\n",
      "\n",
      "Entity Name:  Rishi Sunak\n",
      "Sentiment Scores: [0.0, 0.0, 0.8705173134803772]\n",
      "Text: \"He's not going to convince anyone with this.\n",
      "Bounds Keys: (1817, 1862)\n",
      "\n",
      "Entity Name:  Rishi Sunak\n",
      "Sentiment Scores: [0.0, 0.0, 0.9812803864479065]\n",
      "Text: Everybody knows he's been an abject failure.\n",
      "Bounds Keys: (1863, 1907)\n",
      "\n",
      "Entity Name:  Rishi Sunak\n",
      "Sentiment Scores: [0.0, 0.0, 0.9633317589759827]\n",
      "Text: But not just him - the last 14 years have been an abject failure.\n",
      "Bounds Keys: (1908, 1973)\n",
      "\n",
      "Source Article ID: None\n",
      "Entity Database ID: 1\n",
      "DEMO DB ENTITY IGNORE - this is: Rishi Sunak\n",
      "Number Bound: 11\n",
      "Linear Percent 0: 6.4\n",
      "Linear Percent 1: 10.4\n",
      "Linear Percent 2: 83.2\n",
      "Exp Percent 0: 2.9\n",
      "Exp Percent 1: 3.5\n",
      "Exp Percent 2: 93.6\n"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-25T01:18:57.726790Z",
     "start_time": "2024-02-25T01:18:30.886906Z"
    }
   },
   "id": "521b389897e8ab28"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "command = Command()\n",
    "\n",
    "command.go()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-25T01:11:44.606217Z",
     "start_time": "2024-02-25T01:11:44.601099Z"
    }
   },
   "id": "200689ac1915eee9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
