{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from decimal import Decimal, ROUND_HALF_UP\n",
    "from constants import EXPONENTIAL_K_VALUE\n",
    "from intervaltree import Interval, IntervalTree\n",
    "from NewsSentiment import TargetSentimentClassifier\n",
    "\n",
    "\n",
    "def scaling(avg_array_list, k=3, linear=False):\n",
    "    neutral_points = positive_points = negative_points = 0\n",
    "\n",
    "    for avg_array in avg_array_list:\n",
    "        for i, avg_value in enumerate(avg_array):\n",
    "\n",
    "            if linear:\n",
    "                points = avg_value * 100\n",
    "            else:\n",
    "                points = (avg_value ** k) * 100\n",
    "\n",
    "            if i == 0:\n",
    "                neutral_points += points\n",
    "            elif i == 1:\n",
    "                positive_points += points\n",
    "            elif i == 2:\n",
    "                negative_points += points\n",
    "    return [neutral_points, positive_points, negative_points]\n",
    "\n",
    "\n",
    "def average_array(probabilities):\n",
    "    num_probabilities = len(probabilities)\n",
    "    neutral_total = positive_total = negative_total = 0\n",
    "\n",
    "    for prob_data in probabilities:\n",
    "        if not prob_data:\n",
    "            continue\n",
    "        class_prob = prob_data['class_prob']\n",
    "        class_label = prob_data['class_label']\n",
    "\n",
    "        if class_label == 'neutral':\n",
    "            neutral_total += class_prob\n",
    "        elif class_label == 'positive':\n",
    "            positive_total += class_prob\n",
    "        elif class_label == 'negative':\n",
    "            negative_total += class_prob\n",
    "\n",
    "    neutral_avg = neutral_total / num_probabilities if num_probabilities > 0 else 0\n",
    "    positive_avg = positive_total / num_probabilities if num_probabilities > 0 else 0\n",
    "    negative_avg = negative_total / num_probabilities if num_probabilities > 0 else 0\n",
    "\n",
    "    return [neutral_avg, positive_avg, negative_avg]\n",
    "\n",
    "\n",
    "def round_array_to_1dp(arr):\n",
    "    decimal_array = [Decimal(str(x)) for x in arr]\n",
    "    rounded_array = [x.quantize(Decimal('0.0'), rounding=ROUND_HALF_UP) for x in decimal_array]\n",
    "    rounded_sum = sum(rounded_array)\n",
    "    adjustment = Decimal('100') - rounded_sum\n",
    "    rounded_array[-1] += adjustment\n",
    "    rounded_array = [float(x) for x in rounded_array]\n",
    "    return rounded_array\n",
    "\n",
    "\n",
    "def percentage_contribution(elements):\n",
    "    total = sum(elements)\n",
    "    percentage_contributions = [(element / total) * 100 for element in elements]\n",
    "    return round_array_to_1dp(percentage_contributions)\n",
    "\n",
    "\n",
    "class SentimentAnalyser:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.tsc = TargetSentimentClassifier()\n",
    "\n",
    "    def bounds_sentiment(self, mention_start, mention_end, sentence_start, sentence_end,\n",
    "                         article_text, database_id):\n",
    "        try:\n",
    "            left_segment = article_text[sentence_start:mention_start]\n",
    "            mention_segment = article_text[mention_start:mention_end]\n",
    "            right_segment = article_text[mention_end:sentence_end]\n",
    "\n",
    "            # Could add logging here to see the quality of sentence segmentation.\n",
    "            start_time = time.time()\n",
    "            sentiment = self.tsc.infer_from_text(left_segment, mention_segment, right_segment)\n",
    "            elapsed_time = time.time() - start_time\n",
    "\n",
    "            if elapsed_time > 5:\n",
    "                print(f\"News Sentiment Time > 5 seconds so: {elapsed_time} seconds\")\n",
    "            # print(sentiment[0])\n",
    "\n",
    "            return sentiment[0]\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during sentiment analysis: {e}\")\n",
    "            print(f\"LEFT: {left_segment}\")\n",
    "            print(f\"MENTION: {mention_segment}\")\n",
    "            print(f\"RIGHT: {right_segment}\")\n",
    "\n",
    "            print(f\"Creating BoundError:\\n\"\n",
    "                  f\"Article ID: {database_id}\\n\"\n",
    "                  f\"Bound Start: {mention_start}\\n\"\n",
    "                  f\"Bound End: {mention_end}\\n\"\n",
    "                  f\"Left Segment: {left_segment}\\n\"\n",
    "                  f\"Mention Segment: {mention_segment}\\n\"\n",
    "                  f\"Right Segment: {right_segment}\\n\"\n",
    "                  f\"Error Message: Exception during sentiment analysis\")\n",
    "\n",
    "            return None\n",
    "\n",
    "    def process_clustered_entities(self, clustered_entities, sentence_bounds, article_text,\n",
    "                                   database_id,\n",
    "                                   debug):\n",
    "        START_HIGHLIGHT = '\\033[0m'\n",
    "        END_HIGHLIGHT = '\\033[94m'\n",
    "        GREEN = '\\033[92m'\n",
    "        END_COLOR = '\\033[0m'\n",
    "\n",
    "        bounds_tree = IntervalTree(Interval(start, end) for start, end in\n",
    "                                   sentence_bounds)\n",
    "\n",
    "        bounds_sentiment = {}\n",
    "\n",
    "        ''' Some entities at this point may not have been fully consolidated.\n",
    "            Running the model is the most intensive part of this process.\n",
    "            Since non consolidated entities likely have the same coref cluster.\n",
    "            Running model over save cluster more than once is wasteful.'''\n",
    "\n",
    "        cluster_id_mapping = {}  # Map cluster_id to bounds_sentiment\n",
    "\n",
    "        for entity in clustered_entities:\n",
    "            entity_name = entity['Entity Name']\n",
    "            if 'entity_db_id' in entity:\n",
    "                entity_db_id = entity['entity_db_id']\n",
    "            else:\n",
    "                print(\"Process clustered entities would skip...\")\n",
    "                print(entity_name)\n",
    "                continue\n",
    "            cluster_positions = entity['Cluster Info']['Cluster Positions']\n",
    "            cluster_id = entity['Cluster Info']['Cluster ID']\n",
    "\n",
    "            # Check if the cluster_id has been seen before\n",
    "            if cluster_id in cluster_id_mapping:\n",
    "                # print('Using cached bounds sentiment')\n",
    "                # If so, use the cached bounds_sentiment\n",
    "                for entry in cluster_id_mapping[cluster_id]:\n",
    "                    bounds_key = entry['bounds_key']\n",
    "\n",
    "                    if entity_name not in bounds_sentiment[bounds_key]:\n",
    "                        bounds_sentiment[bounds_key][entity_name] = {}\n",
    "\n",
    "                    if entity_db_id not in bounds_sentiment[bounds_key][entity_name]:\n",
    "                        bounds_sentiment[bounds_key][entity_name][entity_db_id] = []\n",
    "\n",
    "                    bounds_sentiment[bounds_key][entity_name][entity_db_id].append(entry['result'])\n",
    "\n",
    "            else:\n",
    "                cluster_id_mapping[cluster_id] = []\n",
    "\n",
    "                for mention_start, mention_end in cluster_positions:\n",
    "                    overlap = bounds_tree.overlap(mention_start, mention_end)\n",
    "                    if overlap:\n",
    "                        for interval in overlap:\n",
    "                            sentence_start, sentence_end = interval.begin, interval.end\n",
    "                            bounds_key = (sentence_start, sentence_end)\n",
    "\n",
    "                            if bounds_key not in bounds_sentiment:\n",
    "                                bounds_sentiment[bounds_key] = {}\n",
    "\n",
    "                            if entity_name not in bounds_sentiment[bounds_key]:\n",
    "                                bounds_sentiment[bounds_key][entity_name] = {}\n",
    "\n",
    "                            if entity_db_id not in bounds_sentiment[bounds_key][entity_name]:\n",
    "                                bounds_sentiment[bounds_key][entity_name][entity_db_id] = []\n",
    "\n",
    "                            highlighted_text = (\n",
    "                                    START_HIGHLIGHT +\n",
    "                                    article_text[sentence_start:mention_start] + END_HIGHLIGHT +\n",
    "                                    article_text[mention_start:mention_end] + START_HIGHLIGHT +\n",
    "                                    article_text[mention_end:sentence_end] + END_HIGHLIGHT)\n",
    "\n",
    "                            result = self.bounds_sentiment(mention_start, mention_end,\n",
    "                                                           sentence_start, sentence_end,\n",
    "                                                           article_text, database_id)\n",
    "\n",
    "                            bounds_sentiment[bounds_key][entity_name][entity_db_id].append(\n",
    "                                result)\n",
    "\n",
    "                            cluster_id_mapping[cluster_id].append({\n",
    "                                'bounds_key': bounds_key,\n",
    "                                'result': result\n",
    "                            })\n",
    "\n",
    "                            if debug:\n",
    "                                print(\n",
    "                                    START_HIGHLIGHT + f\"{entity_name} - Mention ({mention_start}, {mention_end}) is within bounds ({sentence_start}, {sentence_end})\")\n",
    "                                print(highlighted_text)\n",
    "                                print(\n",
    "                                    GREEN + f\"NewsSentiment Candidateappearance{len(bounds_sentiment[bounds_key][entity_name][entity_db_id])}\" + END_COLOR)\n",
    "\n",
    "        return bounds_sentiment\n",
    "\n",
    "    @staticmethod\n",
    "    def average_sentiment_results(source_article_id, bounds_sentiment, article_text):\n",
    "        if bounds_sentiment is None:\n",
    "            print(\"Error: bounds_sentiment is None\")\n",
    "            return\n",
    "        entity_averages = {}\n",
    "        for bounds_key, entity_results in bounds_sentiment.items():\n",
    "            for entity_name, entity_db_ids in entity_results.items():\n",
    "                # print(\"Entity DB IDs: \")\n",
    "                # print(entity_db_ids)\n",
    "                for entity_db_id, results in entity_db_ids.items():\n",
    "\n",
    "                    if not results:  # Empty results for an entity? Skip...\n",
    "                        continue\n",
    "                    # print(results)\n",
    "                    avg = average_array(results)\n",
    "\n",
    "                    # Store entity - bound mention - bound text - average result in database\n",
    "\n",
    "                    if entity_name not in entity_averages:\n",
    "                        entity_averages[entity_name] = {\n",
    "                            \"entity_db_ids\": [entity_db_id],\n",
    "                            \"bounds_keys\": [bounds_key],\n",
    "                            \"sentiment_scores\": [avg],\n",
    "                            \"text\": [article_text[bounds_key[0]:bounds_key[1]]],\n",
    "                        }\n",
    "                    else:\n",
    "                        entity_averages[entity_name][\"entity_db_ids\"].append(entity_db_id)\n",
    "                        entity_averages[entity_name][\"bounds_keys\"].append(bounds_key)\n",
    "                        entity_averages[entity_name][\"sentiment_scores\"].append(avg)\n",
    "                        entity_averages[entity_name][\"text\"].append(\n",
    "                            article_text[bounds_key[0]:bounds_key[1]])\n",
    "\n",
    "        # print('Sentiment Scores Format: [Neutral, Positive, Negative]')\n",
    "        for entity_name, averages in entity_averages.items():\n",
    "            entity_db_id = averages['entity_db_ids'][0]\n",
    "            # print(f\"Averages for {entity_name} (Entity DB ID: {entity_db_id}):\")\n",
    "            sentiment_scores = averages['sentiment_scores']\n",
    "            text = averages['text']\n",
    "            bounds_keys = averages['bounds_keys']\n",
    "\n",
    "            for i, scores in enumerate(sentiment_scores):\n",
    "                # print(\"Sentiment Scores:\", scores)\n",
    "                # print(\"Text:\", text[i])\n",
    "                # print(\"Bounds Keys:\", bounds_keys[i])\n",
    "                print()\n",
    "\n",
    "                # DatabaseUtils.insert_bound_mention_data(entity_name, source_article_id,\n",
    "                #                                         entity_db_id,\n",
    "                #                                         scores, text[i],\n",
    "                #                                         bounds_keys[i])\n",
    "                \n",
    "                print(f\"Inserting bound mention data: \\n\"\n",
    "                      f\"Entity Name: {entity_name}\\n\"\n",
    "                      f\"Source Article ID: {source_article_id}\\n\"\n",
    "                      f\"Entity Database ID: {entity_db_id}\\n\"\n",
    "                      f\"Scores: {scores}\\n\"\n",
    "                      f\"Text Snippet: {text[i]}\\n\"\n",
    "                      f\"Bounds Keys: {bounds_keys[i]}\")\n",
    "\n",
    "            num_bound = len(averages['sentiment_scores'])\n",
    "            scaled_classification = scaling(averages['sentiment_scores'],\n",
    "                                            k=EXPONENTIAL_K_VALUE)\n",
    "\n",
    "            # Can't scale an array of [0, 0, 0] -> Divide by zero error.\n",
    "            if sum(scaled_classification) == 0:\n",
    "                # print(scaled_classification)\n",
    "                continue\n",
    "\n",
    "            exp_percent = percentage_contribution(scaled_classification)\n",
    "\n",
    "            linear_scaled_classification = scaling(averages['sentiment_scores'],\n",
    "                                                   linear=True)\n",
    "            linear_percent = percentage_contribution(\n",
    "                linear_scaled_classification)\n",
    "            \n",
    "              # DatabaseUtils.insert_overall_sentiment(source_article_id, entity_db_id, num_bound,\n",
    "              #                              linear_percent[0],\n",
    "              #                              linear_percent[1],\n",
    "              #                              linear_percent[2],\n",
    "              #                              exp_percent[0], exp_percent[1], exp_percent[2])\n",
    "\n",
    "            print(\"Inserting overall sentiment data:\")\n",
    "            print(f\"Source Article ID: {source_article_id}\")\n",
    "            print(f\"Entity Name: {entity_name}\")\n",
    "            print(f\"Number of Bounds: {num_bound}\")\n",
    "            print(f\"Linear Percent - Neutral: {linear_percent[0]}\")\n",
    "            print(f\"Linear Percent - Positive: {linear_percent[1]}\")\n",
    "            print(f\"Linear Percent - Negative: {linear_percent[2]}\")\n",
    "            print(f\"Exponential Percent - Neutral: {exp_percent[0]}\")\n",
    "            print(f\"Exponential Percent - Positive: {exp_percent[1]}\")\n",
    "            print(f\"Exponential Percent - Negative: {exp_percent[2]}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T13:25:07.684384Z",
     "start_time": "2024-04-05T13:25:00.445207Z"
    }
   },
   "id": "7d13e6016e10f60c"
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "import nltk\n",
    "import ppdeep\n",
    "from lexicalrichness import LexicalRichness\n",
    "\n",
    "\n",
    "def calculate_statistics(text_body):\n",
    "    lex = LexicalRichness(text_body)\n",
    "    common_stop_words = [\"the\", \"and\", \"is\", \"of\", \"in\", \"it\", \"that\", \"to\", \"with\"]\n",
    "    tokens = nltk.word_tokenize(text_body)\n",
    "    stop_word_counts = {word: tokens.count(word) for word in common_stop_words}\n",
    "\n",
    "    try:\n",
    "        vocd_int = lex.vocd()\n",
    "    except ValueError:\n",
    "        vocd_int = None\n",
    "\n",
    "    linguistic_stats = {\n",
    "        \"fuzzy_hash\": ppdeep.hash(text_body),\n",
    "        \"word_count\": len(tokens),\n",
    "        \"terms_count\": lex.terms,\n",
    "        \"vocd\": vocd_int,\n",
    "        \"yulek\": lex.yulek,\n",
    "        \"simpsond\": lex.simpsond,\n",
    "        \"the_count\": stop_word_counts[\"the\"],\n",
    "        \"and_count\": stop_word_counts[\"and\"],\n",
    "        \"is_count\": stop_word_counts[\"is\"],\n",
    "        \"of_count\": stop_word_counts[\"of\"],\n",
    "        \"in_count\": stop_word_counts[\"in\"],\n",
    "        \"it_count\": stop_word_counts[\"it\"],\n",
    "        \"that_count\": stop_word_counts[\"that\"],\n",
    "        \"to_count\": stop_word_counts[\"to\"],\n",
    "        \"with_count\": stop_word_counts[\"with\"],\n",
    "    }\n",
    "\n",
    "    return linguistic_stats\n",
    "\n",
    "\n",
    "\n",
    "class ArticleUpdate:\n",
    "    def __init__(self, text_body, article_model):\n",
    "        try:\n",
    "            nltk.data.find('tokenizers/punkt')\n",
    "        except LookupError:\n",
    "            nltk.download('punkt')\n",
    "\n",
    "        self.text_body = text_body  # Added by trafilatura\n",
    "        self.linguistic_stats = None\n",
    "        self.article_model = article_model\n",
    "\n",
    "    def get_statistics(self):\n",
    "        self.linguistic_stats = calculate_statistics(self.text_body)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T18:26:37.524203Z",
     "start_time": "2024-03-02T18:26:37.395953Z"
    }
   },
   "id": "133bfd2155d0083d"
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "import urllib.robotparser\n",
    "from collections import defaultdict\n",
    "from functools import reduce\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from textblob import TextBlob\n",
    "from constants import (ENTITY_THRESHOLD_PERCENT,\n",
    "                        MENTION_REQ_PER,\n",
    "                        MERGE_REMOVAL_INDICATOR,\n",
    "                        COMBINED_REMOVAL_INDICATOR,\n",
    "                        COMBINED_CLUSTER_ID_SEPARATOR,\n",
    "                        SIMILAR_SEARCH_DAYS,\n",
    "                        PREVIEW_IMG_TIMEOUT)\n",
    "\n",
    "\n",
    "def can_fetch_url(url_to_check):\n",
    "    \"\"\"Determine if the URL can be fetched by all crawlers - adding politeness / adherence to robot\n",
    "    policy.\"\"\"\n",
    "    parsed_url = urlparse(url_to_check)\n",
    "    base_url = parsed_url.scheme + \"://\" + parsed_url.netloc\n",
    "    rules = urllib.robotparser.RobotFileParser()\n",
    "    rules.set_url(base_url + \"/robots.txt\")\n",
    "    rules.read()\n",
    "    return rules.can_fetch(\"*\", url_to_check)\n",
    "\n",
    "\n",
    "def get_preview_image_url(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=PREVIEW_IMG_TIMEOUT)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            og_image = soup.find('meta', property='og:image')\n",
    "            if og_image:\n",
    "                return og_image['content']\n",
    "\n",
    "            # Twitter Card image tag\n",
    "            twitter_image = soup.find(name='twitter:image')\n",
    "            if twitter_image:\n",
    "                return twitter_image['content']\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching preview image URL: {e}\")\n",
    "\n",
    "\n",
    "def merge_positions(entities, word):\n",
    "    \"\"\"Merge all instances of the same entity into a single entry with multiple\n",
    "    positions. Where same means a lowercase word match. Place them into an\n",
    "    'entities' dictionary.\"\"\"\n",
    "    entity_key = (word.text + word.label_).lower()\n",
    "    if entity_key in entities:\n",
    "        entities[entity_key][1].append([word.start_char, word.end_char])\n",
    "    else:\n",
    "        entities[entity_key] = [word.text, [[word.start_char, word\n",
    "        .end_char]], word.label_]\n",
    "    return entities\n",
    "\n",
    "\n",
    "def cleanse_cluster_text(cluster_text):\n",
    "    return [word.strip() for word in cluster_text if word.lower().strip() not\n",
    "            in undesired_words]\n",
    "\n",
    "undesired_words = [\"i\", \"he\", \"his\", \"she\", \"they\", \"it\", \"this\", \"that\",\n",
    "                   \"these\", \"those\", \"the\", \"a\", \"an\", \"of\"]\n",
    "\n",
    "\n",
    "def remove_titles(text):\n",
    "    title_pattern = r\"^(Mr|Mrs|Ms|Miss|Dr|Prof|Rev|Capt|Sir|Madam|Mx|Esq|Hon|Gen|Col|Sgt|Fr|Sr|Jr|Lord|Lady)\\s\"\n",
    "    text = re.sub(title_pattern, \"\", text)\n",
    "\n",
    "    '''10th Nov adding as:\n",
    "        {'Entity Name': 'Keith', 'Positions': [[11664, 11669], [12301, 12306], [14453, 14458], \n",
    "        [15005, 15010], [15286, 15291]], 'Label': 'PERSON', 'Num Positions': 5, 'Cluster Info': \n",
    "        {'Cluster ID': 12, 'Cluster Text': ['Keith', 'Keith', 'Keith', 'Keith', 'Hugo Keith KC',\n",
    "        'Keith', 'Keith', 'Keith'], 'Cluster Positions': [(11664, 11669), (12301, 12306), \n",
    "        (12312, 12314), (13026, 13031), (13464, 13469), (14374, 14387), (14453, 14458),\n",
    "        (15005, 15010), (15286, 15291)]}}\n",
    "        would have been updated to Hugo Keith had it not been for KC which made it 3 words'''\n",
    "\n",
    "    # Pattern for titles at the end\n",
    "    title_pattern_end = r\"\\s*(KC|QC)\\s*$\"\n",
    "    text = re.sub(title_pattern_end, \"\", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def insert_intervals(initial_list, new_values):\n",
    "    \"\"\"The insert_intervals function enables the segmentation of sentences provided by\n",
    "    TextBlob (as shown in the cell below) to be further divided into smaller boundaries. This\n",
    "     division is based on the identification of specific points where it is deemed necessary\n",
    "     to split sentences, particularly within the context of news articles.\"\"\"\n",
    "\n",
    "    def insert_recursive(intervals, values):\n",
    "        if not values:\n",
    "            return intervals  # Base case: Return the intervals when there are no more values to insert.\n",
    "\n",
    "        value = values[0]\n",
    "        result = []\n",
    "        for interval in intervals:\n",
    "            if interval[0] <= value <= interval[1]:\n",
    "                # If the value falls within an existing interval, split the interval into two parts.\n",
    "                # The first part goes from the interval's start to the value (inclusive), and\n",
    "                # the second part goes from the value+1 to the interval's end.\n",
    "                if interval[0] < value:\n",
    "                    # To mess around with intervals change value + - offset here.\n",
    "                    result.append((interval[0], value))\n",
    "                if value < interval[1]:\n",
    "                    # To mess around with intervals change value + - offset here.\n",
    "                    result.append((value + 1, interval[1]))\n",
    "            else:\n",
    "                # If the value doesn't fall within the interval, keep the interval as is.\n",
    "                result.append(interval)\n",
    "        # Recursively process other values.\n",
    "        return insert_recursive(result, values[1:])\n",
    "\n",
    "        # Recursive function call\n",
    "\n",
    "    updated_list = insert_recursive(initial_list, new_values)\n",
    "    return updated_list\n",
    "\n",
    "\n",
    "def is_substring(entity1, entity2):\n",
    "    return entity1.lower() in entity2.lower() or entity2.lower() in entity1.lower()\n",
    "\n",
    "\n",
    "def combine_entities(entities, cluster_text):\n",
    "    combined_entity = None\n",
    "\n",
    "    for entity1 in entities:\n",
    "        for entity2 in entities:\n",
    "            if entity1 != entity2:\n",
    "                combined1 = entity1 + ' ' + entity2\n",
    "                combined2 = entity2 + ' ' + entity1\n",
    "\n",
    "                if combined1 in cluster_text or combined2 in cluster_text:\n",
    "                    combined_entity = combined1 if combined1 in cluster_text \\\n",
    "                        else combined2\n",
    "                    break\n",
    "\n",
    "    return combined_entity\n",
    "\n",
    "\n",
    "def update_entity_name(entry):\n",
    "    \"\"\"Calls remove titles and removes possessives before checking if an entity name is a\n",
    "     substring of a 2 word entry in the coref cluster e.g. Johnson should become Boris Johnson\"\"\"\n",
    "\n",
    "    entity_name = entry['Entity Name']\n",
    "    cluster_text = entry['Cluster Info']['Cluster Text']\n",
    "\n",
    "    for text in cluster_text:\n",
    "        # Remove titles as not relevant\n",
    "        text = remove_titles(text)\n",
    "        # Replaces left / right quotation mark with standard single quotation mark\n",
    "        text = text.replace('’', \"'\").replace('‘', \"'\")\n",
    "        # Remove possessive markers for comparison\n",
    "        text = text.replace(\"’s\", \"\")\n",
    "        # Remove space and quote\n",
    "        text = text.replace(\" '\", \"\")\n",
    "        # Check if the current entity name is a substring of a 2-word cluster text entry\n",
    "        if len(text.split()) == 2 and entity_name in text:\n",
    "            entry['Entity Name'] = text\n",
    "            break\n",
    "    return entry\n",
    "\n",
    "\n",
    "def clean_up_substrings(clustered_entities):\n",
    "    longest_names = {}\n",
    "    entities_to_keep = []\n",
    "\n",
    "    # Identify longest names in cluster ID and remove shorter ones\n",
    "    for entity in clustered_entities:\n",
    "        cluster_id = entity['Cluster Info']['Cluster ID']\n",
    "        entity_name = entity['Entity Name']\n",
    "        current_longest = longest_names.get(cluster_id, \"\")\n",
    "\n",
    "        if len(entity_name) > len(current_longest):\n",
    "            # Remove the shorter entity without adding it to the list of entities to keep\n",
    "            if current_longest:\n",
    "                entities_to_keep = [e for e in clustered_entities if\n",
    "                                    e['Cluster Info']['Cluster ID'] != cluster_id]\n",
    "            longest_names[cluster_id] = entity_name\n",
    "\n",
    "            # Add the entity to the list of entities to keep\n",
    "            entities_to_keep.append(entity)\n",
    "\n",
    "    # Set merge indicator for entities with more than one associated name in the original list\n",
    "    for entity in clustered_entities:\n",
    "        cluster_id = entity['Cluster Info']['Cluster ID']\n",
    "        if len([e for e in entities_to_keep if\n",
    "                e['Cluster Info']['Cluster ID'] == cluster_id]) > 1:\n",
    "            entity['Num Positions'] = int(MERGE_REMOVAL_INDICATOR)\n",
    "            entity['Positions'] = int(MERGE_REMOVAL_INDICATOR)\n",
    "    return entities_to_keep\n",
    "\n",
    "\n",
    "def create_entity_entry(entity_name, positions, label, num_positions):\n",
    "    return {\n",
    "        'Entity Name': entity_name,\n",
    "        'Positions': positions,\n",
    "        'Label': label,\n",
    "        'Num Positions': num_positions,\n",
    "        'Cluster Info': []\n",
    "    }\n",
    "\n",
    "\n",
    "class Article:\n",
    "\n",
    "    def __init__(self, url, headline, text_body, NER, date, author, site_name):\n",
    "        self.url = url\n",
    "        self.NER = NER\n",
    "        self.headline = headline\n",
    "        self.image_url = None\n",
    "        self.description = None\n",
    "        self.text_body = text_body  # Added by trafilatura\n",
    "        self.coref_clusters = None\n",
    "        self.people_entities = None  # NER results here.\n",
    "        self.sentence_bounds = None\n",
    "        self.num_sentences = None\n",
    "        self.mention_threshold = None\n",
    "        self.entity_to_cluster_mapping = []\n",
    "        self.clustered_entities = None\n",
    "        self.database_candidate = False\n",
    "        self.database_id = None\n",
    "        self.bounds_sentiment = None\n",
    "        self.sentiment_analyser = None\n",
    "        self.publication_date = date\n",
    "        self.author = author\n",
    "        self.site_name = site_name\n",
    "        self.linguistic_stats = None\n",
    "\n",
    "    def set_sentiment_analyser(self, sa):\n",
    "\n",
    "        if sa is None:\n",
    "            self.sentiment_analyser = SentimentAnalyser()\n",
    "        else:\n",
    "            self.sentiment_analyser = sa\n",
    "\n",
    "    def get_bounds_sentiment(self):\n",
    "            self.bounds_sentiment = self.sentiment_analyser.process_clustered_entities(\n",
    "                clustered_entities=self.clustered_entities, sentence_bounds=self.sentence_bounds,\n",
    "                article_text=self.text_body, database_id=self.database_id,\n",
    "                debug=True)\n",
    "\n",
    "    def print_clustered_entities(self):\n",
    "        for entry in self.clustered_entities:\n",
    "            print(entry)\n",
    "            print()\n",
    "\n",
    "    def print_entity_to_cluster_mapping(self):\n",
    "        for entry in self.entity_to_cluster_mapping:\n",
    "            print(entry)\n",
    "            print()\n",
    "\n",
    "    def determine_entity_to_cluster_mapping(self):\n",
    "\n",
    "        \"\"\"\n",
    "        Removing pronouns & other 'useless words' (his, her, he, I etc) then doing a % match rate on\n",
    "        entity text across cluster entries for each entity. If match rate % exceeds threshold\n",
    "        pair them.\n",
    "\n",
    "        Improvement: Spliting the entity names into part words e.g. Sadiq Khan will be split into\n",
    "        Sadiq and Khan for evaluation purposes. This way if they are mostly mentioned by first\n",
    "        or second name the match still has an opportunity to take place.\n",
    "        \"\"\"\n",
    "\n",
    "        for entity_type, entities in self.people_entities.items():\n",
    "            for entity in entities:\n",
    "                entity_name, positions, label, num_positions = entity\n",
    "                entity_entry = create_entity_entry(entity_name, positions, label,\n",
    "                                                   num_positions)\n",
    "                self.process_clusters_for_entity(entity_entry, entity_name)\n",
    "\n",
    "    def process_clusters_for_entity(self, entity_entry, entity_name):\n",
    "        cluster_id = 0\n",
    "        print(f\"Processing clusters for entity: {entity_name}\")\n",
    "    \n",
    "        for index, (cluster_text, cluster_positions, _) in self.coref_clusters:\n",
    "            cluster_id += 1\n",
    "            print(f\"\\nCluster ID: {cluster_id}, Original Text: {cluster_text}\")\n",
    "    \n",
    "            cluster_text = cleanse_cluster_text(cluster_text)\n",
    "            cleaned_cluster_text = [remove_titles(text) for text in cluster_text]\n",
    "    \n",
    "            if len(cleaned_cluster_text) < 4:\n",
    "                print(f\"Skipping cluster {cluster_id} as it has length < 4\")\n",
    "                continue\n",
    "    \n",
    "            total_coref_words = \" \".join(cleaned_cluster_text)\n",
    "            entity_parts = entity_name.split()\n",
    "            max_percentage = 0.0\n",
    "            winning_entity_part = None\n",
    "    \n",
    "            for entity_part in entity_parts:\n",
    "                entity_count = total_coref_words.count(entity_part)\n",
    "                percentage = entity_count / len(cleaned_cluster_text) * 100  # Adjusted for readability\n",
    "                \n",
    "                if percentage > max_percentage:\n",
    "                    max_percentage = percentage\n",
    "                    winning_entity_part = entity_part\n",
    "                    \n",
    "                print(f\"Matching '{entity_part}' in cluster {cluster_id}: {percentage:.2f}% match.\")\n",
    "    \n",
    "            if max_percentage >= ENTITY_THRESHOLD_PERCENT:\n",
    "                print(f\"Accepting cluster {cluster_id} for '{entity_name}' with {max_percentage:.2f}% match on part '{winning_entity_part}'.\")\n",
    "                cluster_entry = {\n",
    "                    'Cluster ID': cluster_id,\n",
    "                    'Cluster Text': cluster_text,\n",
    "                    'Cluster Positions': cluster_positions\n",
    "                }\n",
    "                \n",
    "                entity_entry['Cluster Info'] = cluster_entry\n",
    "                self.entity_to_cluster_mapping.append(entity_entry)\n",
    "                break\n",
    "    \n",
    "        if not entity_entry.get('Cluster Info'):\n",
    "            print(f\"No suitable clusters found for entity: {entity_name}\")\n",
    "\n",
    "\n",
    "    def set_coref_clusters(self, sorted_combined_clusters):\n",
    "        # Add an ID to each cluster\n",
    "        self.coref_clusters = list(enumerate(sorted_combined_clusters))\n",
    "\n",
    "    def source_ner_people(self):\n",
    "\n",
    "        \"\"\"SpaCy is a popular NLP library that offers pre-trained models for various languages, and\n",
    "            its NER component is capable of recognising and categorising named entities within text.\n",
    "            It is utilised here to identify PERSON entities\"\"\"\n",
    "\n",
    "        NER = spacy.load(\"en_core_web_sm\")\n",
    "        article_text = self.text_body\n",
    "        article = NER(article_text)\n",
    "\n",
    "        '''# Recommended mention - 'Discard a cluster c in a document d if |Mc| ≤ 0.2|Sd|,  \n",
    "        where |...| is the number of mentions of a cluster (Mc) and sentences in a document (Sd)\n",
    "        (NEWS-MTSC approach)'''\n",
    "\n",
    "        entity_types = [\"CARDINAL\", \"DATE\", \"EVENT\", \"FAC\", \"GPE\", \"LANGUAGE\", \"LAW\",\n",
    "                        \"LOC\", \"MONEY\", \"NORP\", \"ORDINAL\", \"ORG\", \"PERCENT\",\n",
    "                        \"PERSON\", \"PRODUCT\", \"QUANTITY\", \"TIME\", \"WORK_OF_ART\"]\n",
    "\n",
    "        entity_type_to_entities = {\n",
    "            entity_type: [\n",
    "                [\n",
    "                    entity_text,\n",
    "                    positions,\n",
    "                    label,\n",
    "                    len(positions)\n",
    "                ] for entity_text, positions, label in reduce(\n",
    "                    merge_positions,\n",
    "                    filter(lambda word: word.label_ == entity_type, article.ents),\n",
    "                    {}\n",
    "                ).values()\n",
    "            ] for entity_type in entity_types\n",
    "        }\n",
    "\n",
    "        for entity_type, entities in entity_type_to_entities.items():\n",
    "            print(f\"Entity Type: {entity_type}\")\n",
    "            for entity in entities:\n",
    "                entity_text, positions, label, num_positions = entity\n",
    "                if entity_type == 'PERSON':\n",
    "                    print(f\"Entity: {entity_text}\")\n",
    "                    print(f\"Positions: {positions}\")\n",
    "                    print(f\"Label: {label}\")\n",
    "                    print(f\"Number of Positions: {num_positions}\")\n",
    "                    print()\n",
    "\n",
    "        people_entities = {entity_type: entity_info for entity_type, entity_info in\n",
    "                           entity_type_to_entities.items() if entity_type == 'PERSON'}\n",
    "\n",
    "        for entity in entity_type_to_entities[entity_type]:\n",
    "            entity_text, positions, label, num_positions = entity\n",
    "            if entity_type not in people_entities:\n",
    "                people_entities[entity_type] = []\n",
    "            people_entities[entity_type].append({\n",
    "                'Entity': entity_text,\n",
    "                'Positions': positions,\n",
    "                'Label': label,\n",
    "                'Number of Positions': num_positions\n",
    "            })\n",
    "\n",
    "        self.people_entities = people_entities\n",
    "\n",
    "    def determine_sentences(self):\n",
    "\n",
    "        \"\"\"\n",
    "        Process the article text, tokenise by sentence and add custom adjustments to the\n",
    "        tokenization using insert intervals below.\n",
    "        spaCy was not satisfactory for accurately tokenising for sentence start / end\n",
    "        characters. Trying textblob instead. TextBlob is a Python library for processing textual\n",
    "        data that is bulit upon NLTK.\n",
    "\n",
    "        TextBlob can provide me with the start and end of sentences by using the sentences\n",
    "        attribute of a TextBlob object. This attribute returns a list of Sentence objects, each\n",
    "        of which has a start and end property that indicates the index of the first and last\n",
    "        character of the sentence within the original text.\"\"\"\n",
    "\n",
    "        # Process the article text and adjust tokenization\n",
    "        article_text = self.text_body\n",
    "        blob = TextBlob(article_text)\n",
    "        sentences = blob.sentences\n",
    "\n",
    "        # Determine custom tokenization:\n",
    "        hyphen_sentences = re.split(r'\\n-', article_text)\n",
    "        # In testing article a new line and '-' hyphen use typically means consider\n",
    "        # a different sentence.\n",
    "        hyphen_nl_pos = [pos for pos, char in enumerate(article_text) if\n",
    "                         article_text[pos:pos + 2] == '\\n-']\n",
    "        extra_split = hyphen_nl_pos\n",
    "\n",
    "        sentence_bounds = [(int(sentence.start), int(sentence.end)) for sentence\n",
    "                           in sentences]\n",
    "\n",
    "        # Insert custom intervals\n",
    "        updated_list = insert_intervals(sentence_bounds, extra_split)\n",
    "\n",
    "        self.sentence_bounds = updated_list\n",
    "        self.num_sentences = len(list(sentence_bounds))\n",
    "        self.mention_threshold = math.floor(self.num_sentences * MENTION_REQ_PER)\n",
    "        \n",
    "        print(\"Processing article text for custom tokenization...\")\n",
    "\n",
    "        print(f\"Original Article Text: {article_text[:100]}...\")\n",
    "        print(f\"Number of sentences detected by TextBlob: {len(sentences)}\")\n",
    "        print(f\"Hyphen-newline positions (custom splits): {hyphen_nl_pos}\")\n",
    "        print(\"Sentence bounds before custom adjustments:\")\n",
    "        print(sentence_bounds)\n",
    "        \n",
    "        print(\"Updated sentence bounds after inserting custom intervals:\")\n",
    "        print(updated_list)\n",
    "        \n",
    "        print(f\"Total number of sentences after adjustments: {self.num_sentences}\")\n",
    "        print(f\"Mention threshold (floor of sentences * {MENTION_REQ_PER}): {self.mention_threshold}\")\n",
    "\n",
    "    def entity_cluster_map_consolidation(self):\n",
    "        \"\"\"\n",
    "        Purposes of below code:\n",
    "\n",
    "        1. Substring Matching: If one entity is a substring of another entity, they\n",
    "        are considered as candidates for consolidation. For example, if \"Rishi\" is a\n",
    "        substring of \"Rishi Sunak,\" the code merges them into the longer version, \"Rishi Sunak.\"\n",
    "\n",
    "        2. First and Second Name Combination: If there are two entities, one\n",
    "        representing the first name and the other representing the last name, and\n",
    "        they share the same coreference cluster, the code attempts to combine them\n",
    "        into a single entity.\n",
    "\n",
    "        3. 9th November - add while loop to see if continuing consolidation until no more\n",
    "        consolidation takes place results in a better consolidation as theorised.\n",
    "\n",
    "        4. 16th November - resolve instances of 'King\\n11:43' which should be King.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        cluster_dict = defaultdict(list)\n",
    "        for entry in self.entity_to_cluster_mapping:\n",
    "            entity_name = entry['Entity Name']\n",
    "            cluster_id = entry['Cluster Info']['Cluster ID']\n",
    "            cluster_dict[cluster_id].append(entry)\n",
    "\n",
    "        # While loop to continue consolidation until no more consolidation can be done\n",
    "        consolidation_done = True\n",
    "        while consolidation_done:\n",
    "            consolidation_done = False\n",
    "            for cluster_id, entries in cluster_dict.items():\n",
    "                if len(entries) > 1:\n",
    "                    combined_entry = None\n",
    "                    for i, entry1 in enumerate(entries):\n",
    "                        for j, entry2 in enumerate(entries):\n",
    "                            # print(f\"\\nBefore update:\\nEntry 1: {entry1}\\nEntry 2: {entry2}\")\n",
    "                            entry1 = update_entity_name(entry1)\n",
    "                            entry2 = update_entity_name(entry2)\n",
    "                            # print(f\"After update:\\nEntry 1: {entry1}\\nEntry 2: {entry2}\")\n",
    "                            \n",
    "                            if i < j:\n",
    "                                entity_1_name = entry1['Entity Name']\n",
    "                                entity_2_name = entry2['Entity Name']\n",
    "\n",
    "                                cluster_text = entries[0]['Cluster Info']['Cluster Text']\n",
    "                                combined_entity = combine_entities([entity_1_name,\n",
    "                                                                    entity_2_name],\n",
    "                                                                   cluster_text)\n",
    "\n",
    "                                if is_substring(entity_1_name, entity_2_name):\n",
    "                                    if len(entity_1_name) > len(entity_2_name):\n",
    "                                        if entry2 in entries:\n",
    "                                            entries.remove(entry2)\n",
    "                                            \n",
    "                                            print(f\"Removing {entity_2_name} as substring of \"\n",
    "                                                  f\"{entity_1_name}\")\n",
    "                                        # Coreference cluster will be used anyway -200 to\n",
    "                                        # indicate merge via removal.\n",
    "                                        entry1['Num Positions'] = int(MERGE_REMOVAL_INDICATOR)\n",
    "                                        entry1['Positions'] = int(MERGE_REMOVAL_INDICATOR)\n",
    "\n",
    "                                    else:\n",
    "                                        if entry1 in entries:\n",
    "                                            print(f\"Removing {entity_1_name} as substring of \"\n",
    "                                                  f\"{entity_2_name}\")\n",
    "                                            entries.remove(entry1)\n",
    "                                            \n",
    "                                        # Coreference cluster will be used anyway -200 to\n",
    "                                        # indicate merge via removal.\n",
    "                                        entry2['Num Positions'] = int(MERGE_REMOVAL_INDICATOR)\n",
    "                                        entry2['Positions'] = int(MERGE_REMOVAL_INDICATOR)\n",
    "\n",
    "                                    consolidation_done = True\n",
    "                                    # exit inner for loop\n",
    "                                    break\n",
    "                                elif combined_entity in cluster_text:\n",
    "                                    print(f\"{entry1['Entity Name']} and {entry2['Entity Name']} \"\n",
    "                                          \"exist as a \"\n",
    "                                          \"combination in cluster \"\n",
    "                                          \"text making a new cluster by combining positions\")\n",
    "                                    combined_entry = {\n",
    "                                        'Entity Name': combined_entity,\n",
    "                                        # Coreference cluster will be used anyway -100 to\n",
    "                                        # indicate merge via combined entity.\n",
    "                                        'Positions': int(COMBINED_REMOVAL_INDICATOR),\n",
    "                                        'Label': entry1['Label'],\n",
    "                                        # Coreference cluster will be used anyway -100 to\n",
    "                                        # indicate merge via combined entity.\n",
    "                                        'Num Positions': int(COMBINED_REMOVAL_INDICATOR),\n",
    "                                        'Cluster Info': entry1['Cluster Info']\n",
    "                                    }\n",
    "                                    entries.remove(entry1)\n",
    "                                    entries.remove(entry2)\n",
    "                                    consolidation_done = True\n",
    "                                    # exit inner for loop\n",
    "                                    break\n",
    "                        if combined_entry:\n",
    "                            entries.append(combined_entry)\n",
    "                        if consolidation_done:\n",
    "                            # exit outer for loop\n",
    "                            break\n",
    "\n",
    "            '''\n",
    "            Now look across cluster ids (above we stayed within a cluster id) for substrings of\n",
    "            entity names and merge together cluster ids, positions and text.\n",
    "            '''\n",
    "            for cluster_id1, entries1 in cluster_dict.items():\n",
    "                for cluster_id2, entries2 in cluster_dict.items():\n",
    "                    if cluster_id1 != cluster_id2:  # Prevents comparing entries within the same\n",
    "                        # cluster\n",
    "                        for entry1 in entries1:\n",
    "                            for entry2 in entries2:\n",
    "                                # print(f\"\\nBefore update:\\nEntry 1: {entry1}\\nEntry 2: {entry2}\")\n",
    "                                entry1 = update_entity_name(entry1)\n",
    "                                entry2 = update_entity_name(entry2)\n",
    "                                # print(f\"After update:\\nEntry 1: {entry1}\\nEntry 2: {entry2}\")\n",
    "\n",
    "                                entity_1_name = entry1['Entity Name']\n",
    "                                entity_2_name = entry2['Entity Name']\n",
    "                                # Check if an entity name is a substring of another.\n",
    "                                if entity_1_name in entity_2_name or entity_2_name in entity_1_name:\n",
    "                                    print(f'cross cluster merge triggered as {entity_1_name} '\n",
    "                                          'in {entity_2_name} or {entity_2_name} in '\n",
    "                                          '{entity_1_name}')\n",
    "                                    print(entity_1_name)\n",
    "                                    print(entity_2_name)\n",
    "                                    # Combine cluster IDs with '0000' in between as a strong\n",
    "                                    # indicator.\n",
    "                                    combined_cluster_id = f\"{entry1['Cluster Info']['Cluster ID']}{COMBINED_CLUSTER_ID_SEPARATOR}{entry2['Cluster Info']['Cluster ID']}\"\n",
    "\n",
    "                                    # Set the new cluster ID\n",
    "                                    entry1['Cluster Info']['Cluster ID'] = combined_cluster_id\n",
    "                                    entry2['Cluster Info']['Cluster ID'] = combined_cluster_id\n",
    "\n",
    "                                    # Append cluster texts and positions to entry 1.\n",
    "                                    entry1['Cluster Info']['Cluster Text'].extend(\n",
    "                                        entry2['Cluster Info']['Cluster Text'])\n",
    "                                    entry1['Cluster Info']['Cluster Positions'].extend(\n",
    "                                        entry2['Cluster Info']['Cluster Positions'])\n",
    "\n",
    "                                    if len(entity_2_name) > len(entity_1_name):\n",
    "                                        entry1['Entity Name'] = entity_2_name\n",
    "                                    entries2.remove(entry2)\n",
    "                                    consolidation_done = True\n",
    "\n",
    "        clustered_entities = [entry for entries in cluster_dict.values() for entry in entries]\n",
    "\n",
    "        before_len = len(clustered_entities)\n",
    "\n",
    "        cleaned_entities = []\n",
    "\n",
    "        # Resolve \\n in entity name instances and any below mention threshold.\n",
    "        for entity in clustered_entities:\n",
    "            entity_name = entity['Entity Name']\n",
    "            cleaned_name = re.split(r'\\n\\d*:', entity_name)[0].strip()\n",
    "\n",
    "            entity_name = remove_titles(cleaned_name)\n",
    "\n",
    "            '''Found 'Entity Name': 'Reginald D. Hunter’s' - handle removing 's from last word'''\n",
    "\n",
    "            # Replaces left / right quotation mark with standard single quotation mark\n",
    "            entity_name = entity_name.replace('’', \"'\").replace('‘', \"'\")\n",
    "\n",
    "            # Handle the relatively common case of Meghan Markle '  i.e the space then quote mark\n",
    "            entity_name = entity_name.rstrip(\"'\")\n",
    "\n",
    "            # Split the entity name into words\n",
    "            words = entity_name.split()\n",
    "\n",
    "            # Check if the last word ends with 's, and if so, remove it\n",
    "            if words and words[-1].endswith(\"'s\"):\n",
    "                words[-1] = words[-1][:-2]  # Remove 's from the last word\n",
    "\n",
    "            # Join the words back into the entity name\n",
    "            cleaned_name = ' '.join(words)\n",
    "\n",
    "            # Capitalise the first letter of each word and make the rest lowercase\n",
    "            cleaned_name = ' '.join(word.capitalize() for word in words)\n",
    "\n",
    "            # Remove spaces @ start and end string\n",
    "            cleaned_name = cleaned_name.strip()\n",
    "\n",
    "            # Replace the original entity name with the cleaned name\n",
    "            entity['Entity Name'] = cleaned_name\n",
    "\n",
    "            cluster_positions = entity['Cluster Info']['Cluster Positions']\n",
    "            cluster_id = entity['Cluster Info']['Cluster ID']\n",
    "\n",
    "            # Count the number of entries in cluster_positions\n",
    "            num_entries = len(cluster_positions)\n",
    "\n",
    "            # Check if the number of entries is below the threshold\n",
    "            if num_entries < self.mention_threshold:\n",
    "                print(f\"Cluster ID {cluster_id} has {num_entries} entries, which is below the \"\n",
    "                      f\"threshold of {self.mention_threshold}.\")\n",
    "                # print(f\"Removing entity {entity['Entity Name']} with Cluster ID {cluster_id} due \"\n",
    "                #       f\"to low mention count:\")\n",
    "                # print(entity['Cluster Info'])\n",
    "                clustered_entities.remove(entity)\n",
    "                continue\n",
    "\n",
    "            # At most, an entity should have the first, middle and last name.\n",
    "            if len(cleaned_name.split()) > 3:\n",
    "                print(f\"Cluster ID {cluster_id} has {num_entries} entries, which is below the \"\n",
    "                      f\"threshold of {self.mention_threshold}.\")\n",
    "                print(\n",
    "                    f\"Removing entity {entity['Entity Name']} with Cluster ID {cluster_id} due \"\n",
    "                    f\"to low mention count:\")\n",
    "                print(entity['Cluster Info'])\n",
    "                clustered_entities.remove(entity)\n",
    "                continue\n",
    "\n",
    "            # If the entity is not removed, add it to the cleaned list\n",
    "            cleaned_entities.append(entity)\n",
    "\n",
    "        clustered_entities = cleaned_entities\n",
    "        new_length = len(clustered_entities)\n",
    "\n",
    "        print(f\"number of entities before mention threshold: {before_len}\")\n",
    "        print(f\"number of entities after mention threshold: {new_length}\")\n",
    "\n",
    "        clustered_entities = clean_up_substrings(clustered_entities)\n",
    "\n",
    "        self.clustered_entities = clustered_entities\n",
    "\n",
    "        # Is this article going to go on the web app? If clustered_entities > 0 then yes so get\n",
    "        # article parts and insert into database.\n",
    "        if new_length > 0:\n",
    "            self.set_database_candidate_true()\n",
    "\n",
    "    def set_database_candidate_true(self):\n",
    "        self.database_candidate = True\n",
    "\n",
    "    def get_average_sentiment_results(self):\n",
    "        self.sentiment_analyser.average_sentiment_results(self.database_id, self.bounds_sentiment,\n",
    "                                                          self\n",
    "                                                          .text_body)\n",
    "\n",
    "    def save_to_database(self):\n",
    "\n",
    "        self.image_url = get_preview_image_url(self.url)\n",
    "        \n",
    "        print(\"Would normally save article stats to database now\")\n",
    "        \n",
    "        print(f\"\"\"\n",
    "                fuzzy_hash={self.linguistic_stats[\"fuzzy_hash\"]},\n",
    "                word_count={self.linguistic_stats[\"word_count\"]},\n",
    "                terms_count={self.linguistic_stats[\"terms_count\"]},\n",
    "                vocd={self.linguistic_stats[\"vocd\"]},\n",
    "                yulek={self.linguistic_stats[\"yulek\"]},\n",
    "                simpsond={self.linguistic_stats[\"simpsond\"]},\n",
    "                the_count={self.linguistic_stats[\"the_count\"]},\n",
    "                and_count={self.linguistic_stats[\"and_count\"]},\n",
    "                is_count={self.linguistic_stats[\"is_count\"]},\n",
    "                of_count={self.linguistic_stats[\"of_count\"]},\n",
    "                in_count={self.linguistic_stats[\"in_count\"]},\n",
    "                to_count={self.linguistic_stats[\"to_count\"]},\n",
    "                it_count={self.linguistic_stats[\"it_count\"]},\n",
    "                that_count={self.linguistic_stats[\"that_count\"]},\n",
    "                with_count={self.linguistic_stats[\"with_count\"]},\n",
    "                \"\"\")\n",
    "\n",
    "    def get_statistics(self):\n",
    "        self.linguistic_stats = calculate_statistics(self.text_body)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T18:26:37.524355Z",
     "start_time": "2024-03-02T18:26:37.398528Z"
    }
   },
   "id": "76ea9ff938a16e0f"
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import urllib.robotparser\n",
    "import trafilatura\n",
    "import spacy\n",
    "import time\n",
    "import logging\n",
    "import urllib.request\n",
    "import socket\n",
    "\n",
    "\n",
    "from fastcoref import FCoref\n",
    "from urllib.parse import urlparse\n",
    "from urllib.robotparser import RobotFileParser\n",
    "\n",
    "\n",
    "def can_fetch_url(url_to_check):\n",
    "    \"\"\"Determine if the URL can be fetched by all crawlers - adding politeness / adherence to\n",
    "        robot policy.\"\"\"\n",
    "    parsed_url = urlparse(url_to_check)\n",
    "    base_url = parsed_url.scheme + \"://\" + parsed_url.netloc\n",
    "\n",
    "    rules = RobotFileParser()\n",
    "    try:\n",
    "        with urllib.request.urlopen(base_url + \"/robots.txt\", timeout=5) as response:\n",
    "            rules.parse(response.read().decode('utf-8').splitlines())\n",
    "        return rules.can_fetch(\"*\", url_to_check)\n",
    "    except urllib.error.URLError as e:\n",
    "        print(f\"Error accessing robots.txt: {e}\")\n",
    "    except socket.timeout as e:\n",
    "        print(f\"Timeout occurred: {e}\")\n",
    "\n",
    "    # Default to False as if Robot can't be checked then not compliant + the site may timeout.\n",
    "    return False\n",
    "\n",
    "\n",
    "def perform_coreference_resolution(article_texts, batch_size=100):\n",
    "    model = FCoref(device='mps')\n",
    "    predictions = model.predict(texts=article_texts, max_tokens_in_batch=batch_size)\n",
    "\n",
    "    # Empty list to store clusters for each article\n",
    "    article_text_clusters = []\n",
    "\n",
    "    for prediction in predictions:\n",
    "        clusters_text = prediction.get_clusters()\n",
    "        clusters_positions = prediction.get_clusters(as_strings=False)\n",
    "        combined_clusters = [(text, positions, len(text)) for text, positions in\n",
    "                             zip(clusters_text, clusters_positions)]\n",
    "        sorted_combined_clusters = sorted(combined_clusters, key=lambda x: x[2], reverse=True)\n",
    "        article_text_clusters.append(sorted_combined_clusters)\n",
    "            \n",
    "    for i, (clusterText, clusterPos, cluster_text_count) in enumerate(sorted_combined_clusters, 1):\n",
    "        print(f\"Cluster {i} : {clusterText}\")\n",
    "        print(f\"Cluster Text Count: {cluster_text_count}\")\n",
    "        print(\"Positions:\", clusterPos)\n",
    "        \n",
    "\n",
    "    return article_text_clusters\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T18:26:37.524404Z",
     "start_time": "2024-03-02T18:26:37.446907Z"
    }
   },
   "id": "c8960c3d31a766c8"
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'class_id': 2, 'class_label': 'positive', 'class_prob': 0.9137029647827148}\n"
     ]
    }
   ],
   "source": [
    "left_segment = ''\n",
    "mention_segment = 'He'\n",
    "right_segment = 'got a round of applause'\n",
    "\n",
    "# start_time = time.time()\n",
    "tsc = TargetSentimentClassifier()\n",
    "sentiment = tsc.infer_from_text(left_segment, mention_segment, right_segment)\n",
    "print(sentiment[0])\n",
    "            # elapsed_time = time.time() - start_time"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T18:26:41.381748Z",
     "start_time": "2024-03-02T18:26:37.451206Z"
    }
   },
   "id": "f4c7b8c9bdc9ff84"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'class_id': 0, 'class_label': 'negative', 'class_prob': 0.6077610850334167}\n"
     ]
    }
   ],
   "source": [
    "tsc = TargetSentimentClassifier()\n",
    "sentiment = tsc.infer_from_text(\"The plan devised by  \" ,\"Russell\", \" fell apart.\")\n",
    "print(sentiment[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-20T19:36:57.874630Z"
    }
   },
   "id": "a91f07b7ca96d850"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tsc = TargetSentimentClassifier()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cde34a6cb65b9d5f"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'class_id': 2, 'class_label': 'positive', 'class_prob': 0.5761538147926331}\n",
      "{'class_id': 0, 'class_label': 'negative', 'class_prob': 0.9824183583259583}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sentiment = tsc.infer_from_text(\"\" , \"Peter's\" , \"excellent plan has been impacted by Paul's \"\n",
    "                                                 \"dreadful delivery of it\")\n",
    "print(sentiment[0])\n",
    "\n",
    "sentiment = tsc.infer_from_text(\"Peter's excellent plan has been impacted by \", \"Paul's\",\n",
    "                                                 \"dreadful delivery of it\")\n",
    "print(sentiment[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T13:27:37.591407Z",
     "start_time": "2024-04-05T13:27:36.556662Z"
    }
   },
   "id": "ddabc4ab4bf0a483"
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during sentiment analysis: [<class 'NewsSentiment.customexceptions.TooLongTextException'>] \n"
     ]
    }
   ],
   "source": [
    "left_segment = 'The case was back in court on Friday where lawyers for the Biden administration argued that Harry might have made up stories of his drug use to “sell books.” The Heritage Foundation’s Nile Gardiner said, according to the Telegraph, that this was a “ridiculous argument,” saying, “He has never denied anything in his own book... including the extensive widespread drug use.” Speaking outside court after the hearing, Gardiner said it was “highly unlikely” Prince Harry entered as a diplomat as “he had no official role on behalf of British people, his own relationship with the royal family, that was at a low point as the judge himself actually referenced in his remarks.” The Heritage Foundation previously argued the former royal waived his right to privacy when he “sold every aspect of his private life for, in some estimates, over $135 million,” adding that'\n",
    "mention_segment = 'his'\n",
    "right_segment = 'right to privacy when he “sold every aspect of his private life for, in some \\\n",
    "estimates, over $135 million,” adding that his claims of his right to privacy have been “met with widespread public ridicule.” During Friday’s court session Harry’s recent interview on Good Morning America in which he said he had considered applying for US citizenship, was mentioned.'\n",
    "\n",
    "try:\n",
    "    # start_time = time.time()\n",
    "    tsc = TargetSentimentClassifier()\n",
    "    sentiment = tsc.infer_from_text(left_segment, mention_segment, right_segment)\n",
    "    print(sentiment[0])\n",
    "                # elapsed_time = time.time() - start_time\n",
    "except Exception as e:\n",
    "    print(f\"Error during sentiment analysis: [{type(e)}] {e}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-02T18:26:41.382598Z"
    }
   },
   "id": "57ad0f236da08d4a"
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "# url =  'https://www.thesun.co.uk/news/25217908/rishi-sunak-slashed-small-boat-arrivals-illegal-migration/'\n",
    "# headline  = '<b>Rishi Sunak</b> has slashed small boat arrivals &amp; voters worried about illegal migration should back him to sort it'\n",
    "\n",
    "url = ('https://www.express.co.uk/news/politics/1871347/sir-keir-starmer-labour-polls-populatiry-reform')\n",
    "headline  = 'Sir <b>Keir Starmer</b>&#39;s popularity drops while some Tory voters vow to vote Reform'\n",
    "article_list = []\n",
    "try:\n",
    "    if can_fetch_url(url):\n",
    "        downloaded = trafilatura.fetch_url(url)\n",
    "        # Extract metadata\n",
    "        metadata = trafilatura.extract_metadata(downloaded)\n",
    "        # print(metadata.date)\n",
    "\n",
    "        # Extract publication date\n",
    "        date_str = metadata.date\n",
    "        naive_datetime = datetime.strptime(date_str, '%Y-%m-%d')\n",
    "\n",
    "        # datetime aware to a satisfy model\n",
    "        publication_date = naive_datetime\n",
    "\n",
    "        # Extract some useful trafilatura metadata.\n",
    "        author = metadata.author\n",
    "\n",
    "        site_name = metadata.sitename\n",
    "        article_text = trafilatura.extract(downloaded, favour_recall=True,\n",
    "                                           include_comments=False, include_images=False,\n",
    "                                           include_tables=False)\n",
    "\n",
    "        if article_text and len(article_text) > 249:\n",
    "            article_obj = Article(url, headline, article_text, None,  publication_date,\n",
    "                                  author, site_name)\n",
    "            article_list.append(article_obj)\n",
    "        elif article_text is None:\n",
    "            print('article text is None')\n",
    "        elif len(article_text) < 250:\n",
    "            print(article_text)\n",
    "            \n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error processing article: {url}\")\n",
    "    print(f\"Error message: {str(e)}\")\n",
    "    \n",
    "article_objects = article_list\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T18:26:43.626757Z",
     "start_time": "2024-03-02T18:26:43.341021Z"
    }
   },
   "id": "af63bfbb2418dbc5"
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/02/2024 18:26:45 - INFO - \t missing_keys: []\n",
      "03/02/2024 18:26:45 - INFO - \t unexpected_keys: []\n",
      "03/02/2024 18:26:45 - INFO - \t mismatched_keys: []\n",
      "03/02/2024 18:26:45 - INFO - \t error_msgs: []\n",
      "03/02/2024 18:26:45 - INFO - \t Model Parameters: 90.5M, Transformer: 82.1M, Coref head: 8.4M\n",
      "03/02/2024 18:26:45 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/1 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b3a1dc0269d142b2896a469005dbc28b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/02/2024 18:26:45 - INFO - \t ***** Running Inference on 1 texts *****\n"
     ]
    },
    {
     "data": {
      "text/plain": "Inference:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "158867f1dc484446bffb4e594229728a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 1 : [\"Labour's\", 'the Opposition party', 'Labour’s', 'Labour', 'Labour', 'Labour', 'his Party', 'Labour', 'Labour', 'Labour', 'Labour']\n",
      "Cluster Text Count: 11\n",
      "Positions: [(78, 86), (146, 166), (230, 238), (373, 379), (439, 445), (963, 969), (1042, 1051), (1240, 1246), (1767, 1773), (1895, 1901), (2163, 2169)]\n",
      "Cluster 2 : ['Tory', 'the Tories', 'the Conservatives', 'Tory', 'the Conservatives', 'Tory', 'Tory', 'Tory', 'the party’s']\n",
      "Cluster Text Count: 9\n",
      "Positions: [(47, 51), (249, 259), (403, 420), (2005, 2009), (2046, 2063), (2107, 2111), (2557, 2561), (2579, 2583), (2712, 2723)]\n",
      "Cluster 3 : ['Hoyle', 'Speaker Sir Lindsay Hoyle', 'the Speaker', 'Hoyle', 'Sir Lindsay', 'he', 'Sir Lindsay', 'he', 'his']\n",
      "Cluster Text Count: 9\n",
      "Positions: [(224, 229), (1064, 1089), (1197, 1208), (1324, 1329), (1734, 1745), (1751, 1753), (1824, 1835), (1866, 1868), (1961, 1964)]\n",
      "Cluster 4 : [\"Sir Keir Starmer's\", 'Keir Starmer', 'Sir Keir Starmer', 'his', 'his', 'the Labour leader', 'his', 'Sir Keir']\n",
      "Cluster Text Count: 8\n",
      "Positions: [(0, 18), (168, 180), (574, 590), (600, 603), (900, 903), (959, 976), (1042, 1045), (1385, 1393)]\n",
      "Cluster 5 : ['Lee Anderson MP, who lost the Tory whip', 'The former Tory deputy chairman', 'Mr Anderson', 'his', 'the MP’s']\n",
      "Cluster Text Count: 5\n",
      "Positions: [(2527, 2566), (2568, 2599), (2691, 2702), (2740, 2743), (2822, 2830)]\n",
      "Cluster 6 : ['a fresh poll', 'A survey by Redfield & Wilton Strategies from Sunday', 'the survey', 'The new survey']\n",
      "Cluster Text Count: 4\n",
      "Positions: [(302, 314), (316, 368), (2078, 2088), (2171, 2185)]\n",
      "Cluster 7 : ['Redfield & Wilton Strategies', 'the company’s', 'Redfield & Wilton Strategies', 'our']\n",
      "Cluster Text Count: 4\n",
      "Positions: [(328, 356), (515, 528), (2275, 2303), (2371, 2374)]\n",
      "Cluster 8 : ['us', 'Our', 'us', 'our']\n",
      "Cluster Text Count: 4\n",
      "Positions: [(710, 712), (725, 728), (807, 809), (814, 817)]\n",
      "Cluster 9 : ['Rishi Sunak’s', 'Sunak', 'Mr Sunak', 'he']\n",
      "Cluster Text Count: 4\n",
      "Positions: [(2202, 2215), (2344, 2349), (2677, 2685), (2799, 2801)]\n",
      "Cluster 10 : ['comments made by Lee Anderson MP, who lost the Tory whip', 'said', 'the MP’s comments']\n",
      "Cluster Text Count: 3\n",
      "Positions: [(2510, 2566), (2600, 2604), (2822, 2839)]\n",
      "Cluster 11 : [\"Sir Keir Starmer's popularity\", \"Labour's popularity\"]\n",
      "Cluster Text Count: 2\n",
      "Positions: [(0, 29), (78, 97)]\n",
      "Cluster 12 : ['put', 'This']\n",
      "Cluster Text Count: 2\n",
      "Positions: [(369, 372), (429, 433)]\n",
      "Cluster 13 : ['pressured', 'This']\n",
      "Cluster Text Count: 2\n",
      "Positions: [(1213, 1222), (1272, 1276)]\n",
      "Cluster 14 : ['the motion', 'the SNP motion']\n",
      "Cluster Text Count: 2\n",
      "Positions: [(1260, 1270), (1288, 1302)]\n",
      "Cluster 15 : ['the SNP’s', 'SNP']\n",
      "Cluster Text Count: 2\n",
      "Positions: [(1099, 1108), (1292, 1295)]\n",
      "Cluster 16 : ['intimidated', 'the incident']\n",
      "Cluster Text Count: 2\n",
      "Positions: [(1052, 1063), (1365, 1377)]\n",
      "Cluster 17 : ['allegations that his Party intimidated Speaker Sir Lindsay Hoyle ahead of the SNP’s opposition day debate on a ceasefire in Gaza last week', 'the claims']\n",
      "Cluster Text Count: 2\n",
      "Positions: [(1025, 1163), (1421, 1431)]\n",
      "Cluster 18 : ['selected', 'his move']\n",
      "Cluster Text Count: 2\n",
      "Positions: [(1754, 1762), (1961, 1969)]\n",
      "Cluster 19 : ['2019', '2019']\n",
      "Cluster Text Count: 2\n",
      "Positions: [(2000, 2004), (2102, 2106)]\n",
      "Cluster 20 : ['Some 18% of 2019 Tory voters', 'they']\n",
      "Cluster Text Count: 2\n",
      "Positions: [(2090, 2118), (2124, 2128)]\n",
      "Cluster 21 : ['Reform', 'Reform UK']\n",
      "Cluster Text Count: 2\n",
      "Positions: [(71, 77), (2140, 2149)]\n",
      "Cluster 22 : ['Rishi Sunak’s approval rating', 'it']\n",
      "Cluster Text Count: 2\n",
      "Positions: [(2202, 2231), (2309, 2311)]\n",
      "Cluster 23 : ['The Prime Minister', 'his']\n",
      "Cluster Text Count: 2\n",
      "Positions: [(2441, 2459), (2494, 2497)]\n",
      "Cluster 24 : ['London', 'our capital city']\n",
      "Cluster Text Count: 2\n",
      "Positions: [(2605, 2611), (2640, 2656)]\n",
      "Cluster 25 : ['London mayor Sadiq Khan', 'his']\n",
      "Cluster Text Count: 2\n",
      "Positions: [(2605, 2628), (2665, 2668)]\n",
      "Cluster 26 : ['his “choice of words', 'it']\n",
      "Cluster Text Count: 2\n",
      "Positions: [(2740, 2760), (2780, 2782)]\n",
      "[[([\"Labour's\", 'the Opposition party', 'Labour’s', 'Labour', 'Labour', 'Labour', 'his Party', 'Labour', 'Labour', 'Labour', 'Labour'], [(78, 86), (146, 166), (230, 238), (373, 379), (439, 445), (963, 969), (1042, 1051), (1240, 1246), (1767, 1773), (1895, 1901), (2163, 2169)], 11), (['Tory', 'the Tories', 'the Conservatives', 'Tory', 'the Conservatives', 'Tory', 'Tory', 'Tory', 'the party’s'], [(47, 51), (249, 259), (403, 420), (2005, 2009), (2046, 2063), (2107, 2111), (2557, 2561), (2579, 2583), (2712, 2723)], 9), (['Hoyle', 'Speaker Sir Lindsay Hoyle', 'the Speaker', 'Hoyle', 'Sir Lindsay', 'he', 'Sir Lindsay', 'he', 'his'], [(224, 229), (1064, 1089), (1197, 1208), (1324, 1329), (1734, 1745), (1751, 1753), (1824, 1835), (1866, 1868), (1961, 1964)], 9), ([\"Sir Keir Starmer's\", 'Keir Starmer', 'Sir Keir Starmer', 'his', 'his', 'the Labour leader', 'his', 'Sir Keir'], [(0, 18), (168, 180), (574, 590), (600, 603), (900, 903), (959, 976), (1042, 1045), (1385, 1393)], 8), (['Lee Anderson MP, who lost the Tory whip', 'The former Tory deputy chairman', 'Mr Anderson', 'his', 'the MP’s'], [(2527, 2566), (2568, 2599), (2691, 2702), (2740, 2743), (2822, 2830)], 5), (['a fresh poll', 'A survey by Redfield & Wilton Strategies from Sunday', 'the survey', 'The new survey'], [(302, 314), (316, 368), (2078, 2088), (2171, 2185)], 4), (['Redfield & Wilton Strategies', 'the company’s', 'Redfield & Wilton Strategies', 'our'], [(328, 356), (515, 528), (2275, 2303), (2371, 2374)], 4), (['us', 'Our', 'us', 'our'], [(710, 712), (725, 728), (807, 809), (814, 817)], 4), (['Rishi Sunak’s', 'Sunak', 'Mr Sunak', 'he'], [(2202, 2215), (2344, 2349), (2677, 2685), (2799, 2801)], 4), (['comments made by Lee Anderson MP, who lost the Tory whip', 'said', 'the MP’s comments'], [(2510, 2566), (2600, 2604), (2822, 2839)], 3), ([\"Sir Keir Starmer's popularity\", \"Labour's popularity\"], [(0, 29), (78, 97)], 2), (['put', 'This'], [(369, 372), (429, 433)], 2), (['pressured', 'This'], [(1213, 1222), (1272, 1276)], 2), (['the motion', 'the SNP motion'], [(1260, 1270), (1288, 1302)], 2), (['the SNP’s', 'SNP'], [(1099, 1108), (1292, 1295)], 2), (['intimidated', 'the incident'], [(1052, 1063), (1365, 1377)], 2), (['allegations that his Party intimidated Speaker Sir Lindsay Hoyle ahead of the SNP’s opposition day debate on a ceasefire in Gaza last week', 'the claims'], [(1025, 1163), (1421, 1431)], 2), (['selected', 'his move'], [(1754, 1762), (1961, 1969)], 2), (['2019', '2019'], [(2000, 2004), (2102, 2106)], 2), (['Some 18% of 2019 Tory voters', 'they'], [(2090, 2118), (2124, 2128)], 2), (['Reform', 'Reform UK'], [(71, 77), (2140, 2149)], 2), (['Rishi Sunak’s approval rating', 'it'], [(2202, 2231), (2309, 2311)], 2), (['The Prime Minister', 'his'], [(2441, 2459), (2494, 2497)], 2), (['London', 'our capital city'], [(2605, 2611), (2640, 2656)], 2), (['London mayor Sadiq Khan', 'his'], [(2605, 2628), (2665, 2668)], 2), (['his “choice of words', 'it'], [(2740, 2760), (2780, 2782)], 2)]]\n"
     ]
    }
   ],
   "source": [
    "article_texts = [article.text_body for article in article_objects]\n",
    "article_text_clusters = perform_coreference_resolution(article_texts)\n",
    "\n",
    "print(article_text_clusters)\n",
    "\n",
    "for article, clusters in zip(article_objects, article_text_clusters):\n",
    "    article.set_coref_clusters(clusters)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T18:26:45.534747Z",
     "start_time": "2024-03-02T18:26:43.627372Z"
    }
   },
   "id": "c77fbb7fc86b2a1a"
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity Type: CARDINAL\n",
      "Entity Type: DATE\n",
      "Entity Type: EVENT\n",
      "Entity Type: FAC\n",
      "Entity Type: GPE\n",
      "Entity Type: LANGUAGE\n",
      "Entity Type: LAW\n",
      "Entity Type: LOC\n",
      "Entity Type: MONEY\n",
      "Entity Type: NORP\n",
      "Entity Type: ORDINAL\n",
      "Entity Type: ORG\n",
      "Entity Type: PERCENT\n",
      "Entity Type: PERSON\n",
      "Entity: Keir Starmer\n",
      "Positions: [[4, 16], [168, 180], [578, 590]]\n",
      "Label: PERSON\n",
      "Number of Positions: 3\n",
      "\n",
      "Entity: Lindsay Hoyle\n",
      "Positions: [[1076, 1089]]\n",
      "Label: PERSON\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: Hoyle\n",
      "Positions: [[1324, 1329]]\n",
      "Label: PERSON\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: Keir\n",
      "Positions: [[1389, 1393]]\n",
      "Label: PERSON\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: Penny Mordaunt\n",
      "Positions: [[1466, 1480]]\n",
      "Label: PERSON\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: Lindsay\n",
      "Positions: [[1738, 1745], [1828, 1835]]\n",
      "Label: PERSON\n",
      "Number of Positions: 2\n",
      "\n",
      "Entity: Lee Anderson MP\n",
      "Positions: [[2527, 2542]]\n",
      "Label: PERSON\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: Tory\n",
      "Positions: [[2579, 2583]]\n",
      "Label: PERSON\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: Sadiq Khan\n",
      "Positions: [[2618, 2628]]\n",
      "Label: PERSON\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: Sunak\n",
      "Positions: [[2680, 2685]]\n",
      "Label: PERSON\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity: Anderson\n",
      "Positions: [[2694, 2702]]\n",
      "Label: PERSON\n",
      "Number of Positions: 1\n",
      "\n",
      "Entity Type: PRODUCT\n",
      "Entity Type: QUANTITY\n",
      "Entity Type: TIME\n",
      "Entity Type: WORK_OF_ART\n",
      "NER People Time: 0.3395867347717285 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "article.source_ner_people()\n",
    "print(f\"NER People Time: {time.time() - start_time} seconds\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T18:26:45.876261Z",
     "start_time": "2024-03-02T18:26:45.532139Z"
    }
   },
   "id": "e3d9dc81e78361d0"
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing article text for custom tokenization...\n",
      "Original Article Text: Sir Keir Starmer's popularity drops while some Tory voters vow to vote Reform\n",
      "Labour's popularity dr...\n",
      "Number of sentences detected by TextBlob: 22\n",
      "Hyphen-newline positions (custom splits): []\n",
      "Sentence bounds before custom adjustments:\n",
      "[(0, 167), (168, 315), (316, 428), (429, 563), (564, 659), (660, 827), (828, 858), (859, 938), (939, 1164), (1165, 1271), (1272, 1378), (1379, 1614), (1615, 1823), (1824, 1970), (1971, 2089), (2090, 2170), (2171, 2274), (2275, 2440), (2441, 2567), (2568, 2676), (2677, 2794), (2795, 2880)]\n",
      "Updated sentence bounds after inserting custom intervals:\n",
      "[(0, 167), (168, 315), (316, 428), (429, 563), (564, 659), (660, 827), (828, 858), (859, 938), (939, 1164), (1165, 1271), (1272, 1378), (1379, 1614), (1615, 1823), (1824, 1970), (1971, 2089), (2090, 2170), (2171, 2274), (2275, 2440), (2441, 2567), (2568, 2676), (2677, 2794), (2795, 2880)]\n",
      "Total number of sentences after adjustments: 22\n",
      "Mention threshold (floor of sentences * 0.2): 4\n",
      "Sentence determined Time: 0.0010290145874023438 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "article.determine_sentences()\n",
    "print(f\"Sentence determined Time: {time.time() - start_time} seconds\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T18:26:45.880482Z",
     "start_time": "2024-03-02T18:26:45.877205Z"
    }
   },
   "id": "daff9d7e5bd739f9"
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing clusters for entity: Keir Starmer\n",
      "\n",
      "Cluster ID: 1, Original Text: [\"Labour's\", 'the Opposition party', 'Labour’s', 'Labour', 'Labour', 'Labour', 'his Party', 'Labour', 'Labour', 'Labour', 'Labour']\n",
      "Matching 'Keir' in cluster 1: 0.00% match.\n",
      "Matching 'Starmer' in cluster 1: 0.00% match.\n",
      "\n",
      "Cluster ID: 2, Original Text: ['Tory', 'the Tories', 'the Conservatives', 'Tory', 'the Conservatives', 'Tory', 'Tory', 'Tory', 'the party’s']\n",
      "Matching 'Keir' in cluster 2: 0.00% match.\n",
      "Matching 'Starmer' in cluster 2: 0.00% match.\n",
      "\n",
      "Cluster ID: 3, Original Text: ['Hoyle', 'Speaker Sir Lindsay Hoyle', 'the Speaker', 'Hoyle', 'Sir Lindsay', 'he', 'Sir Lindsay', 'he', 'his']\n",
      "Matching 'Keir' in cluster 3: 0.00% match.\n",
      "Matching 'Starmer' in cluster 3: 0.00% match.\n",
      "\n",
      "Cluster ID: 4, Original Text: [\"Sir Keir Starmer's\", 'Keir Starmer', 'Sir Keir Starmer', 'his', 'his', 'the Labour leader', 'his', 'Sir Keir']\n",
      "Matching 'Keir' in cluster 4: 80.00% match.\n",
      "Matching 'Starmer' in cluster 4: 60.00% match.\n",
      "Accepting cluster 4 for 'Keir Starmer' with 80.00% match on part 'Keir'.\n",
      "Processing clusters for entity: Lindsay Hoyle\n",
      "\n",
      "Cluster ID: 1, Original Text: [\"Labour's\", 'the Opposition party', 'Labour’s', 'Labour', 'Labour', 'Labour', 'his Party', 'Labour', 'Labour', 'Labour', 'Labour']\n",
      "Matching 'Lindsay' in cluster 1: 0.00% match.\n",
      "Matching 'Hoyle' in cluster 1: 0.00% match.\n",
      "\n",
      "Cluster ID: 2, Original Text: ['Tory', 'the Tories', 'the Conservatives', 'Tory', 'the Conservatives', 'Tory', 'Tory', 'Tory', 'the party’s']\n",
      "Matching 'Lindsay' in cluster 2: 0.00% match.\n",
      "Matching 'Hoyle' in cluster 2: 0.00% match.\n",
      "\n",
      "Cluster ID: 3, Original Text: ['Hoyle', 'Speaker Sir Lindsay Hoyle', 'the Speaker', 'Hoyle', 'Sir Lindsay', 'he', 'Sir Lindsay', 'he', 'his']\n",
      "Matching 'Lindsay' in cluster 3: 50.00% match.\n",
      "Matching 'Hoyle' in cluster 3: 50.00% match.\n",
      "Accepting cluster 3 for 'Lindsay Hoyle' with 50.00% match on part 'Lindsay'.\n",
      "Processing clusters for entity: Hoyle\n",
      "\n",
      "Cluster ID: 1, Original Text: [\"Labour's\", 'the Opposition party', 'Labour’s', 'Labour', 'Labour', 'Labour', 'his Party', 'Labour', 'Labour', 'Labour', 'Labour']\n",
      "Matching 'Hoyle' in cluster 1: 0.00% match.\n",
      "\n",
      "Cluster ID: 2, Original Text: ['Tory', 'the Tories', 'the Conservatives', 'Tory', 'the Conservatives', 'Tory', 'Tory', 'Tory', 'the party’s']\n",
      "Matching 'Hoyle' in cluster 2: 0.00% match.\n",
      "\n",
      "Cluster ID: 3, Original Text: ['Hoyle', 'Speaker Sir Lindsay Hoyle', 'the Speaker', 'Hoyle', 'Sir Lindsay', 'he', 'Sir Lindsay', 'he', 'his']\n",
      "Matching 'Hoyle' in cluster 3: 50.00% match.\n",
      "Accepting cluster 3 for 'Hoyle' with 50.00% match on part 'Hoyle'.\n",
      "Processing clusters for entity: Keir\n",
      "\n",
      "Cluster ID: 1, Original Text: [\"Labour's\", 'the Opposition party', 'Labour’s', 'Labour', 'Labour', 'Labour', 'his Party', 'Labour', 'Labour', 'Labour', 'Labour']\n",
      "Matching 'Keir' in cluster 1: 0.00% match.\n",
      "\n",
      "Cluster ID: 2, Original Text: ['Tory', 'the Tories', 'the Conservatives', 'Tory', 'the Conservatives', 'Tory', 'Tory', 'Tory', 'the party’s']\n",
      "Matching 'Keir' in cluster 2: 0.00% match.\n",
      "\n",
      "Cluster ID: 3, Original Text: ['Hoyle', 'Speaker Sir Lindsay Hoyle', 'the Speaker', 'Hoyle', 'Sir Lindsay', 'he', 'Sir Lindsay', 'he', 'his']\n",
      "Matching 'Keir' in cluster 3: 0.00% match.\n",
      "\n",
      "Cluster ID: 4, Original Text: [\"Sir Keir Starmer's\", 'Keir Starmer', 'Sir Keir Starmer', 'his', 'his', 'the Labour leader', 'his', 'Sir Keir']\n",
      "Matching 'Keir' in cluster 4: 80.00% match.\n",
      "Accepting cluster 4 for 'Keir' with 80.00% match on part 'Keir'.\n",
      "Processing clusters for entity: Penny Mordaunt\n",
      "\n",
      "Cluster ID: 1, Original Text: [\"Labour's\", 'the Opposition party', 'Labour’s', 'Labour', 'Labour', 'Labour', 'his Party', 'Labour', 'Labour', 'Labour', 'Labour']\n",
      "Matching 'Penny' in cluster 1: 0.00% match.\n",
      "Matching 'Mordaunt' in cluster 1: 0.00% match.\n",
      "\n",
      "Cluster ID: 2, Original Text: ['Tory', 'the Tories', 'the Conservatives', 'Tory', 'the Conservatives', 'Tory', 'Tory', 'Tory', 'the party’s']\n",
      "Matching 'Penny' in cluster 2: 0.00% match.\n",
      "Matching 'Mordaunt' in cluster 2: 0.00% match.\n",
      "\n",
      "Cluster ID: 3, Original Text: ['Hoyle', 'Speaker Sir Lindsay Hoyle', 'the Speaker', 'Hoyle', 'Sir Lindsay', 'he', 'Sir Lindsay', 'he', 'his']\n",
      "Matching 'Penny' in cluster 3: 0.00% match.\n",
      "Matching 'Mordaunt' in cluster 3: 0.00% match.\n",
      "\n",
      "Cluster ID: 4, Original Text: [\"Sir Keir Starmer's\", 'Keir Starmer', 'Sir Keir Starmer', 'his', 'his', 'the Labour leader', 'his', 'Sir Keir']\n",
      "Matching 'Penny' in cluster 4: 0.00% match.\n",
      "Matching 'Mordaunt' in cluster 4: 0.00% match.\n",
      "\n",
      "Cluster ID: 5, Original Text: ['Lee Anderson MP, who lost the Tory whip', 'The former Tory deputy chairman', 'Mr Anderson', 'his', 'the MP’s']\n",
      "Matching 'Penny' in cluster 5: 0.00% match.\n",
      "Matching 'Mordaunt' in cluster 5: 0.00% match.\n",
      "\n",
      "Cluster ID: 6, Original Text: ['a fresh poll', 'A survey by Redfield & Wilton Strategies from Sunday', 'the survey', 'The new survey']\n",
      "Matching 'Penny' in cluster 6: 0.00% match.\n",
      "Matching 'Mordaunt' in cluster 6: 0.00% match.\n",
      "\n",
      "Cluster ID: 7, Original Text: ['Redfield & Wilton Strategies', 'the company’s', 'Redfield & Wilton Strategies', 'our']\n",
      "Matching 'Penny' in cluster 7: 0.00% match.\n",
      "Matching 'Mordaunt' in cluster 7: 0.00% match.\n",
      "\n",
      "Cluster ID: 8, Original Text: ['us', 'Our', 'us', 'our']\n",
      "Matching 'Penny' in cluster 8: 0.00% match.\n",
      "Matching 'Mordaunt' in cluster 8: 0.00% match.\n",
      "\n",
      "Cluster ID: 9, Original Text: ['Rishi Sunak’s', 'Sunak', 'Mr Sunak', 'he']\n",
      "Skipping cluster 9 as it has length < 4\n",
      "\n",
      "Cluster ID: 10, Original Text: ['comments made by Lee Anderson MP, who lost the Tory whip', 'said', 'the MP’s comments']\n",
      "Skipping cluster 10 as it has length < 4\n",
      "\n",
      "Cluster ID: 11, Original Text: [\"Sir Keir Starmer's popularity\", \"Labour's popularity\"]\n",
      "Skipping cluster 11 as it has length < 4\n",
      "\n",
      "Cluster ID: 12, Original Text: ['put', 'This']\n",
      "Skipping cluster 12 as it has length < 4\n",
      "\n",
      "Cluster ID: 13, Original Text: ['pressured', 'This']\n",
      "Skipping cluster 13 as it has length < 4\n",
      "\n",
      "Cluster ID: 14, Original Text: ['the motion', 'the SNP motion']\n",
      "Skipping cluster 14 as it has length < 4\n",
      "\n",
      "Cluster ID: 15, Original Text: ['the SNP’s', 'SNP']\n",
      "Skipping cluster 15 as it has length < 4\n",
      "\n",
      "Cluster ID: 16, Original Text: ['intimidated', 'the incident']\n",
      "Skipping cluster 16 as it has length < 4\n",
      "\n",
      "Cluster ID: 17, Original Text: ['allegations that his Party intimidated Speaker Sir Lindsay Hoyle ahead of the SNP’s opposition day debate on a ceasefire in Gaza last week', 'the claims']\n",
      "Skipping cluster 17 as it has length < 4\n",
      "\n",
      "Cluster ID: 18, Original Text: ['selected', 'his move']\n",
      "Skipping cluster 18 as it has length < 4\n",
      "\n",
      "Cluster ID: 19, Original Text: ['2019', '2019']\n",
      "Skipping cluster 19 as it has length < 4\n",
      "\n",
      "Cluster ID: 20, Original Text: ['Some 18% of 2019 Tory voters', 'they']\n",
      "Skipping cluster 20 as it has length < 4\n",
      "\n",
      "Cluster ID: 21, Original Text: ['Reform', 'Reform UK']\n",
      "Skipping cluster 21 as it has length < 4\n",
      "\n",
      "Cluster ID: 22, Original Text: ['Rishi Sunak’s approval rating', 'it']\n",
      "Skipping cluster 22 as it has length < 4\n",
      "\n",
      "Cluster ID: 23, Original Text: ['The Prime Minister', 'his']\n",
      "Skipping cluster 23 as it has length < 4\n",
      "\n",
      "Cluster ID: 24, Original Text: ['London', 'our capital city']\n",
      "Skipping cluster 24 as it has length < 4\n",
      "\n",
      "Cluster ID: 25, Original Text: ['London mayor Sadiq Khan', 'his']\n",
      "Skipping cluster 25 as it has length < 4\n",
      "\n",
      "Cluster ID: 26, Original Text: ['his “choice of words', 'it']\n",
      "Skipping cluster 26 as it has length < 4\n",
      "No suitable clusters found for entity: Penny Mordaunt\n",
      "Processing clusters for entity: Lindsay\n",
      "\n",
      "Cluster ID: 1, Original Text: [\"Labour's\", 'the Opposition party', 'Labour’s', 'Labour', 'Labour', 'Labour', 'his Party', 'Labour', 'Labour', 'Labour', 'Labour']\n",
      "Matching 'Lindsay' in cluster 1: 0.00% match.\n",
      "\n",
      "Cluster ID: 2, Original Text: ['Tory', 'the Tories', 'the Conservatives', 'Tory', 'the Conservatives', 'Tory', 'Tory', 'Tory', 'the party’s']\n",
      "Matching 'Lindsay' in cluster 2: 0.00% match.\n",
      "\n",
      "Cluster ID: 3, Original Text: ['Hoyle', 'Speaker Sir Lindsay Hoyle', 'the Speaker', 'Hoyle', 'Sir Lindsay', 'he', 'Sir Lindsay', 'he', 'his']\n",
      "Matching 'Lindsay' in cluster 3: 50.00% match.\n",
      "Accepting cluster 3 for 'Lindsay' with 50.00% match on part 'Lindsay'.\n",
      "Processing clusters for entity: Lee Anderson MP\n",
      "\n",
      "Cluster ID: 1, Original Text: [\"Labour's\", 'the Opposition party', 'Labour’s', 'Labour', 'Labour', 'Labour', 'his Party', 'Labour', 'Labour', 'Labour', 'Labour']\n",
      "Matching 'Lee' in cluster 1: 0.00% match.\n",
      "Matching 'Anderson' in cluster 1: 0.00% match.\n",
      "Matching 'MP' in cluster 1: 0.00% match.\n",
      "\n",
      "Cluster ID: 2, Original Text: ['Tory', 'the Tories', 'the Conservatives', 'Tory', 'the Conservatives', 'Tory', 'Tory', 'Tory', 'the party’s']\n",
      "Matching 'Lee' in cluster 2: 0.00% match.\n",
      "Matching 'Anderson' in cluster 2: 0.00% match.\n",
      "Matching 'MP' in cluster 2: 0.00% match.\n",
      "\n",
      "Cluster ID: 3, Original Text: ['Hoyle', 'Speaker Sir Lindsay Hoyle', 'the Speaker', 'Hoyle', 'Sir Lindsay', 'he', 'Sir Lindsay', 'he', 'his']\n",
      "Matching 'Lee' in cluster 3: 0.00% match.\n",
      "Matching 'Anderson' in cluster 3: 0.00% match.\n",
      "Matching 'MP' in cluster 3: 0.00% match.\n",
      "\n",
      "Cluster ID: 4, Original Text: [\"Sir Keir Starmer's\", 'Keir Starmer', 'Sir Keir Starmer', 'his', 'his', 'the Labour leader', 'his', 'Sir Keir']\n",
      "Matching 'Lee' in cluster 4: 0.00% match.\n",
      "Matching 'Anderson' in cluster 4: 0.00% match.\n",
      "Matching 'MP' in cluster 4: 0.00% match.\n",
      "\n",
      "Cluster ID: 5, Original Text: ['Lee Anderson MP, who lost the Tory whip', 'The former Tory deputy chairman', 'Mr Anderson', 'his', 'the MP’s']\n",
      "Matching 'Lee' in cluster 5: 25.00% match.\n",
      "Matching 'Anderson' in cluster 5: 50.00% match.\n",
      "Matching 'MP' in cluster 5: 50.00% match.\n",
      "Accepting cluster 5 for 'Lee Anderson MP' with 50.00% match on part 'Anderson'.\n",
      "Processing clusters for entity: Tory\n",
      "\n",
      "Cluster ID: 1, Original Text: [\"Labour's\", 'the Opposition party', 'Labour’s', 'Labour', 'Labour', 'Labour', 'his Party', 'Labour', 'Labour', 'Labour', 'Labour']\n",
      "Matching 'Tory' in cluster 1: 0.00% match.\n",
      "\n",
      "Cluster ID: 2, Original Text: ['Tory', 'the Tories', 'the Conservatives', 'Tory', 'the Conservatives', 'Tory', 'Tory', 'Tory', 'the party’s']\n",
      "Matching 'Tory' in cluster 2: 55.56% match.\n",
      "Accepting cluster 2 for 'Tory' with 55.56% match on part 'Tory'.\n",
      "Processing clusters for entity: Sadiq Khan\n",
      "\n",
      "Cluster ID: 1, Original Text: [\"Labour's\", 'the Opposition party', 'Labour’s', 'Labour', 'Labour', 'Labour', 'his Party', 'Labour', 'Labour', 'Labour', 'Labour']\n",
      "Matching 'Sadiq' in cluster 1: 0.00% match.\n",
      "Matching 'Khan' in cluster 1: 0.00% match.\n",
      "\n",
      "Cluster ID: 2, Original Text: ['Tory', 'the Tories', 'the Conservatives', 'Tory', 'the Conservatives', 'Tory', 'Tory', 'Tory', 'the party’s']\n",
      "Matching 'Sadiq' in cluster 2: 0.00% match.\n",
      "Matching 'Khan' in cluster 2: 0.00% match.\n",
      "\n",
      "Cluster ID: 3, Original Text: ['Hoyle', 'Speaker Sir Lindsay Hoyle', 'the Speaker', 'Hoyle', 'Sir Lindsay', 'he', 'Sir Lindsay', 'he', 'his']\n",
      "Matching 'Sadiq' in cluster 3: 0.00% match.\n",
      "Matching 'Khan' in cluster 3: 0.00% match.\n",
      "\n",
      "Cluster ID: 4, Original Text: [\"Sir Keir Starmer's\", 'Keir Starmer', 'Sir Keir Starmer', 'his', 'his', 'the Labour leader', 'his', 'Sir Keir']\n",
      "Matching 'Sadiq' in cluster 4: 0.00% match.\n",
      "Matching 'Khan' in cluster 4: 0.00% match.\n",
      "\n",
      "Cluster ID: 5, Original Text: ['Lee Anderson MP, who lost the Tory whip', 'The former Tory deputy chairman', 'Mr Anderson', 'his', 'the MP’s']\n",
      "Matching 'Sadiq' in cluster 5: 0.00% match.\n",
      "Matching 'Khan' in cluster 5: 0.00% match.\n",
      "\n",
      "Cluster ID: 6, Original Text: ['a fresh poll', 'A survey by Redfield & Wilton Strategies from Sunday', 'the survey', 'The new survey']\n",
      "Matching 'Sadiq' in cluster 6: 0.00% match.\n",
      "Matching 'Khan' in cluster 6: 0.00% match.\n",
      "\n",
      "Cluster ID: 7, Original Text: ['Redfield & Wilton Strategies', 'the company’s', 'Redfield & Wilton Strategies', 'our']\n",
      "Matching 'Sadiq' in cluster 7: 0.00% match.\n",
      "Matching 'Khan' in cluster 7: 0.00% match.\n",
      "\n",
      "Cluster ID: 8, Original Text: ['us', 'Our', 'us', 'our']\n",
      "Matching 'Sadiq' in cluster 8: 0.00% match.\n",
      "Matching 'Khan' in cluster 8: 0.00% match.\n",
      "\n",
      "Cluster ID: 9, Original Text: ['Rishi Sunak’s', 'Sunak', 'Mr Sunak', 'he']\n",
      "Skipping cluster 9 as it has length < 4\n",
      "\n",
      "Cluster ID: 10, Original Text: ['comments made by Lee Anderson MP, who lost the Tory whip', 'said', 'the MP’s comments']\n",
      "Skipping cluster 10 as it has length < 4\n",
      "\n",
      "Cluster ID: 11, Original Text: [\"Sir Keir Starmer's popularity\", \"Labour's popularity\"]\n",
      "Skipping cluster 11 as it has length < 4\n",
      "\n",
      "Cluster ID: 12, Original Text: ['put', 'This']\n",
      "Skipping cluster 12 as it has length < 4\n",
      "\n",
      "Cluster ID: 13, Original Text: ['pressured', 'This']\n",
      "Skipping cluster 13 as it has length < 4\n",
      "\n",
      "Cluster ID: 14, Original Text: ['the motion', 'the SNP motion']\n",
      "Skipping cluster 14 as it has length < 4\n",
      "\n",
      "Cluster ID: 15, Original Text: ['the SNP’s', 'SNP']\n",
      "Skipping cluster 15 as it has length < 4\n",
      "\n",
      "Cluster ID: 16, Original Text: ['intimidated', 'the incident']\n",
      "Skipping cluster 16 as it has length < 4\n",
      "\n",
      "Cluster ID: 17, Original Text: ['allegations that his Party intimidated Speaker Sir Lindsay Hoyle ahead of the SNP’s opposition day debate on a ceasefire in Gaza last week', 'the claims']\n",
      "Skipping cluster 17 as it has length < 4\n",
      "\n",
      "Cluster ID: 18, Original Text: ['selected', 'his move']\n",
      "Skipping cluster 18 as it has length < 4\n",
      "\n",
      "Cluster ID: 19, Original Text: ['2019', '2019']\n",
      "Skipping cluster 19 as it has length < 4\n",
      "\n",
      "Cluster ID: 20, Original Text: ['Some 18% of 2019 Tory voters', 'they']\n",
      "Skipping cluster 20 as it has length < 4\n",
      "\n",
      "Cluster ID: 21, Original Text: ['Reform', 'Reform UK']\n",
      "Skipping cluster 21 as it has length < 4\n",
      "\n",
      "Cluster ID: 22, Original Text: ['Rishi Sunak’s approval rating', 'it']\n",
      "Skipping cluster 22 as it has length < 4\n",
      "\n",
      "Cluster ID: 23, Original Text: ['The Prime Minister', 'his']\n",
      "Skipping cluster 23 as it has length < 4\n",
      "\n",
      "Cluster ID: 24, Original Text: ['London', 'our capital city']\n",
      "Skipping cluster 24 as it has length < 4\n",
      "\n",
      "Cluster ID: 25, Original Text: ['London mayor Sadiq Khan', 'his']\n",
      "Skipping cluster 25 as it has length < 4\n",
      "\n",
      "Cluster ID: 26, Original Text: ['his “choice of words', 'it']\n",
      "Skipping cluster 26 as it has length < 4\n",
      "No suitable clusters found for entity: Sadiq Khan\n",
      "Processing clusters for entity: Sunak\n",
      "\n",
      "Cluster ID: 1, Original Text: [\"Labour's\", 'the Opposition party', 'Labour’s', 'Labour', 'Labour', 'Labour', 'his Party', 'Labour', 'Labour', 'Labour', 'Labour']\n",
      "Matching 'Sunak' in cluster 1: 0.00% match.\n",
      "\n",
      "Cluster ID: 2, Original Text: ['Tory', 'the Tories', 'the Conservatives', 'Tory', 'the Conservatives', 'Tory', 'Tory', 'Tory', 'the party’s']\n",
      "Matching 'Sunak' in cluster 2: 0.00% match.\n",
      "\n",
      "Cluster ID: 3, Original Text: ['Hoyle', 'Speaker Sir Lindsay Hoyle', 'the Speaker', 'Hoyle', 'Sir Lindsay', 'he', 'Sir Lindsay', 'he', 'his']\n",
      "Matching 'Sunak' in cluster 3: 0.00% match.\n",
      "\n",
      "Cluster ID: 4, Original Text: [\"Sir Keir Starmer's\", 'Keir Starmer', 'Sir Keir Starmer', 'his', 'his', 'the Labour leader', 'his', 'Sir Keir']\n",
      "Matching 'Sunak' in cluster 4: 0.00% match.\n",
      "\n",
      "Cluster ID: 5, Original Text: ['Lee Anderson MP, who lost the Tory whip', 'The former Tory deputy chairman', 'Mr Anderson', 'his', 'the MP’s']\n",
      "Matching 'Sunak' in cluster 5: 0.00% match.\n",
      "\n",
      "Cluster ID: 6, Original Text: ['a fresh poll', 'A survey by Redfield & Wilton Strategies from Sunday', 'the survey', 'The new survey']\n",
      "Matching 'Sunak' in cluster 6: 0.00% match.\n",
      "\n",
      "Cluster ID: 7, Original Text: ['Redfield & Wilton Strategies', 'the company’s', 'Redfield & Wilton Strategies', 'our']\n",
      "Matching 'Sunak' in cluster 7: 0.00% match.\n",
      "\n",
      "Cluster ID: 8, Original Text: ['us', 'Our', 'us', 'our']\n",
      "Matching 'Sunak' in cluster 8: 0.00% match.\n",
      "\n",
      "Cluster ID: 9, Original Text: ['Rishi Sunak’s', 'Sunak', 'Mr Sunak', 'he']\n",
      "Skipping cluster 9 as it has length < 4\n",
      "\n",
      "Cluster ID: 10, Original Text: ['comments made by Lee Anderson MP, who lost the Tory whip', 'said', 'the MP’s comments']\n",
      "Skipping cluster 10 as it has length < 4\n",
      "\n",
      "Cluster ID: 11, Original Text: [\"Sir Keir Starmer's popularity\", \"Labour's popularity\"]\n",
      "Skipping cluster 11 as it has length < 4\n",
      "\n",
      "Cluster ID: 12, Original Text: ['put', 'This']\n",
      "Skipping cluster 12 as it has length < 4\n",
      "\n",
      "Cluster ID: 13, Original Text: ['pressured', 'This']\n",
      "Skipping cluster 13 as it has length < 4\n",
      "\n",
      "Cluster ID: 14, Original Text: ['the motion', 'the SNP motion']\n",
      "Skipping cluster 14 as it has length < 4\n",
      "\n",
      "Cluster ID: 15, Original Text: ['the SNP’s', 'SNP']\n",
      "Skipping cluster 15 as it has length < 4\n",
      "\n",
      "Cluster ID: 16, Original Text: ['intimidated', 'the incident']\n",
      "Skipping cluster 16 as it has length < 4\n",
      "\n",
      "Cluster ID: 17, Original Text: ['allegations that his Party intimidated Speaker Sir Lindsay Hoyle ahead of the SNP’s opposition day debate on a ceasefire in Gaza last week', 'the claims']\n",
      "Skipping cluster 17 as it has length < 4\n",
      "\n",
      "Cluster ID: 18, Original Text: ['selected', 'his move']\n",
      "Skipping cluster 18 as it has length < 4\n",
      "\n",
      "Cluster ID: 19, Original Text: ['2019', '2019']\n",
      "Skipping cluster 19 as it has length < 4\n",
      "\n",
      "Cluster ID: 20, Original Text: ['Some 18% of 2019 Tory voters', 'they']\n",
      "Skipping cluster 20 as it has length < 4\n",
      "\n",
      "Cluster ID: 21, Original Text: ['Reform', 'Reform UK']\n",
      "Skipping cluster 21 as it has length < 4\n",
      "\n",
      "Cluster ID: 22, Original Text: ['Rishi Sunak’s approval rating', 'it']\n",
      "Skipping cluster 22 as it has length < 4\n",
      "\n",
      "Cluster ID: 23, Original Text: ['The Prime Minister', 'his']\n",
      "Skipping cluster 23 as it has length < 4\n",
      "\n",
      "Cluster ID: 24, Original Text: ['London', 'our capital city']\n",
      "Skipping cluster 24 as it has length < 4\n",
      "\n",
      "Cluster ID: 25, Original Text: ['London mayor Sadiq Khan', 'his']\n",
      "Skipping cluster 25 as it has length < 4\n",
      "\n",
      "Cluster ID: 26, Original Text: ['his “choice of words', 'it']\n",
      "Skipping cluster 26 as it has length < 4\n",
      "No suitable clusters found for entity: Sunak\n",
      "Processing clusters for entity: Anderson\n",
      "\n",
      "Cluster ID: 1, Original Text: [\"Labour's\", 'the Opposition party', 'Labour’s', 'Labour', 'Labour', 'Labour', 'his Party', 'Labour', 'Labour', 'Labour', 'Labour']\n",
      "Matching 'Anderson' in cluster 1: 0.00% match.\n",
      "\n",
      "Cluster ID: 2, Original Text: ['Tory', 'the Tories', 'the Conservatives', 'Tory', 'the Conservatives', 'Tory', 'Tory', 'Tory', 'the party’s']\n",
      "Matching 'Anderson' in cluster 2: 0.00% match.\n",
      "\n",
      "Cluster ID: 3, Original Text: ['Hoyle', 'Speaker Sir Lindsay Hoyle', 'the Speaker', 'Hoyle', 'Sir Lindsay', 'he', 'Sir Lindsay', 'he', 'his']\n",
      "Matching 'Anderson' in cluster 3: 0.00% match.\n",
      "\n",
      "Cluster ID: 4, Original Text: [\"Sir Keir Starmer's\", 'Keir Starmer', 'Sir Keir Starmer', 'his', 'his', 'the Labour leader', 'his', 'Sir Keir']\n",
      "Matching 'Anderson' in cluster 4: 0.00% match.\n",
      "\n",
      "Cluster ID: 5, Original Text: ['Lee Anderson MP, who lost the Tory whip', 'The former Tory deputy chairman', 'Mr Anderson', 'his', 'the MP’s']\n",
      "Matching 'Anderson' in cluster 5: 50.00% match.\n",
      "Accepting cluster 5 for 'Anderson' with 50.00% match on part 'Anderson'.\n",
      "Entity to cluster map time: 0.0011310577392578125 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "article.determine_entity_to_cluster_mapping()\n",
    "print(f\"Entity to cluster map time: {time.time() - start_time} seconds\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T18:26:45.885007Z",
     "start_time": "2024-03-02T18:26:45.881072Z"
    }
   },
   "id": "bed89a3bc2b79892"
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing Keir Starmer's as substring of Keir Starmer's\n",
      "Removing Hoyle as substring of Lindsay Hoyle\n",
      "Removing Anderson as substring of Lee Anderson MP\n",
      "Removing Lindsay as substring of Lindsay Hoyle\n",
      "number of entities before mention threshold: 4\n",
      "number of entities after mention threshold: 4\n",
      "Entity cluster map consolidation Time: 0.0009729862213134766 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "article.entity_cluster_map_consolidation()\n",
    "print(f\"Entity cluster map consolidation Time: {time.time() - start_time} seconds\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T18:26:45.893023Z",
     "start_time": "2024-03-02T18:26:45.884969Z"
    }
   },
   "id": "b64c5500c717cee6"
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Entity Name': 'Keir Starmer', 'Positions': -200, 'Label': 'PERSON', 'Num Positions': -200, 'Cluster Info': {'Cluster ID': 4, 'Cluster Text': [\"Sir Keir Starmer's\", 'Keir Starmer', 'Sir Keir Starmer', 'the Labour leader', 'Sir Keir'], 'Cluster Positions': [(0, 18), (168, 180), (574, 590), (600, 603), (900, 903), (959, 976), (1042, 1045), (1385, 1393)]}}\n",
      "\n",
      "{'Entity Name': 'Lindsay Hoyle', 'Positions': -200, 'Label': 'PERSON', 'Num Positions': -200, 'Cluster Info': {'Cluster ID': 3, 'Cluster Text': ['Hoyle', 'Speaker Sir Lindsay Hoyle', 'the Speaker', 'Hoyle', 'Sir Lindsay', 'Sir Lindsay'], 'Cluster Positions': [(224, 229), (1064, 1089), (1197, 1208), (1324, 1329), (1734, 1745), (1751, 1753), (1824, 1835), (1866, 1868), (1961, 1964)]}}\n",
      "\n",
      "{'Entity Name': 'Lee Anderson Mp', 'Positions': -200, 'Label': 'PERSON', 'Num Positions': -200, 'Cluster Info': {'Cluster ID': 5, 'Cluster Text': ['Lee Anderson MP, who lost the Tory whip', 'The former Tory deputy chairman', 'Mr Anderson', 'the MP’s'], 'Cluster Positions': [(2527, 2566), (2568, 2599), (2691, 2702), (2740, 2743), (2822, 2830)]}}\n",
      "\n",
      "{'Entity Name': 'Tory', 'Positions': [[2579, 2583]], 'Label': 'PERSON', 'Num Positions': 1, 'Cluster Info': {'Cluster ID': 2, 'Cluster Text': ['Tory', 'the Tories', 'the Conservatives', 'Tory', 'the Conservatives', 'Tory', 'Tory', 'Tory', 'the party’s'], 'Cluster Positions': [(47, 51), (249, 259), (403, 420), (2005, 2009), (2046, 2063), (2107, 2111), (2557, 2561), (2579, 2583), (2712, 2723)]}}\n"
     ]
    }
   ],
   "source": [
    "article.print_clustered_entities()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T18:26:45.893161Z",
     "start_time": "2024-03-02T18:26:45.888020Z"
    }
   },
   "id": "de37c8cd70af8c45"
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Would now be given a database id for the entity:  Keir Starmer\n",
      "Would now be given a database id for the entity:  Lindsay Hoyle\n",
      "Would now be given a database id for the entity:  Lee Anderson Mp\n",
      "Would now be given a database id for the entity:  Tory\n",
      "\u001B[0mKeir Starmer - Mention (0, 18) is within bounds (0, 167)\n",
      "\u001B[0m\u001B[94mSir Keir Starmer's\u001B[0m popularity drops while some Tory voters vow to vote Reform\n",
      "Labour's popularity drops following a rocky week in Westminster for the Opposition party.\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidateappearance1\u001B[0m\n",
      "\u001B[0mKeir Starmer - Mention (168, 180) is within bounds (168, 315)\n",
      "\u001B[0m\u001B[94mKeir Starmer\u001B[0m denies threatening to withdraw support for Hoyle\n",
      "Labour’s lead over the Tories has dropped by three points, according to a fresh poll.\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidateappearance1\u001B[0m\n",
      "\u001B[0mKeir Starmer - Mention (574, 590) is within bounds (564, 659)\n",
      "\u001B[0mMeanwhile \u001B[94mSir Keir Starmer\u001B[0m recorded his lowest approval rating since May last year at plus two.\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidateappearance1\u001B[0m\n",
      "\u001B[0mKeir Starmer - Mention (600, 603) is within bounds (564, 659)\n",
      "\u001B[0mMeanwhile Sir Keir Starmer recorded \u001B[94mhis\u001B[0m lowest approval rating since May last year at plus two.\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidateappearance2\u001B[0m\n",
      "\u001B[0mKeir Starmer - Mention (900, 903) is within bounds (859, 938)\n",
      "\u001B[0mMore info\n",
      "Some 35% of people approved of \u001B[94mhis\u001B[0m performance while 33% disapproved.\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidateappearance1\u001B[0m\n",
      "\u001B[0mKeir Starmer - Mention (959, 976) is within bounds (939, 1164)\n",
      "\u001B[0mMPs have called for \u001B[94mthe Labour leader\u001B[0m to be reported to the Privileges Committee over allegations that his Party intimidated Speaker Sir Lindsay Hoyle ahead of the SNP’s opposition day debate on a ceasefire in Gaza last week.\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidateappearance1\u001B[0m\n",
      "\u001B[0mKeir Starmer - Mention (1042, 1045) is within bounds (939, 1164)\n",
      "\u001B[0mMPs have called for the Labour leader to be reported to the Privileges Committee over allegations that \u001B[94mhis\u001B[0m Party intimidated Speaker Sir Lindsay Hoyle ahead of the SNP’s opposition day debate on a ceasefire in Gaza last week.\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidateappearance2\u001B[0m\n",
      "\u001B[0mKeir Starmer - Mention (1385, 1393) is within bounds (1379, 1614)\n",
      "\u001B[0mWhile \u001B[94mSir Keir\u001B[0m has \"categorically\" denied the claims, reports suggested Commons leader Penny Mordaunt believes there could have been a \"breach of privilege\" and an investigation is one of a number of potential options being considered.\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidateappearance1\u001B[0m\n",
      "\u001B[0mLindsay Hoyle - Mention (224, 229) is within bounds (168, 315)\n",
      "\u001B[0mKeir Starmer denies threatening to withdraw support for \u001B[94mHoyle\u001B[0m\n",
      "Labour’s lead over the Tories has dropped by three points, according to a fresh poll.\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidateappearance1\u001B[0m\n",
      "\u001B[0mLindsay Hoyle - Mention (1064, 1089) is within bounds (939, 1164)\n",
      "\u001B[0mMPs have called for the Labour leader to be reported to the Privileges Committee over allegations that his Party intimidated \u001B[94mSpeaker Sir Lindsay Hoyle\u001B[0m ahead of the SNP’s opposition day debate on a ceasefire in Gaza last week.\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidateappearance1\u001B[0m\n",
      "\u001B[0mLindsay Hoyle - Mention (1197, 1208) is within bounds (1165, 1271)\n",
      "\u001B[0mThere are concerns over whether \u001B[94mthe Speaker\u001B[0m was pressured into accepting a Labour amendment on the motion.\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidateappearance1\u001B[0m\n",
      "\u001B[0mLindsay Hoyle - Mention (1324, 1329) is within bounds (1272, 1378)\n",
      "\u001B[0mThis meant that the SNP motion was not voted on and \u001B[94mHoyle\u001B[0m is now facing calls to resign over the incident.\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidateappearance1\u001B[0m\n",
      "\u001B[0mLindsay Hoyle - Mention (1734, 1745) is within bounds (1615, 1823)\n",
      "\u001B[0mParliamentary convention dictates that there would usually only be a government amendment to an opposition motion, but \u001B[94mSir Lindsay\u001B[0m said he selected the Labour amendment to allow as broad a debate as possible.\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidateappearance1\u001B[0m\n",
      "\u001B[0mLindsay Hoyle - Mention (1751, 1753) is within bounds (1615, 1823)\n",
      "\u001B[0mParliamentary convention dictates that there would usually only be a government amendment to an opposition motion, but Sir Lindsay said \u001B[94mhe\u001B[0m selected the Labour amendment to allow as broad a debate as possible.\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidateappearance2\u001B[0m\n",
      "\u001B[0mLindsay Hoyle - Mention (1824, 1835) is within bounds (1824, 1970)\n",
      "\u001B[0m\u001B[94mSir Lindsay\u001B[0m has also rejected accusations he was put under pressure by Labour and has insisted the safety of MPs was the main reason for his move.\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidateappearance1\u001B[0m\n",
      "\u001B[0mLindsay Hoyle - Mention (1866, 1868) is within bounds (1824, 1970)\n",
      "\u001B[0mSir Lindsay has also rejected accusations \u001B[94mhe\u001B[0m was put under pressure by Labour and has insisted the safety of MPs was the main reason for his move.\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidateappearance2\u001B[0m\n",
      "\u001B[0mLindsay Hoyle - Mention (1961, 1964) is within bounds (1824, 1970)\n",
      "\u001B[0mSir Lindsay has also rejected accusations he was put under pressure by Labour and has insisted the safety of MPs was the main reason for \u001B[94mhis\u001B[0m move.\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidateappearance3\u001B[0m\n",
      "\u001B[0mLee Anderson Mp - Mention (2527, 2566) is within bounds (2441, 2567)\n",
      "\u001B[0mThe Prime Minister has come under fire this week for his response to comments made by \u001B[94mLee Anderson MP, who lost the Tory whip\u001B[0m.\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidateappearance1\u001B[0m\n",
      "\u001B[0mLee Anderson Mp - Mention (2568, 2599) is within bounds (2568, 2676)\n",
      "\u001B[0m\u001B[94mThe former Tory deputy chairman\u001B[0m said London mayor Sadiq Khan had “given our capital city away to his mates”.\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidateappearance1\u001B[0m\n",
      "\u001B[0mLee Anderson Mp - Mention (2691, 2702) is within bounds (2677, 2794)\n",
      "\u001B[0mMr Sunak said \u001B[94mMr Anderson\u001B[0m had lost the party’s backing because his “choice of words wasn’t acceptable, it was wrong”.\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidateappearance1\u001B[0m\n",
      "\u001B[0mLee Anderson Mp - Mention (2740, 2743) is within bounds (2677, 2794)\n",
      "\u001B[0mMr Sunak said Mr Anderson had lost the party’s backing because \u001B[94mhis\u001B[0m “choice of words wasn’t acceptable, it was wrong”.\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidateappearance2\u001B[0m\n",
      "\u001B[0mLee Anderson Mp - Mention (2822, 2830) is within bounds (2795, 2880)\n",
      "\u001B[0mBut he refused to describe \u001B[94mthe MP’s\u001B[0m comments as Islamophobic when pressed repeatedly.\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidateappearance1\u001B[0m\n",
      "\u001B[0mTory - Mention (47, 51) is within bounds (0, 167)\n",
      "\u001B[0mSir Keir Starmer's popularity drops while some \u001B[94mTory\u001B[0m voters vow to vote Reform\n",
      "Labour's popularity drops following a rocky week in Westminster for the Opposition party.\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidateappearance1\u001B[0m\n",
      "\u001B[0mTory - Mention (249, 259) is within bounds (168, 315)\n",
      "\u001B[0mKeir Starmer denies threatening to withdraw support for Hoyle\n",
      "Labour’s lead over \u001B[94mthe Tories\u001B[0m has dropped by three points, according to a fresh poll.\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidateappearance1\u001B[0m\n",
      "\u001B[0mTory - Mention (403, 420) is within bounds (316, 428)\n",
      "\u001B[0mA survey by Redfield & Wilton Strategies from Sunday put Labour on 43% of the vote and \u001B[94mthe Conservatives\u001B[0m on 23%.\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidateappearance1\u001B[0m\n",
      "\u001B[0mTory - Mention (2005, 2009) is within bounds (1971, 2089)\n",
      "\u001B[0mMeanwhile just under half of 2019 \u001B[94mTory\u001B[0m voters currently intend to vote for the Conservatives, according to the survey.\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidateappearance1\u001B[0m\n",
      "\u001B[0mTory - Mention (2046, 2063) is within bounds (1971, 2089)\n",
      "\u001B[0mMeanwhile just under half of 2019 Tory voters currently intend to vote for \u001B[94mthe Conservatives\u001B[0m, according to the survey.\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidateappearance2\u001B[0m\n",
      "\u001B[0mTory - Mention (2107, 2111) is within bounds (2090, 2170)\n",
      "\u001B[0mSome 18% of 2019 \u001B[94mTory\u001B[0m voters said they would back Reform UK and 16% said Labour.\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidateappearance1\u001B[0m\n",
      "\u001B[0mTory - Mention (2557, 2561) is within bounds (2441, 2567)\n",
      "\u001B[0mThe Prime Minister has come under fire this week for his response to comments made by Lee Anderson MP, who lost the \u001B[94mTory\u001B[0m whip.\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidateappearance1\u001B[0m\n",
      "\u001B[0mTory - Mention (2579, 2583) is within bounds (2568, 2676)\n",
      "\u001B[0mThe former \u001B[94mTory\u001B[0m deputy chairman said London mayor Sadiq Khan had “given our capital city away to his mates”.\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidateappearance1\u001B[0m\n",
      "\u001B[0mTory - Mention (2712, 2723) is within bounds (2677, 2794)\n",
      "\u001B[0mMr Sunak said Mr Anderson had lost \u001B[94mthe party’s\u001B[0m backing because his “choice of words wasn’t acceptable, it was wrong”.\u001B[94m\n",
      "\u001B[92mNewsSentiment Candidateappearance1\u001B[0m\n",
      "Bounds sentiment time: 11.345215082168579 seconds\n",
      "\n",
      "Inserting bound mention data: \n",
      "Entity Name: Keir Starmer\n",
      "Source Article ID: None\n",
      "Entity Database ID: 999\n",
      "Scores: [0.0, 0.0, 0.9354697465896606]\n",
      "Text Snippet: Sir Keir Starmer's popularity drops while some Tory voters vow to vote Reform\n",
      "Labour's popularity drops following a rocky week in Westminster for the Opposition party.\n",
      "Bounds Keys: (0, 167)\n",
      "\n",
      "Inserting bound mention data: \n",
      "Entity Name: Keir Starmer\n",
      "Source Article ID: None\n",
      "Entity Database ID: 999\n",
      "Scores: [0.0, 0.0, 0.8984778523445129]\n",
      "Text Snippet: Keir Starmer denies threatening to withdraw support for Hoyle\n",
      "Labour’s lead over the Tories has dropped by three points, according to a fresh poll.\n",
      "Bounds Keys: (168, 315)\n",
      "\n",
      "Inserting bound mention data: \n",
      "Entity Name: Keir Starmer\n",
      "Source Article ID: None\n",
      "Entity Database ID: 999\n",
      "Scores: [0.0, 0.0, 0.9089758992195129]\n",
      "Text Snippet: Meanwhile Sir Keir Starmer recorded his lowest approval rating since May last year at plus two.\n",
      "Bounds Keys: (564, 659)\n",
      "\n",
      "Inserting bound mention data: \n",
      "Entity Name: Keir Starmer\n",
      "Source Article ID: None\n",
      "Entity Database ID: 999\n",
      "Scores: [0.0, 0.6199931502342224, 0.0]\n",
      "Text Snippet: More info\n",
      "Some 35% of people approved of his performance while 33% disapproved.\n",
      "Bounds Keys: (859, 938)\n",
      "\n",
      "Inserting bound mention data: \n",
      "Entity Name: Keir Starmer\n",
      "Source Article ID: None\n",
      "Entity Database ID: 999\n",
      "Scores: [0.0, 0.0, 0.9456232488155365]\n",
      "Text Snippet: MPs have called for the Labour leader to be reported to the Privileges Committee over allegations that his Party intimidated Speaker Sir Lindsay Hoyle ahead of the SNP’s opposition day debate on a ceasefire in Gaza last week.\n",
      "Bounds Keys: (939, 1164)\n",
      "\n",
      "Inserting bound mention data: \n",
      "Entity Name: Keir Starmer\n",
      "Source Article ID: None\n",
      "Entity Database ID: 999\n",
      "Scores: [0.0, 0.0, 0.9782862663269043]\n",
      "Text Snippet: While Sir Keir has \"categorically\" denied the claims, reports suggested Commons leader Penny Mordaunt believes there could have been a \"breach of privilege\" and an investigation is one of a number of potential options being considered.\n",
      "Bounds Keys: (1379, 1614)\n",
      "Inserting overall sentiment data:\n",
      "Source Article ID: None\n",
      "Entity Name: Keir Starmer\n",
      "Number of Bounds: 6\n",
      "Linear Percent - Neutral: 0.0\n",
      "Linear Percent - Positive: 11.7\n",
      "Linear Percent - Negative: 88.3\n",
      "Exponential Percent - Neutral: 0.0\n",
      "Exponential Percent - Positive: 5.5\n",
      "Exponential Percent - Negative: 94.5\n",
      "\n",
      "Inserting bound mention data: \n",
      "Entity Name: Tory\n",
      "Source Article ID: None\n",
      "Entity Database ID: 999\n",
      "Scores: [0.6213993430137634, 0.0, 0.0]\n",
      "Text Snippet: Sir Keir Starmer's popularity drops while some Tory voters vow to vote Reform\n",
      "Labour's popularity drops following a rocky week in Westminster for the Opposition party.\n",
      "Bounds Keys: (0, 167)\n",
      "\n",
      "Inserting bound mention data: \n",
      "Entity Name: Tory\n",
      "Source Article ID: None\n",
      "Entity Database ID: 999\n",
      "Scores: [0.0, 0.0, 0.5514150261878967]\n",
      "Text Snippet: Keir Starmer denies threatening to withdraw support for Hoyle\n",
      "Labour’s lead over the Tories has dropped by three points, according to a fresh poll.\n",
      "Bounds Keys: (168, 315)\n",
      "\n",
      "Inserting bound mention data: \n",
      "Entity Name: Tory\n",
      "Source Article ID: None\n",
      "Entity Database ID: 999\n",
      "Scores: [0.0, 0.0, 0.6625524759292603]\n",
      "Text Snippet: The Prime Minister has come under fire this week for his response to comments made by Lee Anderson MP, who lost the Tory whip.\n",
      "Bounds Keys: (2441, 2567)\n",
      "\n",
      "Inserting bound mention data: \n",
      "Entity Name: Tory\n",
      "Source Article ID: None\n",
      "Entity Database ID: 999\n",
      "Scores: [0.9439519047737122, 0.0, 0.0]\n",
      "Text Snippet: The former Tory deputy chairman said London mayor Sadiq Khan had “given our capital city away to his mates”.\n",
      "Bounds Keys: (2568, 2676)\n",
      "\n",
      "Inserting bound mention data: \n",
      "Entity Name: Tory\n",
      "Source Article ID: None\n",
      "Entity Database ID: 999\n",
      "Scores: [0.6514632701873779, 0.0, 0.0]\n",
      "Text Snippet: Mr Sunak said Mr Anderson had lost the party’s backing because his “choice of words wasn’t acceptable, it was wrong”.\n",
      "Bounds Keys: (2677, 2794)\n",
      "\n",
      "Inserting bound mention data: \n",
      "Entity Name: Tory\n",
      "Source Article ID: None\n",
      "Entity Database ID: 999\n",
      "Scores: [0.793919563293457, 0.0, 0.0]\n",
      "Text Snippet: A survey by Redfield & Wilton Strategies from Sunday put Labour on 43% of the vote and the Conservatives on 23%.\n",
      "Bounds Keys: (316, 428)\n",
      "\n",
      "Inserting bound mention data: \n",
      "Entity Name: Tory\n",
      "Source Article ID: None\n",
      "Entity Database ID: 999\n",
      "Scores: [0.8508779406547546, 0.0, 0.0]\n",
      "Text Snippet: Meanwhile just under half of 2019 Tory voters currently intend to vote for the Conservatives, according to the survey.\n",
      "Bounds Keys: (1971, 2089)\n",
      "\n",
      "Inserting bound mention data: \n",
      "Entity Name: Tory\n",
      "Source Article ID: None\n",
      "Entity Database ID: 999\n",
      "Scores: [0.8030063509941101, 0.0, 0.0]\n",
      "Text Snippet: Some 18% of 2019 Tory voters said they would back Reform UK and 16% said Labour.\n",
      "Bounds Keys: (2090, 2170)\n",
      "Inserting overall sentiment data:\n",
      "Source Article ID: None\n",
      "Entity Name: Tory\n",
      "Number of Bounds: 8\n",
      "Linear Percent - Neutral: 79.3\n",
      "Linear Percent - Positive: 0.0\n",
      "Linear Percent - Negative: 20.7\n",
      "Exponential Percent - Neutral: 86.7\n",
      "Exponential Percent - Positive: 0.0\n",
      "Exponential Percent - Negative: 13.3\n",
      "\n",
      "Inserting bound mention data: \n",
      "Entity Name: Lindsay Hoyle\n",
      "Source Article ID: None\n",
      "Entity Database ID: 999\n",
      "Scores: [0.74396151304245, 0.0, 0.0]\n",
      "Text Snippet: Keir Starmer denies threatening to withdraw support for Hoyle\n",
      "Labour’s lead over the Tories has dropped by three points, according to a fresh poll.\n",
      "Bounds Keys: (168, 315)\n",
      "\n",
      "Inserting bound mention data: \n",
      "Entity Name: Lindsay Hoyle\n",
      "Source Article ID: None\n",
      "Entity Database ID: 999\n",
      "Scores: [0.0, 0.0, 0.7150408625602722]\n",
      "Text Snippet: MPs have called for the Labour leader to be reported to the Privileges Committee over allegations that his Party intimidated Speaker Sir Lindsay Hoyle ahead of the SNP’s opposition day debate on a ceasefire in Gaza last week.\n",
      "Bounds Keys: (939, 1164)\n",
      "\n",
      "Inserting bound mention data: \n",
      "Entity Name: Lindsay Hoyle\n",
      "Source Article ID: None\n",
      "Entity Database ID: 999\n",
      "Scores: [0.0, 0.0, 0.9249615669250488]\n",
      "Text Snippet: There are concerns over whether the Speaker was pressured into accepting a Labour amendment on the motion.\n",
      "Bounds Keys: (1165, 1271)\n",
      "\n",
      "Inserting bound mention data: \n",
      "Entity Name: Lindsay Hoyle\n",
      "Source Article ID: None\n",
      "Entity Database ID: 999\n",
      "Scores: [0.0, 0.0, 0.987793505191803]\n",
      "Text Snippet: This meant that the SNP motion was not voted on and Hoyle is now facing calls to resign over the incident.\n",
      "Bounds Keys: (1272, 1378)\n",
      "\n",
      "Inserting bound mention data: \n",
      "Entity Name: Lindsay Hoyle\n",
      "Source Article ID: None\n",
      "Entity Database ID: 999\n",
      "Scores: [0.0, 0.5709683001041412, 0.0]\n",
      "Text Snippet: Parliamentary convention dictates that there would usually only be a government amendment to an opposition motion, but Sir Lindsay said he selected the Labour amendment to allow as broad a debate as possible.\n",
      "Bounds Keys: (1615, 1823)\n",
      "\n",
      "Inserting bound mention data: \n",
      "Entity Name: Lindsay Hoyle\n",
      "Source Article ID: None\n",
      "Entity Database ID: 999\n",
      "Scores: [0.0, 0.17367384831110635, 0.520195722579956]\n",
      "Text Snippet: Sir Lindsay has also rejected accusations he was put under pressure by Labour and has insisted the safety of MPs was the main reason for his move.\n",
      "Bounds Keys: (1824, 1970)\n",
      "Inserting overall sentiment data:\n",
      "Source Article ID: None\n",
      "Entity Name: Lindsay Hoyle\n",
      "Number of Bounds: 6\n",
      "Linear Percent - Neutral: 16.0\n",
      "Linear Percent - Positive: 16.1\n",
      "Linear Percent - Negative: 67.9\n",
      "Exponential Percent - Neutral: 14.4\n",
      "Exponential Percent - Positive: 6.7\n",
      "Exponential Percent - Negative: 78.9\n",
      "\n",
      "Inserting bound mention data: \n",
      "Entity Name: Lee Anderson Mp\n",
      "Source Article ID: None\n",
      "Entity Database ID: 999\n",
      "Scores: [0.0, 0.0, 0.9347567558288574]\n",
      "Text Snippet: The Prime Minister has come under fire this week for his response to comments made by Lee Anderson MP, who lost the Tory whip.\n",
      "Bounds Keys: (2441, 2567)\n",
      "\n",
      "Inserting bound mention data: \n",
      "Entity Name: Lee Anderson Mp\n",
      "Source Article ID: None\n",
      "Entity Database ID: 999\n",
      "Scores: [0.9321355819702148, 0.0, 0.0]\n",
      "Text Snippet: The former Tory deputy chairman said London mayor Sadiq Khan had “given our capital city away to his mates”.\n",
      "Bounds Keys: (2568, 2676)\n",
      "\n",
      "Inserting bound mention data: \n",
      "Entity Name: Lee Anderson Mp\n",
      "Source Article ID: None\n",
      "Entity Database ID: 999\n",
      "Scores: [0.0, 0.0, 0.9881332218647003]\n",
      "Text Snippet: Mr Sunak said Mr Anderson had lost the party’s backing because his “choice of words wasn’t acceptable, it was wrong”.\n",
      "Bounds Keys: (2677, 2794)\n",
      "\n",
      "Inserting bound mention data: \n",
      "Entity Name: Lee Anderson Mp\n",
      "Source Article ID: None\n",
      "Entity Database ID: 999\n",
      "Scores: [0.0, 0.0, 0.9436575770378113]\n",
      "Text Snippet: But he refused to describe the MP’s comments as Islamophobic when pressed repeatedly.\n",
      "Bounds Keys: (2795, 2880)\n",
      "Inserting overall sentiment data:\n",
      "Source Article ID: None\n",
      "Entity Name: Lee Anderson Mp\n",
      "Number of Bounds: 4\n",
      "Linear Percent - Neutral: 24.5\n",
      "Linear Percent - Positive: 0.0\n",
      "Linear Percent - Negative: 75.5\n",
      "Exponential Percent - Neutral: 23.6\n",
      "Exponential Percent - Positive: 0.0\n",
      "Exponential Percent - Negative: 76.4\n",
      "Article would now be set as processed, similar rejection = False\n"
     ]
    }
   ],
   "source": [
    "if article.database_candidate:\n",
    "    # article.save_to_database() already done earlier in similar check now\n",
    "\n",
    "    if article.database_id != -1:\n",
    "        for entity_data in article.clustered_entities:\n",
    "            entity_name = entity_data['Entity Name']\n",
    "            print(\"Would now be given a database id for the entity: \", entity_name)\n",
    "            # entity_db_id = DatabaseUtils.insert_entity(entity_name, article.database_id)\n",
    "            entity_data['entity_db_id'] = 999\n",
    "\n",
    "        sentiment_analyser_demo = SentimentAnalyser()\n",
    "        article.set_sentiment_analyser(sentiment_analyser_demo)\n",
    "\n",
    "        start_time = time.time()\n",
    "        article.get_bounds_sentiment()\n",
    "        print(f\"Bounds sentiment time: {time.time() - start_time} seconds\")\n",
    "\n",
    "        start_time = time.time()\n",
    "        article.get_average_sentiment_results()\n",
    "        # print(f\"Average results time: {time.time() - start_time} seconds\")\n",
    "        # < 0.5 seconds\n",
    "\n",
    "        print('Article would now be set as processed, similar rejection = False')\n",
    "        # article.set_db_processed(True, similar_rejection=False)\n",
    "\n",
    "elif not article.database_candidate:\n",
    "    # print(\"Not enough mentions to add\")\n",
    "    print(\"Article set db processed, similar rejection = false\")\n",
    "    # article.set_db_processed(True, similar_rejection=False)\n",
    "else:\n",
    "    print(\"Article already exists in the database\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T18:26:59.064398Z",
     "start_time": "2024-03-02T18:26:45.893923Z"
    }
   },
   "id": "6b5732d2b0f83294"
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T18:26:59.064608Z",
     "start_time": "2024-03-02T18:26:59.062768Z"
    }
   },
   "id": "6d7855740edad798"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
